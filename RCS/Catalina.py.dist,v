head	1.176;
access;
symbols;
locks; strict;
comment	@# @;


1.176
date	2007.01.30.19.24.26;	author kenneth;	state Exp;
branches;
next	1.175;

1.175
date	2007.01.29.18.14.33;	author kenneth;	state Exp;
branches;
next	1.174;

1.174
date	2007.01.27.03.12.52;	author kenneth;	state Exp;
branches;
next	1.173;

1.173
date	2007.01.23.23.12.12;	author kenneth;	state Exp;
branches;
next	1.172;

1.172
date	2006.11.22.18.30.23;	author mmargo;	state Exp;
branches;
next	1.171;

1.171
date	2006.08.11.22.34.41;	author mmargo;	state Exp;
branches;
next	1.170;

1.170
date	2006.07.26.21.02.18;	author mmargo;	state Exp;
branches;
next	1.169;

1.169
date	2006.07.18.00.07.19;	author mmargo;	state Exp;
branches;
next	1.168;

1.168
date	2006.07.17.22.22.26;	author mmargo;	state Exp;
branches;
next	1.167;

1.167
date	2006.07.05.21.18.10;	author mmargo;	state Exp;
branches;
next	1.166;

1.166
date	2006.05.22.18.40.40;	author mmargo;	state Exp;
branches;
next	1.165;

1.165
date	2006.03.21.23.28.01;	author mmargo;	state Exp;
branches;
next	1.164;

1.164
date	2005.12.05.20.24.38;	author mmargo;	state Exp;
branches;
next	1.163;

1.163
date	2005.06.10.17.10.18;	author kenneth;	state Exp;
branches;
next	1.162;

1.162
date	2005.05.23.16.18.21;	author kenneth;	state Exp;
branches;
next	1.161;

1.161
date	2005.05.20.17.55.26;	author kenneth;	state Exp;
branches;
next	1.160;

1.160
date	2005.05.10.04.40.20;	author kenneth;	state Exp;
branches;
next	1.159;

1.159
date	2005.04.22.23.15.38;	author kenneth;	state Exp;
branches;
next	1.158;

1.158
date	2005.04.07.18.16.43;	author kenneth;	state Exp;
branches;
next	1.157;

1.157
date	2005.03.30.00.02.30;	author kenneth;	state Exp;
branches;
next	1.156;

1.156
date	2005.03.29.22.04.29;	author kenneth;	state Exp;
branches;
next	1.155;

1.155
date	2005.03.29.07.32.25;	author kenneth;	state Exp;
branches;
next	1.154;

1.154
date	2005.03.28.21.06.57;	author kenneth;	state Exp;
branches;
next	1.153;

1.153
date	2005.03.28.20.34.10;	author kenneth;	state Exp;
branches;
next	1.152;

1.152
date	2005.03.28.20.06.12;	author kenneth;	state Exp;
branches;
next	1.151;

1.151
date	2005.03.22.23.58.46;	author kenneth;	state Exp;
branches;
next	1.150;

1.150
date	2004.11.15.21.58.37;	author kenneth;	state Exp;
branches;
next	1.149;

1.149
date	2004.09.09.05.52.41;	author kenneth;	state Exp;
branches;
next	1.148;

1.148
date	2004.09.09.04.51.38;	author kenneth;	state Exp;
branches;
next	1.147;

1.147
date	2004.09.01.19.25.28;	author kenneth;	state Exp;
branches;
next	1.146;

1.146
date	2004.08.31.21.50.05;	author kenneth;	state Exp;
branches;
next	1.145;

1.145
date	2004.08.31.21.38.01;	author kenneth;	state Exp;
branches;
next	1.144;

1.144
date	2004.08.24.17.24.33;	author kenneth;	state Exp;
branches;
next	1.143;

1.143
date	2004.08.24.16.24.39;	author kenneth;	state Exp;
branches;
next	1.142;

1.142
date	2004.08.17.15.47.54;	author kenneth;	state Exp;
branches;
next	1.141;

1.141
date	2004.08.11.17.42.59;	author kenneth;	state Exp;
branches;
next	1.140;

1.140
date	2004.08.02.17.42.41;	author kenneth;	state Exp;
branches;
next	1.139;

1.139
date	2004.07.28.22.17.42;	author kenneth;	state Exp;
branches;
next	1.138;

1.138
date	2004.07.28.19.02.55;	author kenneth;	state Exp;
branches;
next	1.137;

1.137
date	2004.07.26.22.47.03;	author kenneth;	state Exp;
branches;
next	1.136;

1.136
date	2004.07.19.23.47.51;	author kenneth;	state Exp;
branches;
next	1.135;

1.135
date	2004.05.26.18.55.45;	author kenneth;	state Exp;
branches;
next	1.134;

1.134
date	2004.05.26.15.53.59;	author kenneth;	state Exp;
branches;
next	1.133;

1.133
date	2004.05.18.19.15.46;	author kenneth;	state Exp;
branches;
next	1.132;

1.132
date	2004.05.17.18.12.56;	author kenneth;	state Exp;
branches;
next	1.131;

1.131
date	2004.05.08.05.41.08;	author kenneth;	state Exp;
branches;
next	1.130;

1.130
date	2004.05.03.18.19.11;	author kenneth;	state Exp;
branches;
next	1.129;

1.129
date	2004.05.03.06.08.54;	author kenneth;	state Exp;
branches;
next	1.128;

1.128
date	2004.04.19.22.57.52;	author kenneth;	state Exp;
branches;
next	1.127;

1.127
date	2004.04.12.08.44.16;	author kenneth;	state Exp;
branches;
next	1.126;

1.126
date	2004.04.12.07.34.58;	author kenneth;	state Exp;
branches;
next	1.125;

1.125
date	2004.04.12.05.13.57;	author kenneth;	state Exp;
branches;
next	1.124;

1.124
date	2004.04.11.21.54.44;	author kenneth;	state Exp;
branches;
next	1.123;

1.123
date	2004.04.08.21.49.12;	author kenneth;	state Exp;
branches;
next	1.122;

1.122
date	2004.04.08.21.42.12;	author kenneth;	state Exp;
branches;
next	1.121;

1.121
date	2004.04.08.15.13.47;	author kenneth;	state Exp;
branches;
next	1.120;

1.120
date	2004.04.03.00.18.40;	author kenneth;	state Exp;
branches;
next	1.119;

1.119
date	2004.04.01.13.30.06;	author kenneth;	state Exp;
branches;
next	1.118;

1.118
date	2004.03.16.23.11.12;	author kenneth;	state Exp;
branches;
next	1.117;

1.117
date	2004.03.16.22.31.38;	author kenneth;	state Exp;
branches;
next	1.116;

1.116
date	2004.02.09.23.19.24;	author kenneth;	state Exp;
branches;
next	1.115;

1.115
date	2004.02.05.17.54.54;	author kenneth;	state Exp;
branches;
next	1.114;

1.114
date	2003.10.28.21.58.33;	author kenneth;	state Exp;
branches;
next	1.113;

1.113
date	2003.08.07.22.09.48;	author kenneth;	state Exp;
branches;
next	1.112;

1.112
date	2003.08.07.16.23.25;	author kenneth;	state Exp;
branches;
next	1.111;

1.111
date	2003.08.06.19.47.19;	author kenneth;	state Exp;
branches;
next	1.110;

1.110
date	2003.07.18.16.52.16;	author kenneth;	state Exp;
branches;
next	1.109;

1.109
date	2003.07.02.20.03.57;	author kenneth;	state Exp;
branches;
next	1.108;

1.108
date	2003.07.01.18.00.19;	author kenneth;	state Exp;
branches;
next	1.107;

1.107
date	2003.06.28.15.41.52;	author kenneth;	state Exp;
branches;
next	1.106;

1.106
date	2003.06.27.20.59.30;	author kenneth;	state Exp;
branches;
next	1.105;

1.105
date	2003.06.27.16.32.01;	author kenneth;	state Exp;
branches;
next	1.104;

1.104
date	2003.06.26.23.58.09;	author kenneth;	state Exp;
branches;
next	1.103;

1.103
date	2003.06.26.22.48.00;	author kenneth;	state Exp;
branches;
next	1.102;

1.102
date	2003.06.26.22.46.03;	author kenneth;	state Exp;
branches;
next	1.101;

1.101
date	2003.06.26.22.43.27;	author kenneth;	state Exp;
branches;
next	1.100;

1.100
date	2003.06.26.18.06.11;	author kenneth;	state Exp;
branches;
next	1.99;

1.99
date	2003.05.23.21.38.52;	author kenneth;	state Exp;
branches;
next	1.98;

1.98
date	2003.05.05.22.22.21;	author kenneth;	state Exp;
branches;
next	1.97;

1.97
date	2003.04.24.22.22.38;	author kenneth;	state Exp;
branches;
next	1.96;

1.96
date	2003.04.24.22.18.45;	author kenneth;	state Exp;
branches;
next	1.95;

1.95
date	2003.04.24.22.03.34;	author kenneth;	state Exp;
branches;
next	1.94;

1.94
date	2003.04.24.21.55.10;	author kenneth;	state Exp;
branches;
next	1.93;

1.93
date	2003.04.24.21.49.58;	author kenneth;	state Exp;
branches;
next	1.92;

1.92
date	2003.04.16.20.18.37;	author kenneth;	state Exp;
branches;
next	1.91;

1.91
date	2003.04.15.22.42.08;	author kenneth;	state Exp;
branches;
next	1.90;

1.90
date	2003.04.15.20.28.53;	author kenneth;	state Exp;
branches;
next	1.89;

1.89
date	2003.04.08.22.56.39;	author kenneth;	state Exp;
branches;
next	1.88;

1.88
date	2003.03.07.23.38.50;	author kenneth;	state Exp;
branches;
next	1.87;

1.87
date	2003.02.18.23.30.25;	author kenneth;	state Exp;
branches;
next	1.86;

1.86
date	2003.01.24.23.47.40;	author kenneth;	state Exp;
branches;
next	1.85;

1.85
date	2003.01.24.22.20.40;	author kenneth;	state Exp;
branches;
next	1.84;

1.84
date	2002.12.20.21.36.46;	author kenneth;	state Exp;
branches;
next	1.83;

1.83
date	2002.12.20.18.25.12;	author kenneth;	state Exp;
branches;
next	1.82;

1.82
date	2002.12.19.22.01.12;	author kenneth;	state Exp;
branches;
next	1.81;

1.81
date	2002.12.19.01.15.21;	author kenneth;	state Exp;
branches;
next	1.80;

1.80
date	2002.11.07.18.42.24;	author kenneth;	state Exp;
branches;
next	1.79;

1.79
date	2002.11.06.23.32.00;	author kenneth;	state Exp;
branches;
next	1.78;

1.78
date	2002.11.05.20.17.33;	author kenneth;	state Exp;
branches;
next	1.77;

1.77
date	2002.11.05.19.49.21;	author kenneth;	state Exp;
branches;
next	1.76;

1.76
date	2002.11.03.17.57.06;	author kenneth;	state Exp;
branches;
next	1.75;

1.75
date	2002.11.01.19.38.01;	author kenneth;	state Exp;
branches;
next	1.74;

1.74
date	2002.10.01.19.57.16;	author kenneth;	state Exp;
branches;
next	1.73;

1.73
date	2002.09.04.21.56.38;	author kenneth;	state Exp;
branches;
next	1.72;

1.72
date	2002.06.19.23.19.37;	author kenneth;	state Exp;
branches;
next	1.71;

1.71
date	2002.04.19.16.52.36;	author kenneth;	state Exp;
branches;
next	1.70;

1.70
date	2002.04.11.17.51.20;	author kenneth;	state Exp;
branches;
next	1.69;

1.69
date	2002.04.10.18.18.01;	author kenneth;	state Exp;
branches;
next	1.68;

1.68
date	2002.04.09.21.30.46;	author kenneth;	state Exp;
branches;
next	1.67;

1.67
date	2002.04.09.18.09.28;	author kenneth;	state Exp;
branches;
next	1.66;

1.66
date	2002.04.09.17.25.38;	author kenneth;	state Exp;
branches;
next	1.65;

1.65
date	2002.03.11.17.24.06;	author kenneth;	state Exp;
branches;
next	1.64;

1.64
date	2002.03.05.18.04.21;	author kenneth;	state Exp;
branches;
next	1.63;

1.63
date	2002.03.05.17.59.26;	author kenneth;	state Exp;
branches;
next	1.62;

1.62
date	2002.03.05.17.42.06;	author kenneth;	state Exp;
branches;
next	1.61;

1.61
date	2002.02.28.17.25.26;	author kenneth;	state Exp;
branches;
next	1.60;

1.60
date	2002.02.27.01.35.59;	author kenneth;	state Exp;
branches;
next	1.59;

1.59
date	2002.02.25.19.17.58;	author kenneth;	state Exp;
branches;
next	1.58;

1.58
date	2002.02.22.19.22.29;	author kenneth;	state Exp;
branches;
next	1.57;

1.57
date	2002.02.05.22.46.25;	author kenneth;	state Exp;
branches;
next	1.56;

1.56
date	2002.02.04.17.41.10;	author kenneth;	state Exp;
branches;
next	1.55;

1.55
date	2002.02.01.21.43.30;	author kenneth;	state Exp;
branches;
next	1.54;

1.54
date	2002.01.25.15.10.29;	author kenneth;	state Exp;
branches;
next	1.53;

1.53
date	2002.01.17.22.06.23;	author kenneth;	state Exp;
branches;
next	1.52;

1.52
date	2002.01.16.21.02.17;	author kenneth;	state Exp;
branches;
next	1.51;

1.51
date	2002.01.16.20.51.05;	author kenneth;	state Exp;
branches;
next	1.50;

1.50
date	2002.01.11.20.22.15;	author kenneth;	state Exp;
branches;
next	1.49;

1.49
date	2002.01.11.20.09.20;	author kenneth;	state Exp;
branches;
next	1.48;

1.48
date	2002.01.03.17.10.59;	author kenneth;	state Exp;
branches;
next	1.47;

1.47
date	2002.01.03.17.03.11;	author kenneth;	state Exp;
branches;
next	1.46;

1.46
date	2002.01.03.01.49.47;	author kenneth;	state Exp;
branches;
next	1.45;

1.45
date	2002.01.03.00.26.03;	author kenneth;	state Exp;
branches;
next	1.44;

1.44
date	2001.12.21.20.08.57;	author kenneth;	state Exp;
branches;
next	1.43;

1.43
date	2001.12.21.17.08.00;	author kenneth;	state Exp;
branches;
next	1.42;

1.42
date	2001.12.20.18.16.03;	author kenneth;	state Exp;
branches;
next	1.41;

1.41
date	2001.12.19.01.58.26;	author kenneth;	state Exp;
branches;
next	1.40;

1.40
date	2001.12.18.19.35.06;	author kenneth;	state Exp;
branches;
next	1.39;

1.39
date	2001.12.18.01.10.55;	author kenneth;	state Exp;
branches;
next	1.38;

1.38
date	2001.12.14.18.10.54;	author kenneth;	state Exp;
branches;
next	1.37;

1.37
date	2001.11.30.20.38.08;	author kenneth;	state Exp;
branches;
next	1.36;

1.36
date	2001.11.30.20.35.09;	author kenneth;	state Exp;
branches;
next	1.35;

1.35
date	2001.11.28.22.05.08;	author kenneth;	state Exp;
branches;
next	1.34;

1.34
date	2001.11.27.18.03.25;	author kenneth;	state Exp;
branches;
next	1.33;

1.33
date	2001.11.27.00.56.00;	author kenneth;	state Exp;
branches;
next	1.32;

1.32
date	2001.11.27.00.08.31;	author kenneth;	state Exp;
branches;
next	1.31;

1.31
date	2001.11.07.20.38.51;	author kenneth;	state Exp;
branches;
next	1.30;

1.30
date	2001.11.07.20.15.41;	author kenneth;	state Exp;
branches;
next	1.29;

1.29
date	2001.11.07.19.12.55;	author kenneth;	state Exp;
branches;
next	1.28;

1.28
date	2001.11.07.18.46.29;	author kenneth;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.07.18.41.08;	author kenneth;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.06.23.57.16;	author kenneth;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.06.23.03.38;	author kenneth;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.01.00.13.31;	author kenneth;	state Exp;
branches;
next	1.23;

1.23
date	2001.10.31.18.31.09;	author kenneth;	state Exp;
branches;
next	1.22;

1.22
date	2001.10.23.21.03.52;	author kenneth;	state Exp;
branches;
next	1.21;

1.21
date	2001.10.23.16.56.49;	author kenneth;	state Exp;
branches;
next	1.20;

1.20
date	2001.10.16.21.41.48;	author kenneth;	state Exp;
branches;
next	1.19;

1.19
date	2001.10.05.23.04.24;	author kenneth;	state Exp;
branches;
next	1.18;

1.18
date	2001.10.05.22.54.11;	author kenneth;	state Exp;
branches;
next	1.17;

1.17
date	2001.10.04.17.25.57;	author kenneth;	state Exp;
branches;
next	1.16;

1.16
date	2001.09.25.17.41.11;	author kenneth;	state Exp;
branches;
next	1.15;

1.15
date	2001.09.24.23.26.10;	author kenneth;	state Exp;
branches;
next	1.14;

1.14
date	2001.09.24.21.29.28;	author kenneth;	state Exp;
branches;
next	1.13;

1.13
date	2001.09.20.18.06.55;	author kenneth;	state Exp;
branches;
next	1.12;

1.12
date	2001.09.14.21.24.43;	author kenneth;	state Exp;
branches;
next	1.11;

1.11
date	2001.09.14.19.51.40;	author kenneth;	state Exp;
branches;
next	1.10;

1.10
date	2001.09.13.00.33.46;	author kenneth;	state Exp;
branches;
next	1.9;

1.9
date	2001.09.12.23.54.10;	author kenneth;	state Exp;
branches;
next	1.8;

1.8
date	2001.09.12.23.52.33;	author kenneth;	state Exp;
branches;
next	1.7;

1.7
date	2001.09.12.23.40.58;	author kenneth;	state Exp;
branches;
next	1.6;

1.6
date	2001.09.12.23.18.14;	author kenneth;	state Exp;
branches;
next	1.5;

1.5
date	2001.09.12.23.02.12;	author kenneth;	state Exp;
branches;
next	1.4;

1.4
date	2001.09.12.22.05.22;	author kenneth;	state Exp;
branches;
next	1.3;

1.3
date	2001.09.11.22.09.31;	author kenneth;	state Exp;
branches;
next	1.2;

1.2
date	2001.09.11.21.19.25;	author kenneth;	state Exp;
branches;
next	1.1;

1.1
date	2001.09.11.19.45.39;	author kenneth;	state Exp;
branches;
next	;


desc
@Catalina module
@


1.176
log
@removed bug in DEBUGJOB test
@
text
@# Copyright 2001 The Regents of the University of California
# All Rights Reserved
# 
# Permission to use, copy, modify and distribute any part of this
# Catalina Scheduler program for educational, research and non-profit purposes,
# without fee, and without a written agreement is hereby granted, provided that
# the above copyright notice, this paragraph and the following three paragraphs
# appear in all copies.
# 
# Those desiring to incorporate this Catalina Scheduler program into commercial
# products or use for commercial purposes should contact:
# William J. Decker, Ph.D.
# Licensing Officer
# Technology Transfer and Intellectual Property Services (TTIPS)
# University of California, San Diego
# 9500 Gilman Drive
# La Jolla, CA 92093-0910
# phone:858-822-5128, fax: 858-534-7345
# e-mail:wjdecker@@ucsd.edu
# 
# IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR
# DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING
# LOST PROFITS, ARISING OUT OF THE USE OF THIS CATALINA SCHEDULER PROGRAM,
# EVEN IF THE UNIVERSITY OF CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY
# OF SUCH DAMAGE.
# 
# THE CATALINA SCHEDULER PROGRAM PROVIDED HEREIN IS ON AN "AS IS" BASIS, AND
# THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT,
# UPDATES, ENHANCEMENTS, OR MODIFICATIONS.  THE UNIVERSITY OF CALIFORNIA MAKES
# NO REPRESENTATIONS AND EXTENDS NO WARRANTIES OF ANY KIND, EITHER IMPLIED OR
# EXPRESS, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
# MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, OR THAT THE USE OF
# THE CATALINA SCHEDULER PROGRAM WILL NOT INFRINGE ANY PATENT, TRADEMARK OR
# OTHER RIGHTS.

# Python module for Catalina Reservation System
# Hooks:
# - job_restriction,  screens jobs from running in the reservation
# - node_restriction, screens nodes from being used in the reservation
# - conflict_policy, finds and sorts open windows of all sizes
# - sort_policy, sorts nodes for allocation
# Users should be allowed access to job_restriction and node_restriction
# Rules for binding a job to a reservation:
#  - This can happen either at job submission with a comment line or
#  - at a later time, by calling the bind_job_to_reservation function
#  The comment line should be delimited by semicolons.  The reservation
#  binding should be in the form ...;\w*Catalina_res_bind=<res_id>:<res_id>:<res_id>...;


import fcntl
import traceback
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_job_steps_dict
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resources_list
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_configured_resources_list
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resource_dict_list
#from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resource_list
#from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resource_name_list
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import run_jobs
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_job_step_state
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import cancel_job
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import preempt_job
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import cancel_bad_jobs
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import initialize_job_step
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_scheduler_time
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import JOB_UPDATE_ATTRIBUTE_list
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import JOBSUFFIX
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import SUBMIT_OUTPUT_PATTERN

from operator import itemgetter
import copy
import string
import time
import sys
import math
import re
import cPickle
import stat
import statvfs
import os
import shutil
import pwd
import grp
import ConfigParser
import gc


gc.enable()

mandatory_dict = {
  'SERVERMODE' : None,
  'DEBUG' : None,
  'DEFAULT_JOB_CLASS' : None,
  'NODERESTCODE_STRING' : None,
  'HOMEDIR' : None,
  'ARCHIVE_DIR' : None,
  'CONFIGFILE' : None,
  'CONFIGURATION_DB' : None,
  'CONFIGURED_RESOURCES_DB' : None,
  'EVENTS_DB' : None,
  'JOBS_DB' : None,
  'OLD_JOBS_DB' : None,
  'OLD_RESERVATIONS_DB' : None,
  'RESERVATIONS_DB' : None,
  'RESOURCE_DB' : None,
  'STANDING_RESERVATIONS_DB' : None,
  'FUDGE_FACTOR' : None,
  'MAXJOBOVERRUN' : None,
  'MAXPRIORITY' : None,
  'NODE_SORT_POLICY_CODE_FILE' : None,
  'RESERVATION_DEPTH' : None,
  'DBSIZE_LIMIT' : None,
  'MAIL_RECIPIENT' : None,
  'MAILX' : None,
  'ECHO' : None,
  'LOCK_SUFFIX' : None,
  'CAT_LOCK_OWNER' : None,
  'CAT_LOCK_GROUP' : None,
  'JOB_START_TIME_LIMIT' : None,
  'RESOURCE_DOWN_TIME_LIMIT' : None,
  'MAXJOBPERUSERPOLICY' : None,
  'MAXJOBPERUSERCOUNT' : None,
  'MAXJOBQUEUEDPERUSERPOLICY' : None,
  'MAXJOBQUEUEDPERUSERCOUNT' : None,
  'BADRESOURCELIST' : None,
  'RESOURCE_WEIGHT' : None,
  'EXPANSION_FACTOR_WEIGHT' : None,
  'SYSTEM_QUEUE_TIME_WEIGHT' : None,
  'SUBMIT_TIME_WEIGHT' : None,
  'FAIRSHARE_BONUS_WEIGHT' : None,
  'QOS_PRIORITY_WEIGHT' : None,
  'QOS_TARGET_EXPANSION_FACTOR_WEIGHT' : None,
  'QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT' : None,
  'QOS_PRIORITY_STRING' : None,
  'QOS_MAX_PRIORITY_STRING' : None,
  'QOS_TARGETXF_STRING' : None,
  'QOS_TARGETQT_STRING' : None,
  'TEST_SHORTPOOL_AMOUNT' : None,
  'TEST_SHORTPOOL_SPEC' : None,
  'TEST_SHORTPOOL_DURATION' : None,
  'TEST_STANDING_AMOUNT' : None,
  'TEST_STANDING_SPEC' : None,
  'TEST_STANDING_DURATION' : None,
  'TEST_USERRES_AMOUNT' : None,
  'TEST_USERRES_MOD_AMOUNT' : None,
  'TEST_USERRES_END' : None,
  'TEST_NOEARLIEST_AMOUNT' : None,
  'TEST_JOB' : None,
  'SUBMITCMD' : None,
  'CANCELCMD' : None,
  'RM_TO_CAT_RESOURCE_DICT_STRING' : None,
  'RM_TO_CAT_JOB_DICT_STRING' : None
  }

# In the future, could put this in a function, so
# that clients can specify alternate config files.
HOMEDIR = '___HOMEDIR_PLACEHOLDER___'
DBDIR = '___DBDIR_PLACEHOLDER___'
CONFIGFILE = HOMEDIR + '/' + 'catalina.config'
defaults_dict = {
  'SERVERMODE' : 'TEST',
  'HOMEDIR' : '___HOMEDIR_PLACEHOLDER___',
  'DBDIR' : '___DBDIR_PLACEHOLDER___'
  }

config = ConfigParser.ConfigParser(defaults_dict)
config.read(CONFIGFILE)

options_list = config.options('main')
found_list = []
# string.upper is used here, because config.optionxform = str
# didn't work...maybe needs to be done differently...
for option in options_list :
    upper_option = string.upper(option)
    sys.__dict__['modules']['Catalina'].__dict__[upper_option] = config.get('main', option)
    found_list.append(upper_option)
    if mandatory_dict.has_key(upper_option) :
        del mandatory_dict[upper_option]
mandatory_keys_left = mandatory_dict.keys()
if len(mandatory_keys_left) > 0 :
    print "Mandatory option(s) not found: (%s)!" % mandatory_keys_left
    sys.exit(1)

print "SERVERMODE (%s)" % SERVERMODE
if not vars().has_key('QJ_SIM') :
    QJ_SIM = None
if not vars().has_key('QM_SIM') :
    QM_SIM = None
DEBUGJOB = None
FUDGE_FACTOR = float(FUDGE_FACTOR)
if not vars().has_key('MACHINE_REFRESH_INTERVAL') :
    MACHINE_REFRESH_INTERVAL = 1
else :
    try :
        MACHINE_REFRESH_INTERVAL = int(MACHINE_REFRESH_INTERVAL)
    except :
        print "Could not convert MACHINE_REFRESH_INTERVAL (%s) to int.  Using 1!" % MACHINE_REFRESH_INTERVAL
        MACHINE_REFRESH_INTERVAL = 1
MAXJOBOVERRUN = float(MAXJOBOVERRUN)
MAXPRIORITY = long(MAXPRIORITY)
if RESERVATION_DEPTH == 'None' :
    RESERVATION_DEPTH = None
else :
    RESERVATION_DEPTH = int(RESERVATION_DEPTH)
DBSIZE_LIMIT = int(DBSIZE_LIMIT)
if not vars().has_key('FORCETZ') or FORCETZ == '' :
    FORCETZ = 'NOFORCE'
if FORCETZ != 'NOFORCE' :
    print "forcing TZ to (%s)" % FORCETZ
    os.environ['TZ'] = FORCETZ
try :
    time.tzset()
except :
    TZSET = None
if not vars().has_key('LOGGER') :
    LOGGER = '/bin/logger'
if not vars().has_key('LOGGER_FACILITY') :
    LOGGER_FACILITY = 'daemon'
if not vars().has_key('RUN_AT_RISK_CLEANUP_TIME') :
    RUN_AT_RISK_CLEANUP_TIME = 300.0
else :
    try :
        RUN_AT_RISK_CLEANUP_TIME = float(RUN_AT_RISK_CLEANUP_TIME)
    except :
        RUN_AT_RISK_CLEANUP_TIME = 300.0
if not vars().has_key('RUN_AT_RISK_MIN_RUNTIME') :
    RUN_AT_RISK_MIN_RUNTIME = 900.0
else :
    try :
        RUN_AT_RISK_MIN_RUNTIME = float(RUN_AT_RISK_MIN_RUNTIME)
    except :
        RUN_AT_RISK_MIN_RUNTIME = 900.0
if not vars().has_key('PREEMPT_CLEANUP_TIME') :
    PREEMPT_CLEANUP_TIME = 300.0
else :
    try :
        PREEMPT_CLEANUP_TIME = float(PREEMPT_CLEANUP_TIME)
    except :
        PREEMPT_CLEANUP_TIME = 300.0
if not vars().has_key('PREEMPT_MIN_RUNTIME') :
    PREEMPT_MIN_RUNTIME = 1800.0
else :
    try :
        PREEMPT_MIN_RUNTIME = float(PREEMPT_MIN_RUNTIME)
    except :
        PREEMPT_MIN_RUNTIME = 1800.0
if not vars().has_key('JOB_START_WARN_LIMIT') :
    JOB_START_WARN_LIMIT = None
else :
    JOB_START_WARN_LIMIT = int(JOB_START_WARN_LIMIT)
DB_WARN_COUNT = 0
if not vars().has_key('DB_WARN_LIMIT') :
    DB_WARN_LIMIT = None
else :
    DB_WARN_LIMIT = int(DB_WARN_LIMIT)
JOB_START_TIME_LIMIT = float(JOB_START_TIME_LIMIT)
if not sys.__dict__['modules']['Catalina'].__dict__.has_key('LOST_JOB_WARN') :
    LOST_JOB_WARN = 'FALSE'
if sys.__dict__['modules']['Catalina'].__dict__.has_key('LOST_JOB_LIMIT') :
    try :
        LOST_JOB_LIMIT = float(LOST_JOB_LIMIT)
    except :
        LOST_JOB_LIMIT = None
else :
    LOST_JOB_LIMIT = None
RESOURCE_DOWN_TIME_LIMIT = float(RESOURCE_DOWN_TIME_LIMIT)
if not vars().has_key('FIFOSCREEN') :
    FIFOSCREEN = None
MAXJOBPERUSERCOUNT = int(MAXJOBPERUSERCOUNT)
MAXJOBQUEUEDPERUSERCOUNT = int(MAXJOBQUEUEDPERUSERCOUNT)

priority_weights_tuple = (
  'RESOURCE_WEIGHT',
  'EXPANSION_FACTOR_WEIGHT',
  'SYSTEM_QUEUE_TIME_WEIGHT',
  'SUBMIT_TIME_WEIGHT',
  'FAIRSHARE_BONUS_WEIGHT',
  'LOCAL_USER_WEIGHT',
  'LOCAL_ADMIN_WEIGHT',
  'WALL_TIME_WEIGHT',
  'QOS_PRIORITY_WEIGHT',
  'QOS_TARGET_EXPANSION_FACTOR_WEIGHT',
  'QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT'
  )
max_places = 0
dec_pattern = r"\d*\.(?P<fraction>\d+)"
dec_reo = re.compile(dec_pattern)
for priority_weight in priority_weights_tuple :
    if sys.__dict__['modules']['Catalina'].__dict__.has_key(priority_weight) :
        this_weight_string = sys.__dict__['modules']['Catalina'].__dict__[priority_weight]
        prio_mo = dec_reo.match(this_weight_string)
        if prio_mo != None :
            this_places = len(prio_mo.group('fraction'))
            if this_places > max_places :
                max_places = this_places
if max_places == 0 :
    max_places_float = 1.0
else :
    max_places_float = pow(10,max_places)
RESOURCE_WEIGHT = long(float(RESOURCE_WEIGHT) * max_places_float)
if sys.__dict__['modules']['Catalina'].__dict__.has_key('LOCAL_ADMIN_WEIGHT') :
    try :
        LOCAL_ADMIN_WEIGHT = long(float(LOCAL_ADMIN_WEIGHT) * max_places_float)
    except :
        LOCAL_ADMIN_WEIGHT = 0L
else :
    LOCAL_ADMIN_WEIGHT = 0L
if sys.__dict__['modules']['Catalina'].__dict__.has_key('LOCAL_USER_WEIGHT') :
    try :
        LOCAL_USER_WEIGHT = long(float(LOCAL_USER_WEIGHT) * max_places_float)
    except :
        LOCAL_USER_WEIGHT = 0L
else :
    LOCAL_USER_WEIGHT = 0L
if sys.__dict__['modules']['Catalina'].__dict__.has_key('WALL_TIME_WEIGHT') :
    try :
        WALL_TIME_WEIGHT = long(float(WALL_TIME_WEIGHT) * max_places_float)
    except :
        WALL_TIME_WEIGHT = 0L
else :
    WALL_TIME_WEIGHT = 0L
EXPANSION_FACTOR_WEIGHT = long(float(EXPANSION_FACTOR_WEIGHT) * max_places_float)
SYSTEM_QUEUE_TIME_WEIGHT = long(float(SYSTEM_QUEUE_TIME_WEIGHT)* max_places_float)
SUBMIT_TIME_WEIGHT = long(float(SUBMIT_TIME_WEIGHT) * max_places_float)
#feature 582
FAIRSHARE_BONUS_WEIGHT = long(float(FAIRSHARE_BONUS_WEIGHT) * max_places_float)
QOS_PRIORITY_WEIGHT = long(float(QOS_PRIORITY_WEIGHT) * max_places_float)
QOS_TARGET_EXPANSION_FACTOR_WEIGHT = long(float(QOS_TARGET_EXPANSION_FACTOR_WEIGHT) * max_places_float)
QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT = long(float(QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT) * max_places_float)

DEFAULT_PROC_CHARGE = float(DEFAULT_PROC_CHARGE)
TEST_SHORTPOOL_AMOUNT = int(TEST_SHORTPOOL_AMOUNT)
TEST_SHORTPOOL_DURATION = int(TEST_SHORTPOOL_DURATION)
TEST_STANDING_AMOUNT = int(TEST_STANDING_AMOUNT)
TEST_STANDING_DURATION = int(TEST_STANDING_DURATION)
TEST_USERRES_AMOUNT = int(TEST_USERRES_AMOUNT)
TEST_USERRES_MOD_AMOUNT = int(TEST_USERRES_MOD_AMOUNT)
TEST_USERRES_END = int(TEST_USERRES_END)
TEST_BINDING_AMOUNT = int(TEST_BINDING_AMOUNT)
TEST_NOEARLIEST_AMOUNT = int(TEST_NOEARLIEST_AMOUNT)
exec('QOS_PRIORITY_dict = ' + QOS_PRIORITY_STRING)
exec('QOS_PRIORITY_dict = ' + QOS_PRIORITY_STRING)
exec('QOS_MAX_PRIORITY_dict = ' + QOS_MAX_PRIORITY_STRING)
exec('QOS_TARGETXF_dict = ' + QOS_TARGETXF_STRING)
exec('QOS_TARGETQT_dict = ' + QOS_TARGETQT_STRING)
exec('QOS_MAXJOBPERUSERPOLICY_dict = ' + QOS_MAXJOBPERUSERPOLICY_STRING)
exec('QOS_MAXJOBQUEUEDPERUSERPOLICY_dict = ' + QOS_MAXJOBQUEUEDPERUSERPOLICY_STRING)
if vars().has_key('QOS_MAXJOBPERACCOUNTPOLICY_STRING') :
    exec('QOS_MAXJOBPERACCOUNTPOLICY_dict = ' + QOS_MAXJOBPERACCOUNTPOLICY_STRING)
else :
    QOS_MAXJOBPERACCOUNTPOLICY_dict = {}
if vars().has_key('QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_STRING') :
    exec('QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict = ' + QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_STRING)
else :
    QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict = {}
if vars().has_key('QOS_MAXNODESECQUEUEDPERUSERPOLICY_STRING') :
    exec('QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict = ' + QOS_MAXNODESECQUEUEDPERUSERPOLICY_STRING)
else :
    QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict = {}
if vars().has_key('QOS_MAXNODESECRUNNINGPERUSERPOLICY_STRING') :
    exec('QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict = ' + QOS_MAXNODESECRUNNINGPERUSERPOLICY_STRING)
else :
    QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict = {}
if vars().has_key('QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_STRING') :
    exec('QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict = ' + QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_STRING)
else :
    QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict = {}
if vars().has_key('QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_STRING') :
    exec('QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict = ' + QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_STRING)
else :
    QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict = {}
exec('RM_TO_CAT_RESOURCE_dict = ' + RM_TO_CAT_RESOURCE_DICT_STRING)
exec('RM_TO_CAT_JOB_dict = ' + RM_TO_CAT_JOB_DICT_STRING)
if vars().has_key('USER_SET_LIMITS_DICT_STRING') :
    exec('USER_SET_LIMITS_dict = ' + USER_SET_LIMITS_DICT_STRING)
if vars().has_key('CLASS_PRIORITY_DICT_STRING') :
    exec('CLASS_PRIORITY_dict = ' + CLASS_PRIORITY_DICT_STRING)
NODERESTCODE = re.sub("\n ","\n", NODERESTCODE_STRING)

#if SERVERMODE == 'SIM' :
#    Now_float = float(os.system(TIME_SIM))
#else :
#    Now_float = time.time()
Now_float = get_scheduler_time()
END_OF_SCHEDULING = Now_float + 7776000
FIRST_AVAILABLE = """
import string
def sortfunc (first, second) :
    if first[0] < second[0] :
        return -1
    if first[0] == second[0] :
        return 0
    if first[0] > second[0] :
        return 1
windows = input_tuple[0]
nodes = input_tuple[1]
new_res = input_tuple[2]
windows.sort(sortfunc)
result = windows
"""
LAST_AVAILABLE = """
import string
def sortfunc (first, second) :
    if first[0] > second[0] :
        return -1
    if first[0] == second[0] :
        return 0
    if first[0] < second[0] :
        return 1
windows = input_tuple[0]
nodes = input_tuple[1]
new_res = input_tuple[2]
windows.sort(sortfunc)
result = windows
"""
# This policy is used by standing reservations to choose
# nodes.  If we are in the standing reservation period,
# the code should choose first nodes that have jobs running
# on them.  This is to prevent undesired shortpool behaviour.
# If the standing reservation instance is in the future, then
# we want to choose the last available, in order to keep our
# backfill window as large as possible.
LAST_AVAILABLE_IGNORED_FIRST = """
import string
def sortfunc_immediate (first, second) :
    if DEBUGJOB != None :
        print "first[1] (%s), second[1] (%s)" % (first[1], second[1])
    if first[1] == 'Running' and second[1] == 'Idle' :
        if DEBUGJOB != None :
            print "first[1] == 'Running' and second[1] == 'Idle', returning -1"
        return -1
    if first[1] == 'Idle' and second[1] == 'Running' :
        if DEBUGJOB != None :
            print "first[1] == 'Idle' and second[1] == 'Running', returning 1"
        return 1
    if first[0][0] > second[0][0] :
        return -1
    if first[0][0] == second[0][0] :
        return 0
    if first[0][0] < second[0][0] :
        return 1
def sortfunc_future (first, second) :
    #if first[0][2] in first[2] and not second[0][2] in second[2] :
    #    return -1
    #if second[0][2] in second[2] and not first[0][2] in first[2] :
    #    return 1
    if first[0][0] > second[0][0] :
        return -1
    if first[0][0] == second[0][0] :
        return 0
    if first[0][0] < second[0][0] :
        return 1
windows = input_tuple[0]
nodes = input_tuple[1]
new_res = input_tuple[2]
resources_dict = input_tuple[5]
windows_state_list = []
#original_nodes = ___ORIGINAL_NODE_PLACEHOLDER___
future = 0
for window in windows :
    windows_state_list.append((window, resources_dict[window[2]]['State']))
    #print "window[0] (%s) window[1] (%s) window[2] (%s)" % (time.asctime(time.localtime(window[0])), time.asctime(time.localtime(window[1])), window[2])
    if window[0] > Now_float :
        future = 1
if future == 0 :
    #print "using sortfunc_immediate"
    windows_state_list.sort(sortfunc_immediate)
else :
    #print "using softfunc_future"
    windows_state_list.sort(sortfunc_future)
windows = []
for window_tuple in windows_state_list :
    windows.append(window_tuple[0])
result = windows
"""

username_string = pwd.getpwuid(os.getuid())[0]

def initialize_reservation(reservation_name) :
    new_reservation = {}
    new_reservation['name'] = reservation_name
    new_reservation['job_restriction'] = None
    new_reservation['job_binding'] = None
    new_reservation['purpose_type_string'] = None
    new_reservation['earliest_start_float'] = None
    new_reservation['latest_end_float'] = None
    new_reservation['duration_float'] = None
    new_reservation['latency_float'] = None
    new_reservation['resource_amount_int'] = None
    new_reservation['purpose_type_string'] = None
    new_reservation['account_string'] = None
    new_reservation['creator_string'] = None
    new_reservation['comment_string'] = None
    new_reservation['conflict_policy'] = None
    new_reservation['node_sort_policy'] = None
    new_reservation['node_restriction'] = None
    new_reservation['node_usage'] = None
    new_reservation['resource_dict_list'] = None
    new_reservation['affinity_calculation'] = None
    new_reservation['job_runID'] = None
    new_reservation['start_time_float'] = None
    new_reservation['end_time_float'] = None
    new_reservation['node_list'] = None
    new_reservation['resource_amount_int'] = None
    new_reservation['start_spec_string'] = None
    new_reservation['ignore_reservations_list'] = None
    new_reservation['overlap_running_int'] = 0
    new_reservation['start_count_int'] = 0
    new_reservation['notify_count_int'] = 0
    new_reservation['notify_string'] = None
    new_reservation['allocated_dict_list'] = None
    return new_reservation

def get_new_db_key(db_handle) :
    shelf = db_handle[0]
    new_id_number_float = math.floor(Now_float)
    new_id_string = "%i" % new_id_number_float
    while shelf.has_key(new_id_string) :
        new_id_number_float = new_id_number_float + 1
        new_id_string = "%i" % new_id_number_float
    return new_id_string

def get_file_code(filename_string) :
    input = open(filename_string, 'r')
    file_string = input.read()
    input.close()
    return file_string

def lock_db_files (db_name, db_mode='read') :
    LOCK = DBDIR + '/' + db_name + LOCK_SUFFIX
    print "Waiting for %s db lock" % db_name
    if db_mode == 'write' :
        # open the lock file for append, set exclusive lock
        LOCKFO = open(LOCK, 'a')
        fcntl.lockf(LOCKFO.fileno(), fcntl.LOCK_EX)
    else :
        # open the reservations file for read, set non-exclusive lock
        LOCKFO = open(LOCK, 'r')
        fcntl.lockf(LOCKFO.fileno(), fcntl.LOCK_SH)
    print "Received %s db lock" % db_name
    db_tuple = ( LOCKFO, db_name, db_mode )
    return db_tuple

def unlock_db_files (db_tuple) :
    db_name = db_tuple[1]
    LOCK = DBDIR + '/' + db_name + LOCK_SUFFIX
    LOCKFO = db_tuple[0]
    uid_int = pwd.getpwnam(CAT_LOCK_OWNER)[2]
    gid_int = grp.getgrnam(CAT_LOCK_GROUP)[2]
    try :
        os.chmod(LOCK,0664)
        os.chown(LOCK,uid_int,gid_int)
    except :
        try :
            if DEBUG == 'locks' :
                info_tuple = sys.exc_info()
                print "(%s) (%s) (%s)" % info_tuple
                info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                traceback.print_tb(info_tuple[2])
                tb_list = traceback.format_tb(info_tuple[2])
                info_list = info_list + tb_list
                tb_text = string.join(info_list)
                print tb_text
        except :
            print "print of sys.exc_info() failed!"
    # release lock, close the file
    fcntl.lockf(LOCKFO.fileno(), fcntl.LOCK_UN)
    print "Released %s db %s lock" % (db_tuple[1], db_tuple[2])
    LOCKFO.close()

def initialize_db(db_name) :
    LOCK = DBDIR + '/' + db_name + LOCK_SUFFIX
    LOCKFO = open(LOCK, 'a')
    LOCKFO.close()
    lock_uid = pwd.getpwnam(CAT_LOCK_OWNER)[2]
    lock_gid = grp.getgrnam(CAT_LOCK_GROUP)[2]
    os.chown(LOCK, lock_uid, lock_gid)
    os.chmod(LOCK, 0664)
    db_tuple = lock_db_files(db_name, db_mode='write')
    FILE = DBDIR + '/' + db_name
    FO = open(FILE, 'wb')
    empty_db = {}
    
    # high performance pickler in binary mode, with fast mode
    pickle = cPickle.Pickler(FO,1)
    pickle.fast = 1
    
    pickle.dump(empty_db)
    FO.close()
    os.chown(FILE, lock_uid, lock_gid)
    os.chmod(FILE, 0664)
    unlock_db_files(db_tuple)
    RO_DB_NAME = db_name + '_readonly'
    ROLOCK = DBDIR + '/' + RO_DB_NAME + LOCK_SUFFIX
    ROLOCKFO = open(ROLOCK, 'a')
    ROLOCKFO.close()
    os.chown(ROLOCK, lock_uid, lock_gid)
    os.chmod(ROLOCK, 0664)
    ro_db_tuple = lock_db_files(RO_DB_NAME, db_mode='write')
    ROFILE = DBDIR + '/' + RO_DB_NAME
    ROFO = open(ROFILE, 'wb')
    empty_db = {}
    
    # high performance pickler in binary mode, with fast mode
    pickle = cPickle.Pickler(ROFO,1)
    pickle.fast = 1
    
    pickle.dump(empty_db)
    ROFO.close()
    os.chown(ROFILE, lock_uid, lock_gid)
    os.chmod(ROFILE, 0664)
    unlock_db_files(ro_db_tuple)

def open_db (db_name, db_mode) :
    db_tuple = lock_db_files(db_name, db_mode)
    REMOTEFILE = DBDIR + '/' + db_name
    global DB_WARN_COUNT
    try :
        nonempty = 0
        for i in range(60) :
            FO = open(REMOTEFILE, 'rb')
            contents = FO.read()
            if len(contents) == 0 :
                FO.close()
                continue
            else :
                contents = None
                FO.seek(0)
                nonempty = 1
                break
        contents = None
        if nonempty == 1 :
            good_pickle = 0
            for i in range(60) :
                try :
                    dict = cPickle.load(FO)
                except :
                    if i == 59 :
                        info_tuple = sys.exc_info()
                        info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                        tb_list = traceback.format_tb(info_tuple[2])
                        info_list = info_list + tb_list
                        tb_text = string.join(info_list)
                        recipient = MAIL_RECIPIENT
                        subject = "Attempting to correct DB corruption"
                        message = """Catalina database corruption detected for
db: (%s)""" % REMOTEFILE + '\n' + tb_text
                        print "message (%s)" % message
                        warn(message, subject, recipient)
                        catsyslog(message,'warning')
                    FO.seek(0)
                    continue
                else :
                    good_pickle = 1
                    break
            if good_pickle == 0 :
                FO.close()
                raise 'BadPickle', "open of writable db failed, 60x bad pickle"
            FO.close()
        else :
            FO.close()
            raise 'EmptyDBFile', "open of writable db failed, 60x empty file"
    except :
        if DB_WARN_LIMIT == None or DB_WARN_COUNT < DB_WARN_LIMIT :
            info_tuple = sys.exc_info()
            info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
            tb_list = traceback.format_tb(info_tuple[2])
            info_list = info_list + tb_list
            tb_text = string.join(info_list)
            recipient = MAIL_RECIPIENT
            subject = "Attempting to correct DB corruption"
            message = """Catalina database corruption detected for
db: (%s)""" % REMOTEFILE + '\n' + tb_text
            print "message (%s)" % message
            warn(message, subject, recipient)
            catsyslog(message,'warning')
            DB_WARN_COUNT = DB_WARN_COUNT + 1
        try :
            RO_DB_NAME = db_name + '_readonly'
            ROFILE = DBDIR + '/' + RO_DB_NAME
            FO = open(ROFILE, 'rb')
            print "uncollectable objects (%s)" % gc.collect()
            dict = cPickle.load(FO)
            FO.close()
            FO = open(REMOTEFILE, 'wb')
            pickle = cPickle.Pickler(FO,1)
            pickle.fast = 1
            pickle.dump(dict)
            FO.flush()
            FO.close()
            FO = open(REMOTEFILE, 'rb')
            dict = cPickle.load(FO)
            FO.close()
        except :
            try :
                info_tuple = sys.exc_info()
                print "(%s) (%s) (%s)" % info_tuple
                info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                traceback.print_tb(info_tuple[2])
                tb_list = traceback.format_tb(info_tuple[2])
                info_list = info_list + tb_list
                tb_text = string.join(info_list)
                print tb_text
                if DB_WARN_LIMIT == None or DB_WARN_COUNT < DB_WARN_LIMIT :
                    recipient = MAIL_RECIPIENT
                    subject = "Failed to correct DB corruption"
                    message = """Catalina database corruption could not be fixed for
    db: (%s)""" % REMOTEFILE
                    warn(message, subject, recipient)
                    catsyslog(message,'warning')
            except :
                print "print of sys.exc_info() failed!"
            if DB_WARN_LIMIT == None or DB_WARN_COUNT < DB_WARN_LIMIT :
                recipient = MAIL_RECIPIENT
                subject = "Failed to correct DB corruption"
                message = """Catalina database corruption could not be fixed for
db: (%s)""" % REMOTEFILE
                warn(message, subject, recipient)
                catsyslog(message,'warning')
                DB_WARN_COUNT = DB_WARN_COUNT + 1
            raise 'CatalinaDBError', "open of writable db failed, uncorrectable"
        else :
            if DB_WARN_LIMIT == None or DB_WARN_COUNT < DB_WARN_LIMIT :
                recipient = MAIL_RECIPIENT
                subject = "DB recovery completed"
                message = """Catalina database recovery completed for
db: (%s)
The _readonly verion of the db has been copied back.
Some information from the most recent scheduling iteration
may have been lost.""" % REMOTEFILE
                warn(message, subject, recipient)
                catsyslog(message,'notice')
                DB_WARN_COUNT = 0
    db_handle = ( dict, db_tuple )
    return db_handle

def open_ro_db (db_name, db_mode) :
    RO_DB_NAME = db_name + '_readonly'
    db_tuple = (None, db_name, 'read')
    ROFILE = DBDIR + '/' + RO_DB_NAME
    ROFO = None
    tries = 0
    while tries < 60 :
        try :
            ROFO = open(ROFILE, 'rb')
            dict = cPickle.load(ROFO)
            ROFO.close()
        except :
            if ROFO != None :
                ROFO.close()
            print "Error retrying"
            time.sleep(1)
            print "after time.sleep(1)"
            tries = tries + 1
        else :
            break
    if tries >= 60 :
        raise 'CatalinaDBError', "open of read-only db failed"
    db_handle = ( dict, db_tuple )
    return db_handle

def close_ro_db (db_handle) :
    dict = db_handle[0]
    db_tuple = db_handle[1]
    db_name = db_tuple[1]
    db_tuple = db_handle[1]

def close_db (db_handle) :
    dict = db_handle[0]
    db_tuple = db_handle[1]
    db_name = db_tuple[1]
    db_mode = db_tuple[2]
    db_tuple = db_handle[1]
    if db_mode == 'write' :
        REMOTEFILE = DBDIR + '/' + db_name
        FO = open(REMOTEFILE, 'wb')
        pickle = cPickle.Pickler(FO,1)
        pickle.fast = 1
        if DEBUG == 'cd' :
            print "writing out (%s)" % db_name
        pickle.dump(dict)
        FO.flush()
        FO.close()
        ROFILE = DBDIR + '/' + db_name + '_readonly'
        shutil.copyfile(REMOTEFILE, ROFILE)
        uid_int = pwd.getpwnam(CAT_LOCK_OWNER)[2]
        gid_int = grp.getgrnam(CAT_LOCK_GROUP)[2]
        try :
            os.chmod(REMOTEFILE,0664)
            os.chmod(ROFILE,0664)
            os.chown(REMOTEFILE,uid_int,gid_int)
            os.chown(ROFILE,uid_int,gid_int)
        except :
            try :
                if DEBUG == 'locks' :
                    info_tuple = sys.exc_info()
                    print "(%s) (%s) (%s)" % info_tuple
                    info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                    traceback.print_tb(info_tuple[2])
                    tb_list = traceback.format_tb(info_tuple[2])
                    info_list = info_list + tb_list
                    tb_text = string.join(info_list)
                    print tb_text
            except :
                print "print of sys.exc_info() failed!"
        # stat of the file seems to sync it up in NFS...
        file_stat = os.stat(REMOTEFILE)
        file_stat = os.stat(ROFILE)
    unlock_db_files(db_tuple)

def apply_policy_code(code_string,input_tuple) :
    result = None
    # if code_string is compiled code, result does not get set...
    #print "code_string (%s)" % code_string
    exec code_string
    #print "result (%s) for input_tuple[0] (%s)" % (result, input_tuple[0])
    return result

def get_broken_reservations_tuple(
  reservations_db_handle,
  resource_db_handle,
  jobs_db_handle,
  res_id=None) :
    exit_status = 0
    # Retrieve a dictionary of reservations:
    reservations_dict = reservations_db_handle[0]
    if res_id == None :
        check_list = filter(lambda x : x['purpose_type_string'] in ['generic', 'user_set'], get_object_list(reservations_db_handle))
    else :
        if not reservations_dict.has_key(res_id) :
            raise 'KeyNotFound', res_id
        check_list = [reservations_dict[res_id],]
    reservations_list = get_object_list(reservations_db_handle)
    jobs_dict = jobs_db_handle[0]
    report_dict = {}
    exit_status = 0
    for res in check_list :
        report_dict[res['name']] = {'overlap_dict' : {}, 'down_nodes_list' : [] , 'reservation' : None}
        report_dict[res['name']]['reservation'] = res
        # initialize overlap dictionary.  key is reservation name, value
        # is reservation
        overlap_dict = {}
        time_overlap_list = filter(
          lambda x, res=res : \
          ((res['start_time_float'] <= x['start_time_float'] < res['end_time_float']) or\
           (res['start_time_float'] < x['end_time_float'] <= res['end_time_float']) or \
           (x['start_time_float'] <= res['start_time_float'] < x['end_time_float']) or \
           (x['start_time_float'] < res['end_time_float'] < x['end_time_float'])) \
           and x['name'] != res['name'],
          reservations_list
          )
        for reservation in time_overlap_list :
            for node in reservation['node_list'] :
                if node not in res['node_list'] :
                    continue
                if reservation['purpose_type_string'] == 'job' :
                    continue
                if reservation['purpose_type_string'] == 'running' :
                    runID_string = reservation['job_runID']
                    job_step = jobs_dict[runID_string]
                    input_tuple = ( job_step, )
                    result = apply_policy_code(res['job_restriction'],
                      input_tuple)
                    if result == 0 :
                        continue
                overlap_dict[reservation['name']] = reservation
        if len(overlap_dict.keys()) > 0 :
            report_dict[res['name']]['overlap_dict'] = overlap_dict
        # Check for down nodes
        resource_list = get_object_list(resource_db_handle)
        down_nodes_list = filter(lambda x, res=res : \
          x['name'] in res['node_list'] and x['State'] not in ['Idle','Running'], \
          resource_list)
        if len(down_nodes_list) > 0 :
            report_dict[res['name']]['down_nodes_list'] = down_nodes_list
        if len(overlap_dict.keys()) > 0 or len(down_nodes_list) > 0 :
            exit_status = 1
    return (report_dict, exit_status)

def get_accepted_nodes_list(node_restriction_code, resource_db_handle) :
    #normal_re = re.compile(".*normal.*")
    dict = resource_db_handle[0]
    resource_list = dict.values()
    accepted_nodes_list = []
    for resource in resource_list :
        if node_restriction_code != None :
            #print "node_restriction_code (%s)" % node_restriction_code
            input_tuple = (resource,)
            acceptance = apply_policy_code(node_restriction_code, input_tuple)
            #print "acceptance (%s) for resource (%s)" % (acceptance,resource['name'])
        elif resource['State'] == 'Down' :
            acceptance = 'Down'
        elif resource['State'] == 'Drain' :
            acceptance = 'Drain'
        elif resource['State'] == 'Drained' :
            acceptance = 'Drained'
        elif resource['State'] == 'None' :
            acceptance = 'None'
        elif resource['State'] == None :
            acceptance = None
        elif resource['State'] == 'Unknown' :
            acceptance = 'Unknown'
        #elif resource['Max_Starters'] == 0 :
        #    acceptance = 'Max_Starters=0'
        #elif normal_re.search(resource['ConfiguredClasses']) == None :
        #    acceptance = 'NoNormalClass'
        else :
            acceptance = 0
        if acceptance != 0 :
            continue
        accepted_nodes_list.append(resource['name'])
    return accepted_nodes_list

def get_object_list(db_handle) :
    dict = db_handle[0]
    list = dict.values()
    return list

def get_object_names_list(object_list) :
    def get_name(object) : return object['name']
    list = map( get_name, object_list )
    return list

def get_object(key, db_handle) :
    dict = db_handle[0]
    if dict.has_key(key) :
        return dict[key]
    else :
        raise 'KeyNotFound', key

#def get_screened_nodes(job_step_id, accepted_nodes_list, jobs_db_handle, resources_db_handle) :
#    jobs_dict = jobs_db_handle[0]
#    job_step = jobs_dict[job_step_id]
#    if job_step.has_key('resource_list') :
#        screened_resource_list = job_step['resource_list']
#    else :
#        raise 'NoResourceList', job_step
#    screened_resource_name_list = get_resource_name_list(screened_resource_list)
#    screened_nodes_list = []
#    for node in accepted_nodes_list :
#        if node in screened_resource_name_list :
#            screened_nodes_list.append(node)
#    return screened_nodes_list

def get_open_windows_list(accepted_nodes_list, new_res, temp_reservations_list, resources_db_handle) :
    def get_nonconflicting(input_tuple) :
        # This python code fragment can be used to obtain a list of available
        # node reservation windows.
        # It works on input_tuple containing
        # ( <nodes matching requirements>, <the requested reservation>,
        #   <all existing reservations> , Now)
        # returns a list, each element of which is a tuple:
        # ( <window start_time_float>, <window end_time_float>, <node name> )
        # To accomodate cpu and memory scheduling, need to return
        # ( <window start_time_float>, <window end_time_float>, { 'nodename' : <node name>, 'cpu' : <cpus>, 'memory' : <memory>} )
        def sort_by_start(first, second) :
            if first[0] < second[0] :
                return -1
            if first[0] == second[0] :
                return 0
            if first[0] > second[0] :
                return 1
        accepted_nodes = input_tuple[0]
        new_res = input_tuple[1]
        reservations = input_tuple[2]
        resources_db_handle = input_tuple[4]
        resources_dict = resources_db_handle[0]
        end_limit = END_OF_SCHEDULING
        reservation_windows = []
        node_reservations = {}
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "len(accepted_nodes) (%s)" % len(accepted_nodes)
        #if new_res['purpose_type_string'] in ['preempted_job',] :
        #    print "reservations (%s)" % (reservations,)
        for reservation in reservations :
            #print "reservation (%s)" % reservation
            #if reservation['purpose_type_string'] == 'running' :
            #    print "reservation name (%s) purpose (%s) start (%s) end (%s)" % \
            #      (reservation['name'], reservation['purpose_type_string'], reservation['start_time_float'], reservation['end_time_float'])
            #    print "running reservation (%s)" % reservation
            #print "reservation name (%s) purpose (%s) start (%s) end (%s)" % \
            #  (reservation['name'], reservation['purpose_type_string'], reservation['start_time_float'], reservation['end_time_float'])
            res_id = reservation['name']
            node_list = reservation['node_list']
            if reservation.has_key('allocated_dict_list') and reservation['allocated_dict_list'] != None :
                allocated_dict_list = reservation['allocated_dict_list']
                #Kenneth patch leaking reservation. 7/24/06
                for allocated_dict in allocated_dict_list:
                    if allocated_dict.has_key('type') and allocated_dict['type'] == 'node_exclusive' and allocated_dict.has_key('nodename'):
                        if resources_dict[allocated_dict['nodename']].has_key('ConsumableCpus'):
                            allocated_dict['cpu'] = resources_dict[allocated_dict['nodename']]['ConsumableCpus']
                        if resources_dict[allocated_dict['nodename']].has_key('ConsumableMemory'):
                            allocated_dict['memory'] = resources_dict[allocated_dict['nodename']]['ConsumableMemory']
                #end of patch

                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "existing allocated_dict_list (%s)" % (allocated_dict_list,)
            else :
                allocated_dict_list = []
                for node in reservation['node_list'] :
                    #print "node (%s)" % node
                    if resources_dict.has_key(node) and resources_dict[node].has_key('ConsumableCpus') and resources_dict[node].has_key('ConsumableMemory') :
                        allocated_consumablecpus = resources_dict[node]['ConsumableCpus']
                        allocated_consumablememory = resources_dict[node]['ConsumableMemory']
                    else :
                        allocated_consumablecpus = 0
                        allocated_consumablememory = 0
                    allocated_dict_list.append(
                      { 'nodename' : node,
                        'type' : 'node_exclusive',
                        'node' : 1,
                        'cpu' : allocated_consumablecpus,
                        'memory' : allocated_consumablememory }
                      )
                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "generated allocated_dict_list (%s)" % (allocated_dict_list,)
            #print "allocated_dict_list (%s)" % (allocated_dict_list,)
            node_resource_list = filter(lambda x : x.has_key('nodename'), allocated_dict_list)
            #print "node_resource_list (%s)" % (node_resource_list,)
            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "node_resource_list (%s)" % (node_resource_list,)
            start_time_float = reservation['start_time_float']
            if reservation.has_key('end_time_float') :
                end_time_float = reservation['end_time_float']
            else :
                end_time_float = END_OF_SCHEDULING
            #print "node_resource_list (%s)" % (node_resource_list,)
            for index in range(len(node_resource_list)) :
                node = node_resource_list[index]['nodename']
                #print "doing node (%s)" % node
                if not resources_dict.has_key(node) or not node in accepted_nodes :
                    continue
                if not node_reservations.has_key(node) :
                    node_reservations[node] = []
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "node_reservations[node].append (%s)" % ( (0.0, 'increment', 
                      { 'nodename' : node, 'type' : 'node_exclusive',
                        'node' : 1,
                        'cpu' : resources_dict[node]['ConsumableCpus'],
                        'memory' : resources_dict[node]['ConsumableMemory']
                      }
                      ),
                      )
                    #print "node (%s)" % node
                    node_reservations[node].append( (0.0, 'add', 
                      { 'nodename' : node, 'type' : 'node_exclusive',
                        'node' : 1,
                        'cpu' : resources_dict[node]['ConsumableCpus'],
                        'memory' : resources_dict[node]['ConsumableMemory']
                      }
                      )
                      )
                    node_reservations[node].append( (END_OF_SCHEDULING, 'decrement', 
                      { 'nodename' : node, 'type' : 'node_exclusive',
                        'node' : 1,
                        'cpu' : resources_dict[node]['ConsumableCpus'],
                        'memory' : resources_dict[node]['ConsumableMemory']
                      } )
                      )
                if node in accepted_nodes :
                    #print "adding (%s) to node_reservations" % node
                    if node_reservations.has_key(node) :
                        #node_reservations[node].append( (start_time_float, end_time_float, node_resource_list[index]) )
                        #if reservation['purpose_type_string'] == 'running' :
                        #    print "appending running node_reservation (%s, %s, %s)" % (start_time_float, end_time_float, node)
                        node_reservations[node].append( (start_time_float, 'decrement', node_resource_list[index]) )
                        node_reservations[node].append( (end_time_float, 'add', node_resource_list[index]) )
                        #print "appending later node_reservation (%s, %s, %s)" % (start_time_float, end_time_float, node)
                    else :
                        #if reservation['purpose_type_string'] == 'running' :
                        #    print "appending running node_reservation (%s, %s, %s)" % (start_time_float, end_time_float, node)
                        node_reservations[node] = [ (start_time_float, 'decrement', node_resource_list[index]) ]
                        node_reservations[node].append( (end_time_float, 'add', node_resource_list[index]) )
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "appending node_reservation (%s, %s, %s)" % (start_time_float, end_time_float, node)
                    #if reservation['purpose_type_string'] == 'running' :
                    #    print "appending running node_reservation (%s, %s, %s)" % (start_time_float, end_time_float, node)
        #potential_start_time_float = new_res['earliest_start_float']
        #if potential_start_time_float == None :
        #    potential_start_time_float = Now_float
        potential_start_time_float = 0.0
        for accepted_node in accepted_nodes :
            if not node_reservations.has_key(accepted_node) :
                # node is free, create window for Now_float
                #reservation_windows.append( (potential_start_time_float, end_limit, accepted_node) )
                # I need resource_dict here, to see ConsumableCpus and ConsumableMemory for the node...
                #print "accepted_node (%s)" % accepted_node
                if resources_dict[accepted_node].has_key('ConsumableCpus') :
                    acConsumableCpus = resources_dict[accepted_node]['ConsumableCpus']
                else :
                    acConsumableCpus = 0
                if resources_dict[accepted_node].has_key('ConsumableMemory') :
                    acConsumableMemory = resources_dict[accepted_node]['ConsumableMemory']
                else :
                    acConsumableMemory = 0
                reservation_windows.append( (potential_start_time_float, end_limit, {'nodename' : accepted_node,
     'node' : 1}) )
                reservation_windows.append( (potential_start_time_float, end_limit, {'nodename' : accepted_node,
     'cpu' : acConsumableCpus}) )
                reservation_windows.append( (potential_start_time_float, end_limit, {'nodename' : accepted_node,
     'memory' : acConsumableMemory}) )
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "appending open window (%s, %s, %s)" % (potential_start_time_float, end_limit, accepted_node)
                    print "end_limit is (%s)" % time.asctime(time.localtime(end_limit))
        # sort the reservations for each node, in ascending start_time_float order
        # keep a potential start time for the reservation window.
        # for each reservation, if the start_time_float - the potential start_time_float
        # is less than the duration_float, the window is too small (or negative)
        # instead of doing this by reservation, need to do it by event
        # + or - of cpus or memory.  Create an open window between each event
        for node in node_reservations.keys() :
            #node_reservations[node].sort(sort_by_start)
            node_reservations[node].sort()
            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "node_reservations[node] (%s)" % (node_reservations[node],)
            #print "node_reservations[node] (%s)" % (node_reservations[node],)
            #potential_start_time_float = new_res['earliest_start_float']
            #if potential_start_time_float == None :
            #    potential_start_time_float = Now_float
            potential_start_time_float = 0.0
            freenodes = 0
            freecpus = 0
            freememory = 0
            #for index in range(len(node_reservations[node])) :
            started_windows_list = []
            negative_node = 0
            negative_cpu = 0
            negative_memory = 0
            for event in node_reservations[node] :
                if event[1] == 'add' :
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "add event (%s)" % (event,)
                    #print "add event (%s)" % (event,)
                    #if event[2]['type'] == 'node_exclusive' :
                    if new_res['node_usage'] == 'node_exclusive' :
                        node_amount = 1
                    else :
                        if event[2].has_key('node') :
                            node_amount = event[2]['node']
                        else :
                            node_amount = 1
                    if event[2].has_key('node') :
                        started_windows_list.append(
                          ( event[0],
                            { 'start' : event[0], 'type' : 'node',
                              'amount' : node_amount, 'state' : 'active'
                            }
                          )
                          )
                    else :
                        # is this right?  should we really say this is the
                        # start of a time window for a free node?
                        started_windows_list.append(
                          ( event[0],
                            { 'start' : event[0], 'type' : 'node',
                              'amount' : 1, 'state' : 'active'
                            }
                          )
                          )
                    started_windows_list.append(
                      ( event[0],
                        { 'start' : event[0], 'type' : 'cpu',
                          'amount' : event[2]['cpu'], 'state' : 'active'
                        }
                      )
                      )
                    started_windows_list.append(
                      ( event[0],
                        { 'start' : event[0], 'type' : 'memory',
                          'amount' : event[2]['memory'], 'state' : 'active'
                        }
                      )
                      )
                if event[1] == 'decrement' :
                    # sort started_windows_list LASTAVAILABLE
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "decrement event(%s)" % (event,)
                    #print "decrement event(%s)" % (event,)
                    #if negative_cpu > 0 :
                    #    # choose early cpu starts first, to get
                    #    # rid of cpu debt
                    #    cpu_started_windows_list.sort()
                    #else :
                    #    cpu_started_windows_list.sort()
                    #    cpu_started_windows_list.reverse()
                    #if negative_memory > 0 :
                    #    # choose early memory starts first, to get
                    #    # rid of memory debt
                    #    memory_started_windows_list.sort()
                    #else :
                    #    memory_started_windows_list.sort()
                    #    memory_started_windows_list.reverse()
                    started_windows_list.sort()
                    started_windows_list.reverse()
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "started_windows_list (%s)" % (started_windows_list,)
                    foundnodes = 0
                    foundcpus = 0
                    foundmemory = 0
                    #if event[2]['type'] == 'node_exclusive' :
                    #    neednode = event[2]['node']
                    #else :
                    #    neednode = 0
                    if new_res['node_usage'] == 'node_exclusive' :
                        neednodes = 1
                    else :
                        if event[2].has_key('node') :
                            neednodes = event[2]['node']
                        else :
                            neednodes = 1
                    needcpus = event[2]['cpu']
                    needmemory = event[2]['memory']
                    new_windows_list = []

                    # pay node debt
                    if negative_node > 0 :
                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "negative_node (%s)" % negative_node
                        # choose early node starts first, to get
                        # rid of cpu debt
                        started_windows_list.sort()
                        extranodes = 0
                        for index in range(len(started_windows_list)) :
                            started_window = started_windows_list[index]
                            if started_window[1]['state'] == 'inactive' :
                                continue
                            if started_window[1]['type'] == 'node' :
                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "started_window (%s)" % (started_window,)
                                if negative_node == 0 :
                                    break
                                foundnodes = started_window[1]['amount']
                                if foundnodes >= negative_node :
                                    extranodes = foundnodes - negative_node
                                    negative_node = 0
                                    if extranodes > 0 :
                                        new_started_window = copy.deepcopy(started_window)
                                        new_started_window[1]['amount'] = extranodes
                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "new_started_window (%s)" % (new_started_window,)
                                        new_windows_list.append(new_started_window)
                                else :
                                    negative_node = negative_node - foundnodes
                                started_windows_list[index][1]['state'] = 'inactive'

                    # pay cpu debt
                    if negative_cpu > 0 :
                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "negative_cpu (%s)" % negative_cpu
                        # choose early cpu starts first, to get
                        # rid of cpu debt
                        started_windows_list.sort()
                        extracpus = 0
                        for index in range(len(started_windows_list)) :
                            started_window = started_windows_list[index]
                            if started_window[1]['state'] == 'inactive' :
                                continue
                            if started_window[1]['type'] == 'cpu' :
                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "started_window (%s)" % (started_window,)
                                if negative_cpu == 0 :
                                    break
                                foundcpus = started_window[1]['amount']
                                if foundcpus >= negative_cpu :
                                    extracpus = foundcpus - negative_cpu
                                    negative_cpu = 0
                                    if extracpus > 0 :
                                        new_started_window = copy.deepcopy(started_window)
                                        new_started_window[1]['amount'] = extracpus
                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "new_started_window (%s)" % (new_started_window,)
                                        new_windows_list.append(new_started_window)
                                else :
                                    negative_cpu = negative_cpu - foundcpus
                                started_windows_list[index][1]['state'] = 'inactive'




                    # pay memory debt
                    if negative_memory > 0 :
                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "negative_memory (%s)" % negative_memory
                        # choose early memory starts first, to get
                        # rid of memory debt
                        started_windows_list.sort()
                        extramemory = 0
                        for index in range(len(started_windows_list)) :
                            started_window = started_windows_list[index]
                            if started_window[1]['state'] == 'inactive' :
                                continue
                            if started_window[1]['type'] == 'memory' :
                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "started_window (%s)" % (started_window,)
                                if negative_memory == 0 :
                                    break
                                foundmemory = started_window[1]['amount']
                                if foundmemory >= negative_memory :
                                    extramemory = foundmemory - negative_memory
                                    negative_memory = 0
                                    if extramemory > 0 :
                                        new_started_window = copy.deepcopy(started_window)
                                        new_started_window[1]['amount'] = extramemory
                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "new_started_window (%s)" % (new_started_window,)
                                        new_windows_list.append(new_started_window)
                                else :
                                    negative_memory = negative_memory - foundmemory
                                started_windows_list[index][1]['state'] = 'inactive'


                    started_windows_list = started_windows_list + new_windows_list
                    new_windows_list = []
                    started_windows_list.sort()
                    started_windows_list.reverse()
                    foundnodes = 0
                    foundcpus = 0
                    foundmemory = 0
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "negative_node (%s) negative_cpu (%s) negative_memory (%s)" % (negative_node, negative_cpu, negative_memory)
                    for index in range(len(started_windows_list)) :
                        started_window = started_windows_list[index]
                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "started_window (%s)" % (started_window,)
                        if started_window[1]['state'] == 'inactive' :
                            continue

                        if started_window[1]['type'] == 'node' :
                            if foundnodes >= neednodes :
                                continue
                            foundnodes = foundnodes + started_window[1]['amount']
                            if foundnodes >= 0 :
                                negative_node = 0
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "foundnodes (%s)" % foundnodes
                                print "neednodes (%s)" % neednodes
                            if foundnodes >= neednodes :
                                extranodes = foundnodes - neednodes
                                if extranodes > 0 :
                                    new_started_window = copy.deepcopy(started_window)
                                    new_started_window[1]['amount'] = extranodes
                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "new_started_window (%s)" % (new_started_window,)
                                    new_windows_list.append(new_started_window)
                                if started_window[1]['start'] != event[0] :
                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                          {'nodename' : node,
                                           'node' : started_window[1]['amount'] - extranodes}
                                          ),
                                          )
                                        print "start (%s) end (%s)" % (time.asctime(time.localtime(started_window[1]['start'])), time.asctime(time.localtime(event[0])))
                                    reservation_windows.append(
                                      (started_window[1]['start'], event[0],
                                       {'nodename' : node,
                                        'node' : started_window[1]['amount'] - extranodes}
                                      )
                                      )
                            else :
                                # remove started_window from list,
                                # end the previously started windows
                                # append to reservation_windows
                                if started_window[1]['amount'] > 0 and foundnodes > 0 :
                                    if started_window[1]['start'] != event[0] :
                                        #print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                        #  {'nodename' : node,
                                        #   'node' : started_window[1]['amount']}
                                        #  ),
                                        #  )
                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                              {'nodename' : node,
                                               'node' : started_window[1]['amount']}
                                              ),
                                              )
                                        reservation_windows.append(
                                          (started_window[1]['start'], event[0],
                                           {'nodename' : node,
                                            'node' : started_window[1]['amount']}
                                          )
                                          )
                            started_windows_list[index][1]['state'] = 'inactive'


                        if started_window[1]['type'] == 'cpu' :
                            if foundcpus >= needcpus :
                                continue
                            foundcpus = foundcpus + started_window[1]['amount']
                            if foundcpus >= 0 :
                                negative_cpu = 0
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "foundcpus (%s)" % foundcpus
                                print "needcpus (%s)" % needcpus
                            if foundcpus >= needcpus :
                                extracpus = foundcpus - needcpus
                                if extracpus > 0 :
                                    new_started_window = copy.deepcopy(started_window)
                                    new_started_window[1]['amount'] = extracpus
                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "new_started_window (%s)" % (new_started_window,)
                                    new_windows_list.append(new_started_window)
                                if started_window[1]['start'] != event[0] :
                                    #print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                    #  {'nodename' : node,
                                    #   'cpu' : started_window[1]['amount'] - extracpus}
                                    #  ),
                                    #  )
                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                          {'nodename' : node,
                                           'cpu' : started_window[1]['amount'] - extracpus}
                                          ),
                                          )
                                    reservation_windows.append(
                                      (started_window[1]['start'], event[0],
                                       {'nodename' : node,
                                        'cpu' : started_window[1]['amount'] - extracpus}
                                      )
                                      )
                            else :
                                # remove started_window from list,
                                # end the previously started windows
                                # append to reservation_windows
                                if started_window[1]['amount'] > 0 and foundcpus > 0 :
                                    if started_window[1]['start'] != event[0] :
                                        #print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                        #  {'nodename' : node,
                                        #   'cpu' : started_window[1]['amount']}
                                        #  ),
                                        #  )
                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                              {'nodename' : node,
                                               'cpu' : started_window[1]['amount']}
                                              ),
                                              )
                                        reservation_windows.append(
                                          (started_window[1]['start'], event[0],
                                           {'nodename' : node,
                                            'cpu' : started_window[1]['amount']}
                                          )
                                          )
                            started_windows_list[index][1]['state'] = 'inactive'



                        if started_window[1]['type'] == 'memory' :
                            if foundmemory >= needmemory :
                                continue
                            foundmemory = foundmemory + started_window[1]['amount']
                            if foundmemory >= 0 :
                                negative_memory = 0
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "foundmemory (%s)" % foundmemory
                                print "needmemory (%s)" % needmemory
                            if foundmemory >= needmemory :
                                extramemory = foundmemory - needmemory
                                if extramemory > 0 :
                                    new_started_window = copy.deepcopy(started_window)
                                    new_started_window[1]['amount'] = extramemory
                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "new_started_window (%s)" % (new_started_window,)
                                    new_windows_list.append(new_started_window)
                                if started_window[1]['start'] != event[0] :
                                    #print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                    #  {'nodename' : node,
                                    #   'memory' : started_window[1]['amount'] - extramemory}
                                    #  ),
                                    #  )
                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                          {'nodename' : node,
                                           'memory' : started_window[1]['amount'] - extramemory}
                                          ),
                                          )
                                    reservation_windows.append(
                                      (started_window[1]['start'], event[0],
                                       {'nodename' : node,
                                        'memory' : started_window[1]['amount'] - extramemory}
                                      )
                                      )
                            else :
                                # remove started_window from list,
                                # end the previously started windows
                                # append to reservation_windows
                                if started_window[1]['amount'] > 0 and foundmemory > 0 :
                                    if started_window[1]['start'] != event[0] :
                                        #print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                        #  {'nodename' : node,
                                        #   'memory' : started_window[1]['amount']}
                                        #  ),
                                        #  )
                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                              {'nodename' : node,
                                               'memory' : started_window[1]['amount']}
                                              ),
                                              )
                                        reservation_windows.append(
                                          (started_window[1]['start'], event[0],
                                           {'nodename' : node,
                                            'memory' : started_window[1]['amount']}
                                          )
                                          )
                            #del started_windows_list[index]
                            started_windows_list[index][1]['state'] = 'inactive'
                        if foundnodes >= neednodes and foundcpus >= needcpus and foundmemory >= needmemory :
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "found nodes and cpus and memory (%s) (%s) (%s)" % (foundnodes, foundcpus, foundmemory)
                            break
                    if foundnodes < neednodes or foundcpus < needcpus or foundmemory < needmemory :
                        # nodes or cpus or memory are overcommitted.  Create
                        # a negative amount started window.
                        if foundnodes < neednodes :
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "overcommitted nodes neednodes - foundnodes (%s)" % (neednodes - foundnodes,)
                            negative_node = negative_node + neednodes - foundnodes
                        if foundcpus < needcpus :
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "overcommitted cpus needcpus - foundcpus (%s)" % (needcpus - foundcpus,)
                            negative_cpu = negative_cpu + needcpus - foundcpus
                        if foundmemory < needmemory :
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "overcommitted cpus needmemory - foundmemory (%s)" % (needmemory - foundmemory,)
                            negative_memory = negative_memory + needmemory - foundmemory
                    started_windows_list = started_windows_list + new_windows_list
                #if event[0] > potential_start_time_float :
                #    # create open window here
                #reservation_windows.append( 
                #  (potential_start_time_float,
                #  reservation[0], { 'nodename' : node, 'cpu' : ) ) 
#            for reservation in node_reservations[node] :
#                if reservation[0] < end_limit :
#                    # reservation starts before end_limit
#                    if reservation[0] - potential_start_time_float >= \
#                    new_res['duration_float'] :
#                        # There is enough space ( duration )
#                        # between the potential start time and the start of this
#                        # blocking node reservation
#                        reservation_windows.append( 
#                          (potential_start_time_float,
#                          reservation[0], { 'nodename' : node, 'cpu' : ) ) 
#                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                            print "appending open window (%s, %s, %s)" % (potential_start_time_float, reservation[0], node)
#                            print "appending open window (%s, %s, %s)" % (time.asctime(time.localtime(potential_start_time_float)), time.asctime(time.localtime(reservation[0])), node)
#                else :
#                    # reservations starts after end_limit
#                    if end_limit - potential_start_time_float >= \
#                    new_res['duration_float'] :
#                        # There is enough space between the potential
#                        # start time and the end_limit for duration
#                        reservation_windows.append(
#                        (potential_start_time_float,
#                        end_limit, node) ) 
#                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                            print "appending open window (%s, %s, %s)" % (potential_start_time_float, end_limit, node)
#                if reservation[1] > potential_start_time_float :
#                    # The node reservation ends after the potential start time
#                    # advance the potential start time for this node
#                    # for cpu/memory scheduling the next cpu/memory open
#                    # window might overlap this reservations end time...
#                    potential_start_time_float = reservation[1]
            # handle the last window for the node
            #last_window = None
            #if end_limit >= potential_start_time_float + new_res['duration_float'] :
            #    last_window = ( potential_start_time_float, end_limit, node )
            #if last_window != None :
            #    reservation_windows.append(last_window)
            #    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            #        print "appending open window (%s, %s, %s)" % (potential_start_time_float, end_limit, node)
        reservation_windows.sort()
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "checking reservation_windows before returning"
            for new_window in reservation_windows :
                print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
                  (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])
        result = reservation_windows
        return result
        # End of get_nonconflicting function

    conflict_policy_code = new_res['conflict_policy']
    #if new_res['purpose_type_string'] in ['preempted_job',] :
    #    print "conflict_policy_code (%s)" % conflict_policy_code
    temp_nodes_list = accepted_nodes_list[:]
    #print "temp_nodes_list (%s)" % (temp_nodes_list,)
    input_tuple = (temp_nodes_list, new_res, temp_reservations_list, Now_float, resources_db_handle)
    if conflict_policy_code == None :
        if DEBUGJOB !=None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "calling get_nonconflicting"
        open_windows_list = get_nonconflicting(input_tuple)
    else :
        # if a nonstandard conflict policy was provided,
        # create the get_noncoflicting function out of that
        # this allows arbitrary code to be used to determine
        # reservation windows...
        open_windows_list = apply_policy_code(conflict_policy_code, input_tuple)
    if DEBUGJOB !=None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "checking open_windows_list before returning"
        for new_window in open_windows_list :
            print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
              (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])
    #for new_window in open_windows_list :
    #    print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
    #      (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])
    return open_windows_list

def get_big_windows_list(open_windows_list, new_res) :
    duration = new_res['duration_float']
    big_windows_list = filter(lambda x, duration=duration : x[1] - x[0] >= duration, open_windows_list)
    #big_windows_list = []
    #for window in open_windows_list :
    #    if float(window[1] - window[0]) >= duration :
    #        big_windows_list.append(window)
    #    else :
    #        print "window[0] (%s), window[1] (%s), duration (%s)" % \
    #          (time.asctime(time.localtime(window[0])), time.asctime(time.localtime(window[1])), duration)
    #print "len(big_windows_list) (%s)" % len(big_windows_list)
    return big_windows_list

def get_sized_windows_list(new_res, open_windows_list, resources_db_handle, jobs_db_handle, resource_dict_list = None, requested_resource_list = [] ) :
    def getfirstindex(x) :
        return(x[1])
    #print "in get_sized"
    #print "requested_resource_list (%s)" % (requested_resource_list,)
    # From open windows, choose a set of nodes and a start_time_float
    # for the reservation.  open_windows_list should have been sorted
    # by the code in conflict_policy_code in ascending start order
    # To do: 1. if resource_amount not provided, loop to find _all_ collections
    # of windows with sufficient duration.  Choose the largest of these.
    # 2.  To work with arbitrary job geometries and multiple jobs/node,
    # convert resource to proc, create a dictionary of windows for
    # each set of windows, with the key as node name and the value a list
    # of proc windows.  Select sets of proc windows, based on the length
    # of the proc window list for each node.
    def get_longer(first_list, second_list) :
        if len(first_list[1]) >= len(second_list[1]) :
            return first_list
        else :
            return second_list
    resources_dict = resources_db_handle[0]
    # Assume that if any resource_dict has 'amount_int' == None,
    # then we are supposed to return the largest set of sized
    # windows for that resource list.
    #resource_amount_int = new_res['resource_amount_int']
    #max_resource_int = new_res['max_resource_int']
    earliest_start_float = new_res['earliest_start_float']
    sized_windows_lists_list = []
    #if earliest_start_float != None :
    #    time_tuple = time.localtime(math.floor(earliest_start_float))
    #    time_string = time.asctime(time_tuple)
    sized_windows_list = []
    if earliest_start_float != None :
        #print "earliest_start_float != None (%s)" % time.asctime(time.localtime(earliest_start_float))
        current_start_time_float = earliest_start_float
    else :
        #print "earliest_start_float == None, Now_float (%s)" % time.asctime(time.localtime(Now_float))
        current_start_time_float = Now_float
    #print "in get_sized, new_res['name'] (%s) new_res['purpose_type_string'] (%s)" % (new_res['name'], new_res['purpose_type_string'])
    #print "current_start_time_float (%s)" % time.asctime(time.localtime(current_start_time_float))
    # resource_dict_list for PBS supports multireq.  Each dict
    # represents one type of node, the nodes that can fit the req,
    # and how many of those needs were requested.  To support multiple
    # job/node, the allocated_dict_list needs to be mapped onto the
    # resource_dict_list.  Hmmm, maybe they should be the same from
    # the start?
    #print "resource_dict_list (%s)" % (resource_dict_list,)
    if resource_dict_list == None :
        dict = {}
        for open_window in open_windows_list :
            #dict[resources_dict[open_window[2]['nodename']]['name']] = resources_dict[open_window[2]['nodename']]
            dict[resources_dict[open_window[2]['nodename']]['name']] = {}
        resource_dict_list = [{
          'amount_int' : new_res['resource_amount_int'],
          'resource_dict' : dict
          },]
    #print "after for loop 1"
    sized_dict_list = []
    getlongest = 0
    # map requested_resource_list, if present to
    # resource_dict_list.  Both resource_dict_list and
    # requested_resource_list should be based on initiatormap.
    #if new_res.has_key('job_runID') and jobs_db_handle[0][new_res['job_runID']].has_key['requested_resource_list'] :
    #    requested_resource_list = jobs_db_handle[0][new_res['job_runID']]['requested_resource_list']
    #for dict in resource_dict_list :
    #print "resource_dict_list (%s)" % (resource_dict_list,)
    #print "requested_resource_list (%s)" % (requested_resource_list,)
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "len(resource_dict_list) (%s)" % len(resource_dict_list)
    nodeindex = 0
    for index in range(len(resource_dict_list)) :
        # need to add 'req_list' to each dict...
        dict = resource_dict_list[index]
        if dict['amount_int'] == None :
            getlongest = 1
            sized_dict = {
              'initiatormap_index' : index,
              'amount_int' : dict['amount_int'],
              'saved_windows_list' : [],
              'resource_dict' : dict['resource_dict']
              }
            if nodeindex < len(requested_resource_list) :
                #print "setting req_list to (%s)" % (requested_resource_list[index]['req_list'],)
                sized_dict['req_list'] = requested_resource_list[nodeindex]['req_list']
            else :
                print "ran out of requested_resource_list, setting req_list to []"
                if len(requested_resource_list) >= 1 :
                    sized_dict['req_list'] = requested_resource_list[-1]['req_list']
                else :
                    sized_dict['req_list'] = []
            #print "sized_dict (%s)" % sized_dict
            #sized_dict['requested_resource'] = requested_resource_list[index]
            sized_dict_list.append(sized_dict)
        else :
            # FIXTHIS
            # need to correctly map requested_resource_list elements to
            # resource_dict['amount_int'] specs...
            # May need to filter, if floating licenses or storage are
            # part of this...
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "dict['amount_int'] (%s)" % dict['amount_int']
            for sizedindex in range(dict['amount_int']) :
                sized_dict = {
                  'initiatormap_index' : index,
                  'amount_int' : 1,
                  'saved_windows_list' : [],
                  'resource_dict' : dict['resource_dict']
                  }
                if nodeindex < len(requested_resource_list) :
                    #print "setting req_list to (%s)" % (requested_resource_list[nodeindex]['req_list'],)
                    sized_dict['req_list'] = requested_resource_list[nodeindex]['req_list']
                else :
                    #print "ran out of requested_resource_list, setting req_list to %s" % requested_resource_list[-1]['req_list']
                    if len(requested_resource_list) >= 1 :
                        sized_dict['req_list'] = requested_resource_list[-1]['req_list']
                    else :
                        sized_dict['req_list'] = []
                #sized_dict['requested_resource'] = requested_resource_list[index]
                sized_dict_list.append(sized_dict)
                nodeindex = nodeindex + 1
        #if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        #    print "sized_dict_list (%s)" % sized_dict_list
        #print "sized_dict_list (%s)" % sized_dict_list
        #if len(requested_resource_list) == len(resource_dict_list) :
        #    sized_dict['requested_resource'] = requested_resource_list[index]
        #else :
        #    sized_dict['requested_resource'] = {'type' : 'node_exclusive',
        #                                        'req_list' : []}
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "len(sized_dict_list) (%s)" % len(sized_dict_list)
        #continue_var = raw_input("continue? ")
    #print "after for loop 2"
    candidate_sized_list = []
    counting_dict_list = []
    #print "after loop 2, setting need_more = 1"
    need_more = 1
    #if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
    #    print "open_windows_list (%s)" % (open_windows_list,)
    #print "open_windows_list (%s)" % (open_windows_list,)
    #print "len(open_windows_list) (%s)" % (len(open_windows_list),)
    node_bin_dict = {}
    # startopt
    #for new_window in open_windows_list :
    last_window_index = len(open_windows_list) - 1
    for new_window_index in range(len(open_windows_list)) :
        new_window = open_windows_list[new_window_index]
        #input_text = raw_input('continue? ')
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "new_window (%s)" % (new_window,)
        #print "new_window (%s)" % (new_window,)
        #print "considering this window (%s) - (%s) < (%s)" % \
        #  (time.asctime(time.localtime(new_window[1])), time.asctime(time.localtime(current_start_time_float)), new_res['duration_float'])
        # Break if we have reached our target resource_amount_int and the next window
        # starts later than the current crop
        # - update the start and end times, if necessary
        # - remove any windows that end before the new end time
        # Break if latest_end_float exists, and there is not enough time
        if new_res['latest_end_float'] != None and \
          ( current_start_time_float + \
          new_res['duration_float'] > new_res['latest_end_float'] or \
          new_window[0] + new_res['duration_float'] > new_res['latest_end_float'] ) :
            if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
                print "breaking, not enough time left before end of window"
            #print "breaking, not enough time left before end of window"
            #print "current_start (%s) new_res['duration_float'] (%s) new_res['latest_end_float'] (%s) new_window[0]+new_res['duration_float'] (%s)" % \
            #  (time.asctime(time.localtime(current_start_time_float)), new_res['duration_float'], time.asctime(time.localtime(new_res['latest_end_float'])), time.asctime(time.localtime(new_window[0] + new_res['duration_float'])))
            break
        # Skip if there is not enough time in this window
        # need to get a node count check in...
        if new_window[1] - current_start_time_float < \
          new_res['duration_float'] :
            if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
                print "skipping this window due to lack of time (%s) - (%s) < (%s)" % \
                  (time.asctime(time.localtime(new_window[1])), time.asctime(time.localtime(current_start_time_float)), new_res['duration_float'])
            #print "skipping this window due to lack of time (%s) - (%s) < (%s)" % \
            #  (time.asctime(time.localtime(new_window[1])), time.asctime(time.localtime(current_start_time_float)), new_res['duration_float'])
            #if new_window[0] > current_start_time_float :
            #    current_start_time_float = new_window[0]
            #continue
        else :
            #print "adding this window (%s)" % (new_window,)
            # Add the new window to the list of sized_windows_list
            sized_windows_list.append(new_window)
 
        #print "1. len(sized_windows_list) (%s)" % len(sized_windows_list)

        if new_window_index == last_window_index or open_windows_list[new_window_index + 1][0] > current_start_time_float :

            #  MOVE THIS check only if the window under consideration is later
            #  than the current_start
            # - check to see if the sized_dict is fully populated
            if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                print "current_start_time_float (%s) len(sized_windows_list) (%s) new_window[2] (%s)" % \
                  ( time.asctime(time.localtime(current_start_time_float)), len(sized_windows_list), new_window[2] )
                print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
                  (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])

            # If the new window start time is greater than the current_start_time_float
            # reset the current_start_time_float.  Delete any of the sized_windows_list
            # that have an end_time_float too early to accomodate the reservation
            ## If resource_amount_int == None, save sized_windows_list with
            ## current start time, so that the largest of the sets can be returned
            #if getlongest == 1 :
            #    candidate_windows_list = [current_start_time_float, sized_windows_list, sized_dict_list]
            #    if len(sized_windows_list) > 0 :
            #        sized_windows_lists_list.append(candidate_windows_list)
            # Should current_start_time_float be set to the next window?
            # No, leave like this.  We are evaluating the current windows,
            # and current_start_time_float should reflect the current set,
            # rather than the next set.
            # Probably don't need this check, since the new window should
            # never be later than current_start_time_float
            if new_window[0] > current_start_time_float :
                current_start_time_float = new_window[0]
                #print "current_start_time reset to (%s)" % time.asctime(time.localtime(current_start_time_float))
            #if new_window_index != last_window_index and open_windows_list[new_window_index + 1][0] > current_start_time_float :
            #    current_start_time_float = open_windows_list[new_window_index + 1][0]
            # Do I need to filter out short windows at this point?
            # That should have happened above...
            saved_windows_list = []
            for i in range(len(sized_windows_list)) :
                if sized_windows_list[i][1] - current_start_time_float >= \
                new_res['duration_float'] :
                    saved_windows_list.append(sized_windows_list[i])
            sized_windows_list = saved_windows_list

    
            # For multiple-requirement reservation, need to see if each
            # resource_dict['amount'] has been reached with the current
            # set of open windows...
            #counting_dict_list = copy.deepcopy(sized_dict_list)
            counting_dict_list = sized_dict_list
            #print "counting_dict_list (%s)" % (counting_dict_list,)
            for sized_dict in counting_dict_list :
                sized_dict['saved_windows_list'] = []
            #print "after for loop 1a"
    
            # aggregate all the cpu and memory windows into bins by node
            # I lose start and end info for the individual open_window
            # here...I need to keep a window_list for use by get_sorted
            # later on....
            node_bin_dict = {}
            for sized_window in sized_windows_list :
                #print "sized_window (%s)" % (sized_window,)
                if node_bin_dict.has_key(sized_window[2]['nodename']) :
                    node_bin_dict[sized_window[2]['nodename']]['window_list'].append(sized_window)
                    if sized_window[2].has_key('node') :
                        node_bin_dict[sized_window[2]['nodename']]['node'] = node_bin_dict[sized_window[2]['nodename']]['node'] + sized_window[2]['node']
                    if sized_window[2].has_key('cpu') :
                        node_bin_dict[sized_window[2]['nodename']]['cpu'] = node_bin_dict[sized_window[2]['nodename']]['cpu'] + sized_window[2]['cpu']
                    if sized_window[2].has_key('memory') and sized_window[2]['memory'] != None :
                        #print "adding (%s)" % (sized_window[2]['memory'],)
                        node_bin_dict[sized_window[2]['nodename']]['memory'] = node_bin_dict[sized_window[2]['nodename']]['memory'] + sized_window[2]['memory']
                else :
                    node_bin_dict[sized_window[2]['nodename']] = {'window_list' : [sized_window,]}
                    if sized_window[2].has_key('cpu') and sized_window[2]['cpu'] != None :
                        node_bin_dict[sized_window[2]['nodename']]['node'] = 0
                        node_bin_dict[sized_window[2]['nodename']]['cpu'] = sized_window[2]['cpu']
                        node_bin_dict[sized_window[2]['nodename']]['memory'] = 0
                    elif sized_window[2].has_key('memory') :
                        node_bin_dict[sized_window[2]['nodename']]['node'] = 0
                        node_bin_dict[sized_window[2]['nodename']]['memory'] = sized_window[2]['memory']
                        #print "adding (%s)" % (sized_window[2]['memory'],)
                        node_bin_dict[sized_window[2]['nodename']]['cpu'] = 0
                    elif sized_window[2].has_key('node') :
                        node_bin_dict[sized_window[2]['nodename']]['node'] = sized_window[2]['node']
                        node_bin_dict[sized_window[2]['nodename']]['cpu'] = 0
                        node_bin_dict[sized_window[2]['nodename']]['memory'] = 0
                    else :
                        node_bin_dict[sized_window[2]['nodename']]['node'] = 0
                        node_bin_dict[sized_window[2]['nodename']]['cpu'] = 0
                        node_bin_dict[sized_window[2]['nodename']]['memory'] = 0
            #print "after for loop 2a"
            for nodename in node_bin_dict.keys() :
                node_bin_dict[nodename]['viable_request_count'] = 0
            #print "after for loop 3a"
    
            #for nodename in node_bin_dict.keys() :
    
            saved_sorting_list = []
            request_sorting_list = []
            # counting_dict has structure of requested_resource:
            # {'type' : <node_shared|node_exclusive>,
            #  'req_list' : [ {'cpu' : <cpus>, 'memory' : <memory>}...]}
            # req_list has one cpu-memory dictionary for each initiator
            # in the initiator map
            # indexa is which node in initiator map
            # indexb is which initiator in that node
            # For each initiator request, how many windows can satisfy
            # that request?
            #print "counting_dict_list (%s)" % (counting_dict_list,)
            for indexa in range(len(counting_dict_list)) :
                dict = counting_dict_list[indexa]
                #print "dict (%s)" % dict
                # for each term (representing a single node) in the
                # initiatormap, sum requested cpu and memory for all
                # initiators for that node.
                # each entry of the req_list is:
                # { 'cpu' : <cpus requested>,
                #   'memory' : <MB requested>}
                requested_node = 0
                requested_cpu = 0
                requested_memory = 0
                if new_res['node_usage'] == 'node_exclusive' :
                    requested_node = 1
                else :
                    requested_node = 0
                for indexb in range(len(dict['req_list'])) :
                    requested_resource = dict['req_list'][indexb]
                    if requested_resource.has_key('cpu') :
                        requested_cpu = requested_cpu + requested_resource['cpu']
                    if requested_resource.has_key('memory') :
                        requested_memory = requested_memory + requested_resource['memory']
                dict['node_usage'] = new_res['node_usage']
                dict['requested_node'] = requested_node
                dict['requested_cpu'] = requested_cpu
                dict['requested_memory'] = requested_memory
                #print "requested_node (%s) requested_cpu (%s) requested_memory (%s)" % (requested_node, requested_cpu, requested_memory)
                #print "dict (%s)" % dict
                # now, requested_cpu and requested_memory is the total for this
                # initiator term.  how many entries in node_bin_dict can
                # meet this request?
                viable_request_count = 0
                for nodename in node_bin_dict.keys() :
                    if requested_node <= node_bin_dict[nodename]['node'] :
                        if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                            print "sufficient node is 1 for (%s) requested_node (%s) node_bin_dict[nodename]['node'] (%s)" % (nodename, requested_node, node_bin_dict[nodename]['node'])
                        sufficient_node = 1
                    else :
                        if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                            print "sufficient node is 0 for (%s)" % nodename
                        sufficient_node = 0
                    if requested_cpu <= node_bin_dict[nodename]['cpu'] :
                        sufficient_cpu = 1
                    else :
                        sufficient_cpu = 0
                    if requested_memory <= node_bin_dict[nodename]['memory'] :
                        sufficient_memory = 1
                    else :
                        sufficient_memory = 0
                    if sufficient_node == 1 and sufficient_cpu == 1 and sufficient_memory == 1 :
                        viable_request_count = viable_request_count + 1
                        # how many of the requested_resources could fit in the node open
                        # window aggregate?  Need to fit all req_list into the aggregate
                        # node cpu and memory
                        # If this dict can fit, then increment the viable_request_count
                        # for each nodename.  node_bin_dict needs a 'viable_request_count'
                        # key
                        node_bin_dict[nodename]['viable_request_count'] = \
                          node_bin_dict[nodename]['viable_request_count'] + 1
                request_sorting_list.append( (viable_request_count, dict ) )
    
            #print "after for loop 4a"
            # need to allow for the situation where two initiatormap entries
            # could fit on a single node....
            for nodename in node_bin_dict.keys() :
                saved_sorting_list.append( (node_bin_dict[nodename]['viable_request_count'], nodename) )
            #if len(saved_sorting_list) > 0 :
            #    if getlongest == 1 :
            #        candidate_windows_list = [current_start_time_float, sized_windows_list, sized_dict_list]
            #        if len(sized_windows_list) > 0 and (new_res['node_usage'] =='node_shared' or (sufficient_node == 1 and sufficient_cpu == 1 and sufficient_memory == 1)) :
            #            # This appends a set of windows that does not fulfill
            #            # the sharedmap requirement.  This should happen
            #            # After the check of counting_dict_list
            #            print "appending candidate_windows_list with current_start_time (%s)" % time.asctime(time.localtime(current_start_time_float))
            #            print "sized_windows_list (%s)" % (sized_windows_list,)
            #            sized_windows_lists_list.append(candidate_windows_list)
            request_sorting_list.sort()
            request_sorting_list.reverse()
            #counting_dict_list = map(lambda x : x[1], request_sorting_list)
            counting_dict_list = map(getfirstindex, request_sorting_list)
            #print "counting_dict_list (%s)" % (counting_dict_list,)
            saved_sorting_list.sort()
            #saved_sorting_list.reverse()
            # this will break, next time through loop, sized_windows_list
            # elements will be wrong...
            #sized_windows_list = map(lambda x : x[1], saved_sorting_list)
            # This check should be whether there are sufficient
            # proc and memory windows to satisfy all
            # dict['requested_resource'] entries
    
            # populate saved_windows_list for each dict in counting_dict_list
            if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                print "populating counting_dict_list"
                print "len(counting_dict_list) (%s)" % len(counting_dict_list)
                #for counting_dict in counting_dict_list :
                #    print "saved_windows_list (%s)" % (counting_dict['saved_windows_list'],)
                #continue_var = raw_input("continue? ")
            assigned_nodes_list = []
            for window in saved_sorting_list :
                assigned_node = 0
                assigned_cpu = 0
                assigned_memory = 0
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "saved_sorting_list window (%s)" % (window,)
                #print "saved_sorting_list window (%s)" % (window,)
                if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                    print "start of window for loop for (%s)" % window[1]
                for dict in counting_dict_list :
                    #if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                    #    print "start of dict for loop for (%s)" % dict['amount_int']
                    if dict['amount_int'] == None :
                        #if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        #    print "dict['amount_int'] == None, continuing"
                        if dict['resource_dict'].has_key(window[1]) :
                            assigned_nodes_list.append(window[1])
                            dict['saved_windows_list'].append(window)
                            break
                        continue
                    if len(dict['saved_windows_list']) >= dict['amount_int'] :
                        #if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        #    print "1. len(dict['saved_windows_list']) >= dict['amount_int'], continuing"
                        #    print "1. len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                        #    print "1. dict['amount_int'] (%s)" % dict['amount_int']
                        continue
                    else :
                        #if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        #    print "2. len(dict['saved_windows_list']) < dict['amount_int'], continuing"
                        #    print "2. len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                        #    print "2. dict['amount_int'] (%s)" % dict['amount_int']
                        if dict['resource_dict'].has_key(window[1]) :
                            # if node_bin_dict[nodename]['cpu'] and memory more
                            # than total requested cpu and memory for the req dict,
                            # add the node

                            # The assigned_nodes_list check is to prevent
                            # scheduling on the same node twice for shared
                            # jobs.  I think this should be allowed, but
                            # LoadLeveler, at least, complains...
                            if not window[1] in assigned_nodes_list and dict['requested_node'] <= node_bin_dict[window[1]]['node'] - assigned_node and dict['requested_cpu'] <= node_bin_dict[window[1]]['cpu'] - assigned_cpu and dict['requested_memory'] <= node_bin_dict[window[1]]['memory'] - assigned_memory :
                                if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                                    print "appending node (%s)" % window[1]
                                #print "appending node (%s)" % window[1]
                                assigned_nodes_list.append(window[1])
                                dict['saved_windows_list'].append(window)
                                #assigned_node = assigned_node + dict['requested_node']
                                #assigned_node = assigned_node + 1
                                assigned_cpu = assigned_cpu + dict['requested_cpu']
                                assigned_memory = assigned_memory + dict['requested_memory']
                                continue
                            else :
                                #print "not appending (%s)" % (window,)
                                #print "not appending (%s)" % (window,)
                                #print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                #print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                #print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                #print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "not appending (%s)" % (window,)
                                    print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                    print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                    print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                    print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']

                        continue
            # check for any under-populated counting_dict_list entries
            if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                print "checking counting_dict_list"
                for counting_dict in counting_dict_list :
                    print "saved_windows_list (%s) amount (%s) resource_dict (%s)" % (counting_dict['saved_windows_list'],counting_dict['amount_int'], counting_dict['resource_dict'].keys())
                #continue_var = raw_input("continue? ")
            #for counting_dict in counting_dict_list :
                #print "saved_windows_list (%s) amount (%s) resource_dict (%s)" % (counting_dict['saved_windows_list'],counting_dict['amount_int'], counting_dict['resource_dict'].keys())
            #print "1 setting need_more = 0"
            need_more = 0
            want_more = 0
            for dict in counting_dict_list :
                #print "dict (%s)" % dict
                if dict['amount_int'] == None :
                    if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        print "dict['amount_int'] == None, need_more = 1"
                    #print "dict['amount_int'] == None, need_more = 1"
                    want_more = 1
                # Look for new_res['node_usage'] node_shared or node_exclusive
                # if node_exclusive, look for node_bin_dict entries of type
                # 'node'.  If node_shared, count up cpu and memory.  if
                # dict has enough, save the nodename in a used_nodes_list.

                enough_cpu = 0
                enough_memory = 0
                accrued_cpu = 0
                accrued_memory = 0
                used_nodes_list = []
                if dict.has_key('req_list') and len(dict['req_list']) > 0 :
                    if dict['req_list'][0].has_key('cpu') :
                        requested_cpu = dict['req_list'][0]['cpu']
                    else :
                        requested_cpu = 0
                    if dict['req_list'][0].has_key('memory') :
                        requested_memory = dict['req_list'][0]['memory']
                    else :
                        requested_memory = 0
                for saved_window in dict['saved_windows_list'] :
                    nodename = saved_window[1]
                    if nodename in used_nodes_list :
                        continue
                    if node_bin_dict.has_key(nodename) :
                        if node_bin_dict[nodename].has_key('cpu') :
                            accrued_cpu = node_bin_dict[nodename]['cpu']
                        if node_bin_dict[nodename].has_key('memory') :
                            accrued_memory = node_bin_dict[nodename]['memory']
                    if accrued_cpu >= requested_cpu and accrued_memory >= requested_memory :
                        used_nodes_list.append(nodename)
                        enough_cpu = 1
                        enough_memory = 1
                        break
                if enough_cpu == 0 or enough_memory == 0 :
                    need_more = 1
                    break

            if need_more == 0 :
                if len(saved_sorting_list) > 0 :
                    if getlongest == 1 :
                        candidate_windows_list = [current_start_time_float, sized_windows_list, sized_dict_list]
                        if len(sized_windows_list) > 0 and (new_res['node_usage'] =='node_shared' or (sufficient_node == 1 and sufficient_cpu == 1 and sufficient_memory == 1)) :
                            # This appends a set of windows that does not fulfill
                            # the sharedmap requirement.  This should happen
                            # After the check of counting_dict_list
                            #print "appending candidate_windows_list with current_start_time (%s)" % time.asctime(time.localtime(current_start_time_float))
                            #print "sized_windows_list (%s)" % (sized_windows_list,)
                            sized_windows_lists_list.append(candidate_windows_list)
                if want_more == 0 :
                    #print "breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(current_start_time_float)))
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "want_more == 0"
                    break
                #else :
                    #print "not breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(current_start_time_float)))
            # if this is not the last window, set current start time
            # and filter out all windows too short with the new current
            # current start time
            if new_window_index + 1 < len(open_windows_list) :
                current_start_time_float = open_windows_list[new_window_index + 1][0]
                saved_windows_list = []
                for i in range(len(sized_windows_list)) :
                    if sized_windows_list[i][1] - current_start_time_float >= \
                    new_res['duration_float'] :
                        saved_windows_list.append(sized_windows_list[i])
                sized_windows_list = saved_windows_list
    #if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        #continue_var = raw_input("continue? ")
    #print "after for loop 3"
    # endopt
    if getlongest == 1 :
        #print "getting longest"
        # Append the last sized_windows_list
        candidate_windows_list = [current_start_time_float, sized_windows_list]
        #if len(sized_windows_list) > 0 :
        #    sized_windows_lists_list.append(candidate_windows_list)
        if len(sized_windows_lists_list) > 0 :
            #longest_candidate = reduce(get_longer, sized_windows_lists_list)
            #sized_windows_list = longest_candidate[1]
            #current_start_time_float = longest_candidate[0]
            # choose the set of sized windows that fulfills sized_dict_list
            # and has the most nodes for all 'amount_int' == None
            # sort sized_dict_list to have any 'amount_int' == None
            # dicts last
            sized_dict_tuple_list = []
            for dict in sized_dict_list :
                if dict['amount_int'] == None :
                    sized_dict_tuple_list.append((1,len(dict['resource_dict'].keys()), dict))
                else :
                    sized_dict_tuple_list.append((0,len(dict['resource_dict'].keys()), dict))
            sized_dict_tuple_list.sort()
            tmplist = []
            for tuple in sized_dict_tuple_list :
                tmplist.append(tuple[2])
            sized_dict_list = tmplist
            candidate_tuple_list = []
            for start_and_list in sized_windows_lists_list :
                window_list = start_and_list[1]
                if len(window_list) == 0 :
                    continue
                current_start_time_float = start_and_list[0]
                #counting_dict_list = copy.deepcopy(sized_dict_list)
                counting_dict_list = sized_dict_list
                for sized_dict in counting_dict_list :
                    sized_dict['saved_windows_list'] = []
                for window in window_list :
                    for sized_dict in counting_dict_list :
                        if sized_dict['amount_int'] == None :
                            # Here's where I check to see if the resource is
                            # usable.  Hmmm, a has_key should be lots faster
                            # Than seeing if name is in list.  For performance
                            # maybe should use resource_dict lists, but use
                            # empty values?
                            if sized_dict['resource_dict'].has_key(window[2]['nodename']) :
                                sized_dict['saved_windows_list'].append(window)
                                break
                            continue
                        if len(sized_dict['saved_windows_list']) >= sized_dict['amount_int'] :
                            continue
                        else :
                            if sized_dict['resource_dict'].has_key(window[2]['nodename']) :
                                sized_dict['saved_windows_list'].append(window)
                                break
                            continue
                need_more = 0
                none_count = 0
                for dict in counting_dict_list :
                    if dict['amount_int'] == None :
                        none_count = none_count + len(dict['saved_windows_list'])
                        if len(dict['saved_windows_list']) > 0 :
                            continue
                        else :
                            #print "dict[['amount_int'] == None and len(dict['saved_windows_list']) > 0, setting need_more = 1"
                            need_more = 1
                            break
                    if len(dict['saved_windows_list']) < dict['amount_int'] :
                        #print "len(dict['saved_windows_list']) (%s) dict['amount_int'] (%s) setting need_more = 1" % (len(dict['saved_windows_list']), dict['amount_int'])
                        need_more = 1
                        break
                if need_more == 0 :
                    #candidate_tuple_list.append((none_count, counting_dict_list))
                    #candidate_tuple_list.append((none_count, copy.deepcopy(counting_dict_list)))
                    #break
                    if new_res.has_key('max_resource_int') and new_res['max_resource_int'] != None :
                        window_count = min(none_count, new_res['max_resource_int'])
                    else :
                        window_count = none_count
                    candidate_tuple_list.append((-(window_count), -(copy.copy(current_start_time_float)), copy.deepcopy(counting_dict_list)))
            if len(candidate_tuple_list) > 0 :
                candidate_tuple_list.sort()
                #candidate_tuple_list.reverse()
                counting_dict_list = candidate_tuple_list[-1][2]
                #print "setting current_start_time to last candidate_tuple_list"
                current_start_time_float = -(candidate_tuple_list[-1][1])
                sized_windows_list = []
                for counting_dict in counting_dict_list :
                    sized_windows_list = sized_windows_list + counting_dict['saved_windows_list']
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "len(candidate_tuple_list) (%s)" % len(candidate_tuple_list)
                    print "len(sized_windows_list) (%s)" % len(sized_windows_list)
            else :
                if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                    print "zero-length candidate_tuple_list"
                sized_windows_list = []
        else :
            if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                print "zero-length sized_windows_lists_list"
            sized_windows_list = []
    #print "2. len(sized_windows_list) (%s)" % len(sized_windows_list)
    if need_more == 1 :
        if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
            print "need_more == 1"
        #print "need_more == 1, setting sized_windows_list to []"
        sized_windows_list = []
    #print "get_sized_dict_list: counting_dict_list (%s)" % (counting_dict_list,)
    #print "after for loop 4"
    #if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        #continue_var = raw_input("continue? ")
    #print "final current_start_time (%s)" % time.asctime(time.localtime(current_start_time_float))
    return ( sized_windows_list, current_start_time_float, counting_dict_list, node_bin_dict )

def get_sorted_windows_list(sized_windows_list, new_res, accepted_nodes_list, sort_policy_code, resources_db_handle, reservations_db_handle, jobs_db_handle, windows_and_reservations_list=None) :
    #print "in get_sorted"
    # The ideal way for the sort policy to work, is to allow the
    # administrator to specify a set of sort policies and the order in
    # which they are to be applied.  Since the built-in sort() will
    # sort lexicographically, each policy can be expressed as a number
    # for the window, and the order of each policy can be its place in
    # the sort tuple.
    def get_sorted(input_tuple) :
        # (start, end, node name)
        # this should sort first on window start time
        # and second on resource usability
        windows = input_tuple[0]
        nodes = input_tuple[1]
        new_res = input_tuple[2]
        resources_dict = input_tuple[5]
        temp_windows = []
        for window in windows :
            if resources_dict[window[2]['nodename']].has_key('resource_usability_int') :
                temp_windows.append((- (window[0]), long(resources_dict[window[2]]['resource_usability_int']), window))
            else :
                temp_windows.append((- (window[0]), 0L, window))
        temp_windows.sort()
        result = []
        for window in temp_windows :
            result.append(window[-1]['nodename'])
        return result

    def get_sorted_with_affinity(input_tuple) :
        windows = input_tuple[0]
        nodes = input_tuple[1]
        new_res = input_tuple[2]
        windows_and_reservations_list = input_tuple[4]
        resources_dict = input_tuple[5]
        reservations_dict = input_tuple[6]
        jobs_dict = input_tuple[7]
        temp_windows = []
        
        for window_tuple in windows_and_reservations_list :
            window = window_tuple[0]
            relevant_reservations_list = window_tuple[1]
            job_step = window_tuple[2]
            # find those "pushback" reservations on this node
            # decrement affinity for each pushback res on the node
            pushback_affinity = 0
            for res_key in reservations_dict.keys() :
                if reservations_dict[res_key]['purpose_type_string'] == "pushback" :
                    for allocated_dict in reservations_dict[res_key]['allocated_dict_list'] :
                        if allocated_dict['nodename'] == window_tuple[0][2] :
                            pushback_affinity = pushback_affinity + jobs_dict[reservations_dict[res_key]['job_runID']]['priority_int']
            affinity = 0
            for reservation in relevant_reservations_list :
                input_tuple = (job_step,)
                if reservation['affinity_calculation'] != None :
                    result = 0
                    reservation_affinity_code = reservation['affinity_calculation']
                    exec reservation_affinity_code in globals(), locals()
                else :
                    result = 1
                affinity = affinity + result
            if resources_dict[window_tuple[0][2]['nodename']].has_key('resource_usability_int') :
                resource_sort_int = resources_dict[window_tuple[0][2]['nodename']]['resource_usability_int']
            else :
                resource_sort_int = 0
            if affinity == 0 :
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "sort components (%s) (%s)" % (window[0], resource_sort_int)
                sort_tuple = ( 0, -(pushback_affinity), -(window[0]), long(resource_sort_int), window)
            else :
                sort_tuple = (-(affinity), -(pushback_affinity), -(window[0]), long(resource_sort_int), window)
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "sort_tuple (%s)" % (sort_tuple,)
            temp_windows.append(sort_tuple)
        temp_windows.sort()
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            for sort_tuple in temp_windows :
                print "post-sort sort_tuple (%s)" % (sort_tuple,)
        windows = []
        for window_tuple in temp_windows :
            windows.append(window_tuple[-1])
        result = windows
        return result

    temp_nodes_list = accepted_nodes_list[:]
    resources_dict = resources_db_handle[0]
    reservations_dict = reservations_db_handle[0]
    jobs_dict = jobs_db_handle[0]
    input_tuple = (sized_windows_list,temp_nodes_list,new_res,Now_float,windows_and_reservations_list,resources_dict, reservations_dict, jobs_dict)
    if windows_and_reservations_list != None :
        sorted_windows_list_list = get_sorted_with_affinity(input_tuple)
    elif sort_policy_code == None :
        sorted_windows_list_list = get_sorted(input_tuple)
    else :
        sorted_windows_list_list = apply_policy_code(sort_policy_code, input_tuple)
    if DEBUGJOB != None and new_res.has_key('job_runID') and DEBUGJOB == new_res['job_runID'] :
        print "windows"
        for window in sorted_windows_list_list :
            print "%s %s %s" % (time.asctime(time.localtime(window[0])),time.asctime(time.localtime(window[1])),window[2])
        print "end windows"
    return sorted_windows_list_list

def get_chosen_nodes_list(new_res, sorted_windows_list, counting_dict_list, node_bin_dict, resources_db_handle) :
    def getfirstindex(x) :
        return(x[1])
    # for multiple job/node, need to construct a resoure_dict_list based
    # on initiatormap.  For each initiator set, choose a node that
    # matches, deduct the requested_cpu and requested_memory from that
    # node, then go on to the next initiator set.  Windows should be
    # sorted already...Need to sort the initiator sets...Maybe should
    # save the sorted list from get_sized?
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "in get_chosen"
    #print "get_chosen_nodes_list: counting_dict_list (%s)" % (counting_dict_list,)
    chosen_nodes_list = []
    resource_amount_int = new_res['resource_amount_int']
    #resource_dict_list = new_res['resource_dict_list']
    #counting_dict_list = resource_dict_list
    for counting_dict in counting_dict_list :
        counting_dict['saved_windows_list'] = []
    max_resource_int = new_res['max_resource_int']
    # need to allow for case where a node has enough cpu and memory
    # for two initiator sets...
    for nodename in node_bin_dict.keys() :
        node_bin_dict[nodename]['remaining_node'] = node_bin_dict[nodename]['node']
        node_bin_dict[nodename]['remaining_cpu'] = node_bin_dict[nodename]['cpu']
        node_bin_dict[nodename]['remaining_memory'] = node_bin_dict[nodename]['memory']
    # for each element in the counting_dict_list, accumulate sorted_windows
    # in order until the cpu and memory requirement for all initiators is
    # met.  Choose that node.  It may be possible to miss a good combination...
    # counting_dict_list should be sorted by viable_request_count from
    # get_sized...re-sort here, according to len(resource_dict.keys())
    # That should ensure that the reqs get filled.
    sorting_resource_dict_list = []
    for counting_dict in counting_dict_list :
        sorting_resource_dict_list.append((len(counting_dict['resource_dict'].keys()), counting_dict))
    sorting_resource_dict_list.sort()
    counting_dict_list = map(getfirstindex, sorting_resource_dict_list)
    consumed_cpu_memory_dict = {}
    allocated_dict_list = []
    for counting_dict in counting_dict_list :
        # right here, may need to do something special if requested_cpu
        # or requested_memory is 0.
        # if amount_int for this dict is None, then grab as many
        # nodes as are available.
        # if requested_cpu is None or not present, grab all cpu
        # on each node.  same for memory
        if not counting_dict.has_key('requested_node') or counting_dict['requested_node'] == None :
            requested_node = None
        else :
            requested_node = counting_dict['requested_node']
        if not counting_dict.has_key('requested_cpu') or counting_dict['requested_cpu'] == None :
            requested_cpu = None
        else :
            requested_cpu = counting_dict['requested_cpu']
        if not counting_dict.has_key('requested_memory') or counting_dict['requested_memory'] == None :
            requested_memory = None
        else :
            requested_memory = counting_dict['requested_memory']
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "requested_node (%s)" % requested_node
            print "requested_cpu (%s)" % requested_cpu
            print "requested_memory (%s)" % requested_memory
        #print "requested_node (%s)" % requested_node
        #print "requested_cpu (%s)" % requested_cpu
        #print "requested_memory (%s)" % requested_memory
        node_cpu_memory_dict = {}
        #print "in get_chosen, counting_dict check setting need_more = 1"
        need_more = 1
        #print "len(sorted_windows_list) (%s)" % len(sorted_windows_list)
        for window in sorted_windows_list :
            #print "sorted_window (%s)" % (window,)
            nodename = window[2]['nodename']
            if not nodename in counting_dict['resource_dict'].keys() :
                continue
            if consumed_cpu_memory_dict.has_key(nodename) :
                consumed_node = consumed_cpu_memory_dict[nodename]['node']
                consumed_cpu = consumed_cpu_memory_dict[nodename]['cpu']
                consumed_memory = consumed_cpu_memory_dict[nodename]['memory']
            else :
                consumed_node = 0
                consumed_cpu = 0
                consumed_memory = 0
            if not node_cpu_memory_dict.has_key(nodename) :
                node_cpu_memory_dict[nodename] = {
                  'node' : 0,
                  'cpu' : 0,
                  'memory' : 0
                  }
            if window[2].has_key('node') :
                node_cpu_memory_dict[nodename]['node'] = node_cpu_memory_dict[nodename]['node'] + window[2]['node']
            if window[2].has_key('cpu') and window[2]['cpu'] != None :
                node_cpu_memory_dict[nodename]['cpu'] = node_cpu_memory_dict[nodename]['cpu'] + window[2]['cpu']
            if window[2].has_key('memory') and window[2]['memory'] != None :
                node_cpu_memory_dict[nodename]['memory'] = node_cpu_memory_dict[nodename]['memory'] + window[2]['memory']
            #print "nodename (%s)" % nodename
            #print "window (%s)" % (window,)
            #print "node_cpu_memory_dict[nodename]['node'] (%s)" % node_cpu_memory_dict[nodename]['node']
            #print "node_cpu_memory_dict[nodename]['cpu'] (%s)" % node_cpu_memory_dict[nodename]['cpu']
            #print "node_cpu_memory_dict[nodename]['memory'] (%s)" % node_cpu_memory_dict[nodename]['memory']
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "nodename (%s)" % nodename
                print "window (%s)" % (window,)
                print "node_cpu_memory_dict[nodename]['node'] (%s)" % node_cpu_memory_dict[nodename]['node']
                print "node_cpu_memory_dict[nodename]['cpu'] (%s)" % node_cpu_memory_dict[nodename]['cpu']
                print "node_cpu_memory_dict[nodename]['memory'] (%s)" % node_cpu_memory_dict[nodename]['memory']
                #continue_var = raw_input("continue? ")
            if requested_node == None or requested_cpu == None or requested_memory == None :
                if requested_node == None :
                    if consumed_cpu_memory_dict.has_key(nodename) :
                        allocated_node = 1 - consumed_cpu_memory_dict[nodename]['node']
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "requested_node is None, consumed_cpu_memory_dict.has_key(%s) allocated_node (%s)" % (nodename, allocated_node)
                    else :
                        allocated_node = 1
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "requested_node is None, NOT consumed_cpu_memory_dict.has_key(%s) allocated_node (%s)" % (nodename, allocated_node)
                else :
                    allocated_node = requested_node
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "requested_node is (%s), allocated_node (%s)" % (requested_node, allocated_node)
                if requested_cpu == None :
                    if consumed_cpu_memory_dict.has_key(nodename) :
                        allocated_cpu = resources_db_handle[0][nodename]['ConsumableCpus'] - consumed_cpu_memory_dict[nodename]['cpu']
                    else :
                        allocated_cpu = resources_db_handle[0][nodename]['ConsumableCpus']
                else :
                    allocated_cpu = requested_cpu
                if requested_memory == None :
                    if consumed_cpu_memory_dict.has_key(nodename) :
                        allocated_memory = resources_db_handle[0][nodename]['ConsumableMemory'] - consumed_cpu_memory_dict[nodename]['memory']
                    else :
                        allocated_memory = resources_db_handle[0][nodename]['ConsumableMemory']
                else :
                    allocated_memory = requested_memory
                if not consumed_cpu_memory_dict.has_key(nodename) and node_cpu_memory_dict[nodename]['node'] - consumed_node >= allocated_node and \
                  node_cpu_memory_dict[nodename]['cpu'] - consumed_cpu >= allocated_cpu and \
                  node_cpu_memory_dict[nodename]['memory'] - consumed_memory >= allocated_memory :
                    if consumed_cpu_memory_dict.has_key(nodename) :
                        consumed_cpu_memory_dict[nodename]['node'] = consumed_cpu_memory_dict[nodename]['node'] + allocated_node
                        consumed_cpu_memory_dict[nodename]['cpu'] = consumed_cpu_memory_dict[nodename]['cpu'] + allocated_cpu
                        consumed_cpu_memory_dict[nodename]['memory'] = consumed_cpu_memory_dict[nodename]['memory'] + allocated_memory
                    else :
                        consumed_cpu_memory_dict[nodename] = \
                          { 'node' : allocated_node,
                            'cpu' : allocated_cpu,
                            'memory' : allocated_memory
                          }
                    allocated_dict_list.append(
                      {'nodename' : window[2]['nodename'],
                       'type' : new_res['node_usage'],
                       'initiatormap_index' : counting_dict['initiatormap_index'],
                       'node' : allocated_node,
                       'cpu' : allocated_cpu,
                       'memory' : allocated_memory}
                      )
                else :
                    #print "not enough node (%s) cpu (%s) memory (%s)" % (node_cpu_memory_dict[nodename]['node'] - consumed_node, node_cpu_memory_dict[nodename]['cpu'] - consumed_cpu, node_cpu_memory_dict[nodename]['memory'] - consumed_memory)
                    #print "node_cpu_memory_dict (%s)" % node_cpu_memory_dict
                    #print "requested_node (%s) requested_cpu (%s) requested_memory (%s)" % (requested_node, requested_cpu, requested_memory)
                    continue
            else :
                if not consumed_cpu_memory_dict.has_key(nodename) and node_cpu_memory_dict[nodename]['node'] - consumed_node >= requested_node and \
                  node_cpu_memory_dict[nodename]['cpu'] - consumed_cpu >= requested_cpu and \
                  node_cpu_memory_dict[nodename]['memory'] - consumed_memory >= requested_memory :
                    if counting_dict['node_usage'] == 'node_shared' :
                        final_requested_node = 0
                    else :
                        final_requested_node = requested_node
                    if consumed_cpu_memory_dict.has_key(nodename) :
                        consumed_cpu_memory_dict[nodename]['node'] = consumed_cpu_memory_dict[nodename]['node'] + requested_node
                        consumed_cpu_memory_dict[nodename]['cpu'] = consumed_cpu_memory_dict[nodename]['cpu'] + requested_cpu
                        consumed_cpu_memory_dict[nodename]['memory'] = consumed_cpu_memory_dict[nodename]['memory'] + requested_memory
                    else :
                        consumed_cpu_memory_dict[nodename] = \
                          { 'node' : final_requested_node,
                            'cpu' : requested_cpu,
                            'memory' : requested_memory
                          }
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "Nothing is None, nodename (%s) final_requested_node is (%s)" % (nodename, final_requested_node)
                    #print "Nothing is None, nodename (%s) final_requested_node is (%s)" % (nodename, final_requested_node)
                    allocated_dict_list.append(
                      {'nodename' : window[2]['nodename'],
                       'type' : new_res['node_usage'],
                       'initiatormap_index' : counting_dict['initiatormap_index'],
                       'node' : final_requested_node,
                       'cpu' : requested_cpu,
                       'memory' : requested_memory}
                      )
                else :
                    #print "not enough node (%s) cpu (%s) memory (%s)" % (node_cpu_memory_dict[nodename]['node'] - consumed_node, node_cpu_memory_dict[nodename]['cpu'] - consumed_cpu, node_cpu_memory_dict[nodename]['memory'] - consumed_memory)
                    #print "node_cpu_memory_dict (%s)" % node_cpu_memory_dict
                    #print "requested_node (%s) requested_cpu (%s) requested_memory (%s)" % (requested_node, requested_cpu, requested_memory)
                    continue
            chosen_nodes_list.append(window[2]['nodename'])
            #print "allocated_dict_list (%s)" % (allocated_dict_list,)
            if counting_dict['amount_int'] == None and not (max_resource_int != None and len(chosen_nodes_list) >= max_resource_int) :
                # choose _all_ nodes that meet requested_cpu and
                # requested_memory, under max if present
                #print "counting_dict['amount_int'] == None, setting need_more = 1"
                need_more = 1
            else :
                #print "counting_dict['amount_int'] (%s), max_resource_int (%s)" % (counting_dict['amount_int'], max_resource_int)
                need_more = 0
                break
    sorting_allocated_dict_list = []
    for allocated_dict in allocated_dict_list :
        sorting_allocated_dict_list.append((allocated_dict['initiatormap_index'], allocated_dict))
    sorting_allocated_dict_list.sort()
    allocated_dict_list = map(getfirstindex, sorting_allocated_dict_list)
    return (chosen_nodes_list, allocated_dict_list)

def insert_new_object_with_key(key, new_object, db_handle) :
    dict = db_handle[0]
    if dict.has_key(key) :
        raise 'KeyExists', key
    dict[key] = new_object

def insert_new_object(new_object, db_handle) :
    dict = db_handle[0]
    dict[new_object['name']] = new_object

def update_object_attribute(name, value, object, db_handle) :
    dict = db_handle[0]
    temp_object = copy.copy(dict[object['name']])
    temp_object[name] = value
    dict[temp_object['name']] = temp_object

def update_object_attributes(names_values_dict, object, db_handle) :
    dict = db_handle[0]
    temp_object = copy.copy(dict[object['name']])
    for key in names_values_dict.keys() :
        temp_object[key] = names_values_dict[key]
    dict[object['name']] = temp_object

def delete_object(name, db_handle) :
    dict = db_handle[0]
    if dict.has_key(name) :
        del dict[name]
    else :
        print "name (%s) does not exist!" % name
        raise 'NameNotInDB', name

def bind_job_to_reservation(job_step_id_string, reservation_name_string, events_db_handle, jobs_db_handle) :
    temp_job_step = get_object(job_step_id_string, jobs_db_handle)
    reservation_binding_list = temp_job_step['reservation_binding']
    if reservation_binding_list == None :
        reservation_binding_list = [reservation_name_string]
    else :
        reservation_binding_list.append(reservation_name_string)
    update_object_attribute('reservation_binding', reservation_binding_list, temp_job_step, jobs_db_handle)

def unbind_job_from_reservation(job_step_id_string, reservation_name_string, events_db_handle, jobs_db_handle) :
    temp_job_step = get_object(job_step_id_string, jobs_db_handle)
    reservation_binding_list = temp_job_step['reservation_binding']
    new_reservation_binding_list = filter(
      lambda reservation_binding_string, reservation_name=reservation_name_string : reservation_binding_string != reservation_name,
      reservation_binding_list
    )
    update_object_attribute('reservation_binding', new_reservation_binding_list, temp_job_step, jobs_db_handle)

def bind_reservation_to_job(reservation_id_string, job_step_id_string, events_db_handle, reservations_db_handle) :
    temp_res_step = get_object(reservation_id_string, reservations_db_handle)
    if not temp_res_step.has_key('job_binding') :
        temp_res_step['job_binding'] = None
    job_binding_list = temp_res_step['job_binding']
    if job_binding_list == None :
        job_binding_list = [job_step_id_string]
    else :
        job_binding_list.append(job_step_id_string)
    job_restriction = "if input_tuple[0]['name'] in " + `job_binding_list` + \
      ' : result = 0'
    print "job_restriction (%s)" % job_restriction
    update_object_attribute('job_binding', job_binding_list, temp_res_step, reservations_db_handle)
    update_object_attribute('job_restriction', job_restriction, temp_res_step, reservations_db_handle)

def unbind_reservation_from_job(reservation_id_string, job_step_id_string, events_db_handle, reservations_db_handle) :
    temp_res_step = get_object(reservation_id_string, reservations_db_handle)
    job_binding_list = temp_res_step['job_binding']
    new_job_binding_list = filter(
      lambda job_binding_string, job_id=job_step_id_string : job_binding_string != job_id,
      job_binding_list
    )
    if len(new_job_binding_list) == 0 :
        job_restriction = "if input_tuple[0]['user'] == '" + temp_res_step['creator_string'] + "' : result = 0"
    else :
        job_restriction = "if input_tuple[0]['name'] in " + `new_job_binding_list` + \
          ' : result = 0'
    print "job_restriction (%s)" % job_restriction
    update_object_attribute('job_binding', new_job_binding_list, temp_res_step, reservations_db_handle)
    update_object_attribute('job_restriction', job_restriction, temp_res_step, reservations_db_handle)

def move_old_jobs(events_db_handle, jobs_db_handle, old_jobs_db_handle) :
    update_job_info(jobs_db_handle)
    jobs_dict = jobs_db_handle[0]
    new_job_steps_dict = get_job_steps_dict()
    new_job_steps_names_list = new_job_steps_dict.keys()
    job_step_list = get_object_list(jobs_db_handle)
    for job_step in job_step_list :
        if not job_step['name'] in new_job_steps_names_list \
          and not jobs_dict[job_step['name']]['state'] in ['Idle', 'Running', 'Starting'] :
            key = get_new_db_key(old_jobs_db_handle)
            try :
                insert_new_object_with_key(key, job_step, old_jobs_db_handle)
            except 'KeyExists', key :
                print "The key (%s) already exists in the db" % key
            except :
                continue
            else :
                try :
                    delete_object(job_step['name'], jobs_db_handle)
                except :
                    continue

def move_old_reservations(events_db_handle, reservations_db_handle, old_reservations_db_handle) :
    reservations_list = get_object_list(reservations_db_handle)
    for reservation in reservations_list :
        if reservation.has_key('epiloguerc') \
          and reservation['epiloguerc'] != 0 :
            continue
        if reservation['end_time_float'] < Now_float :
            key = get_new_db_key(old_reservations_db_handle)
            try :
                insert_new_object_with_key(key, reservation, old_reservations_db_handle)
            except 'KeyExists', key :
                print "The key (%s) already exists in the db" % key
            except :
                continue
            else :
                try :
                    delete_object(reservation['name'], reservations_db_handle)
                except :
                    continue

def update_job_info(jobs_db_handle) :
    dict = jobs_db_handle[0]
    new_job_steps_dict = get_job_steps_dict()
    new_job_ids = new_job_steps_dict.keys()
    old_job_steps_list = get_object_list(jobs_db_handle)
    old_job_ids = get_object_names_list(old_job_steps_list)
    attribute_list = JOB_UPDATE_ATTRIBUTE_list
    for new_job_step in new_job_steps_dict.values() :
        if new_job_step['name'] in old_job_ids :
            # Prevent default values in the new job step
            # from overwriting the old info, for certain
            # attributes...should clean this up...
            # Probably should do it the other way: only
            # change attributes that should be updated...
            old_job_step = dict[new_job_step['name']]
            temp_job_step = copy.copy(old_job_step)
            for key in attribute_list :
                if new_job_step.has_key(key) :
                    temp_job_step[key] = new_job_step[key]
            temp_job_step['last_seen_time_float'] = Now_float
            update_object_attributes( temp_job_step, old_job_step,
              jobs_db_handle)
        else :
            new_job_step['last_seen_time_float'] = Now_float
            insert_new_object(new_job_step, jobs_db_handle)
    for old_job_step in old_job_steps_list :
        if old_job_step['name'] in new_job_ids \
          and new_job_steps_dict[old_job_step['name']]['state'] == "Hold" :
            if DEBUGJOB != None and old_job_step['name'] == DEBUGJOB :
                print "setting system_queue_time to None for job (%s)" % old_job_step['name']
            update_object_attribute('system_queue_time', None, old_job_step,
              jobs_db_handle)
            
        if old_job_step['name'] not in new_job_ids \
          and old_job_step.has_key('last_seen_time_float') \
          and ( LOST_JOB_LIMIT == None \
          or Now_float - old_job_step['last_seen_time_float'] > LOST_JOB_LIMIT ) :
            update_object_attribute('state', 'Unknown', old_job_step,
              jobs_db_handle)
            if LOST_JOB_WARN == 'TRUE' and old_job_step['state'] == 'Running' :
                recipient = MAIL_RECIPIENT
                subject = "LOST_JOB_LIMIT exceeded"
                message = " Running Job (%s) is no longer detected.  State has been changed to Unknown.  " % old_job_step['name'] 
                warn(message, subject, recipient)
                catsyslog(message,'notice')

def get_QOS_priority(QOS) :
    try :
        if string.atoi(QOS) < len(QOS_PRIORITY_dict.keys()) :
            return QOS_PRIORITY_dict[QOS]
        else :
            return 0L
    except :
        return 0L

def get_QOS_target_expansion_factor(QOS) :
    # This returns the target expansion factor for
    # the different QOSs.
    try :
        if string.atoi(QOS) < len(QOS_TARGETXF_dict.keys()) :
            return QOS_TARGETXF_dict[QOS]
        else :
            return None
    except :
        return None

def get_QOS_target_queue_wait_time(QOS) :
    # This function returns a target queue wait time
    # for a given QOS.
    try :
        if string.atoi(QOS) < len(QOS_TARGETQT_dict.keys()) :
            return QOS_TARGETQT_dict[QOS]
        else :
            return None
    except :
        return None

def update_job_priorities (jobs_db_handle) :
    # priority calculation:
    # resource_number * RESOURCE_WEIGHT
    # expansion factor * EXPANSION_FACTOR_WEIGHT
    # system queue time * SYSTEM_QUEUE_TIME_WEIGHT
    # submit time * SUBMIT_TIME_WEIGHT
    # QOS_priority * QOS_PRIORITY_WEIGHT
    # QOS_target_expansion_factor * IGHT
    # QOS_target_queue_wait_time * QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT
    # Adjust these weights to emphasize different elements of the
    # priority calculation
    #RESOURCE_WEIGHT = 1100.0
    #EXPANSION_FACTOR_WEIGHT = 10.0
    #SYSTEM_QUEUE_TIME_WEIGHT = 0.1
    #SUBMIT_TIME_WEIGHT = 0.0
    #QOS_PRIORITY_WEIGHT = 6000.0
    #IGHT = 1
    #QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT = 1
    jobs_shelf = jobs_db_handle[0]
    for key in jobs_shelf.keys() :
        # Set up a dict to store priority calculation terms for later reports
        priority_element_dict = {
          'RESOURCE_WEIGHT'                    : RESOURCE_WEIGHT,
          'LOCAL_ADMIN_WEIGHT'                 : LOCAL_ADMIN_WEIGHT,
          'LOCAL_USER_WEIGHT'                  : LOCAL_USER_WEIGHT,
          'EXPANSION_FACTOR_WEIGHT'            : EXPANSION_FACTOR_WEIGHT,
          'SYSTEM_QUEUE_TIME_WEIGHT'           : SYSTEM_QUEUE_TIME_WEIGHT,
          'SUBMIT_TIME_WEIGHT'                 : SUBMIT_TIME_WEIGHT,
          'FAIRSHARE_BONUS_WEIGHT'             : FAIRSHARE_BONUS_WEIGHT,
          'WALL_TIME_WEIGHT'                   : WALL_TIME_WEIGHT,
          'QOS_PRIORITY_WEIGHT'                : QOS_PRIORITY_WEIGHT,
          'QOS_TARGET_EXPANSION_FACTOR_WEIGHT' : QOS_TARGET_EXPANSION_FACTOR_WEIGHT,
          'QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT'  : QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT
        }

        temp_job = jobs_shelf[key]

        # Get number of resources from initiatormap
        # Should really object-orient the resources
        # each Resource should have its own methods for:
        # allocate, de-allocate, get_status...
        # This would make it possible to have a Resource.node
        # Resource.cpu, Resource.license, Resource.switch_port, etc...
        resourcemap_list = string.split(jobs_shelf[key]['initiatormap'], '+')
        resource_number = len(resourcemap_list)

        # Get local priority from local_priority_string
        if temp_job.has_key('local_admin_priority_string') and \
          temp_job['local_admin_priority_string'] != None :
            try :
                local_admin_float = float(temp_job['local_admin_priority_string'])
            except :
                local_admin_float = 0.0
        else :
            local_admin_float = 0.0
        if temp_job.has_key('local_user_priority_string') and \
          temp_job['local_user_priority_string'] != None :
            try :
                local_user_float = float(temp_job['local_user_priority_string']) 
                if local_user_float > 0.0 :
                    local_user_float = 0.0
            except :
                local_user_float = 0.0
        else :
            local_user_float = 0.0

        #feature 582
        if temp_job.has_key('fairshare_value') and \
          temp_job['fairshare_value'] != None :
            try :
                fairshare_value_float = float(temp_job['fairshare_value'])
            except :
                fairshare_value_float = 0.0
        else :
            fairshare_value_float = 0.0
        
        # Calculate expansion factor
        expansion_factor = \
          ( Now_float -
              float(temp_job['speculative_system_queue_time']) + \
            float(temp_job['wall_clock_limit']) ) / \
          float(temp_job['wall_clock_limit'])

        # Calculate queue wait time
        queue_wait_time = Now_float - float(temp_job['speculative_system_queue_time'])

        # Calculate submit wait time
        submit_wait_time = Now_float - float(temp_job['SubmitTime'])

        # Get wall clock time
        wall_clock_time = temp_job['wall_clock_limit']

        # Get QOS priority
        try :
            QOS_priority = float(get_QOS_priority(temp_job['QOS']))
        except :
            print "Bad QOS_priority for job (%s)!" % temp_job['name']
            QOS_priority = 0.0

        # Get QOS target expansion factor
        QOS_target_expansion_factor = get_QOS_target_expansion_factor(temp_job['QOS'])
        if QOS_target_expansion_factor == None :
            QOS_target_xf_value = 0
        else :
            if expansion_factor >= QOS_target_expansion_factor :
                QOS_target_xf_value = float(Catalina.QOS_MAX_PRIORITY_dict[temp_job['QOS']])
            else :
                QOS_target_xf_value = 1 / (QOS_target_expansion_factor - expansion_factor)
            if QOS_target_xf_value > float(Catalina.QOS_MAX_PRIORITY_dict[temp_job['QOS']]) :
                QOS_target_xf_value = float(Catalina.QOS_MAX_PRIORITY_dict[temp_job['QOS']])
        # Get QOS target queue time
        QOS_target_queue_wait_time = get_QOS_target_queue_wait_time(temp_job['QOS'])
        if QOS_target_queue_wait_time == None :
            QOS_target_qwt_value = 0
        else :
            if queue_wait_time >= QOS_target_queue_wait_time :
                QOS_target_qwt_value = float(QOS_MAX_PRIORITY_dict[temp_job['QOS']])
            else :
                QOS_target_qwt_value = 1 / (QOS_target_queue_wait_time - queue_wait_time)
            if QOS_target_qwt_value > float(QOS_MAX_PRIORITY_dict[temp_job['QOS']]) :
                QOS_target_qwt_value = float(QOS_MAX_PRIORITY_dict[temp_job['QOS']])

        priority_element_dict['resource_number'] = resource_number
        priority_element_dict['local_admin_float'] = local_admin_float
        priority_element_dict['local_user_float'] = local_user_float
        priority_element_dict['expansion_factor'] = expansion_factor
        priority_element_dict['queue_wait_time'] = queue_wait_time
        priority_element_dict['submit_wait_time'] = submit_wait_time
        priority_element_dict['wall_clock_time'] = wall_clock_time
        priority_element_dict['QOS_priority'] = QOS_priority
        priority_element_dict['QOS_target_xf_value'] = QOS_target_xf_value
        priority_element_dict['QOS_target_qwt_value'] = QOS_target_qwt_value

        if temp_job['system_priority_int'] != None :
            #priority = long(float( MAXPRIORITY * max_places_float)) + \
            #  long(temp_job['system_priority_int'] )
            priority = long(MAXPRIORITY) * long(max_places_float) + \
              long(temp_job['system_priority_int'] )
        else :
            priority = \
                long(resource_number * 100) * RESOURCE_WEIGHT + \
                long(local_admin_float * 100) * LOCAL_ADMIN_WEIGHT + \
                long(local_user_float * 100) * LOCAL_USER_WEIGHT + \
                long(expansion_factor * 100) * EXPANSION_FACTOR_WEIGHT + \
                long(queue_wait_time * 100) * SYSTEM_QUEUE_TIME_WEIGHT + \
                long(submit_wait_time * 100) * SUBMIT_TIME_WEIGHT + \
                long(fairshare_value_float * 100) * FAIRSHARE_BONUS_WEIGHT + \
                long(wall_clock_time * 100) * WALL_TIME_WEIGHT + \
                long(QOS_priority * 100) * QOS_PRIORITY_WEIGHT + \
                long(QOS_target_xf_value * 100) * QOS_TARGET_EXPANSION_FACTOR_WEIGHT + \
                long(QOS_target_qwt_value * 100) * QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT
            if QOS_MAX_PRIORITY_dict.has_key(temp_job['QOS']) :
                max_pri = long(float(QOS_MAX_PRIORITY_dict[temp_job['QOS']]) * max_places_float ) * 100L
            else :
                max_pri = long(MAXPRIORITY * max_places_float) * 100L
            #if priority > max_pri - max_pri * 0.1 :
            #    priority =  (priority * max_pri)/(priority + max_pri * 0.1)
            if priority > max_pri - (max_pri / 10) :
                priority =  (priority * max_pri * 10)/(priority * 10 + max_pri)
        update_object_attribute('priority', priority, temp_job, jobs_db_handle)
        update_object_attribute('priority_element_dict', priority_element_dict, temp_job, jobs_db_handle)

#def cancel_risk_jobs(reservations_db_handle,
#                     jobs_db_handle,
#                     events_db_handle,
#                     resources_db_handle) :
#    jobs_dict = jobs_db_handle[0]
#    reservations_dict = reservations_db_handle[0]
#    resources_dict = resources_db_handle[0]
#    job_steps_list = get_object_list(jobs_db_handle)
#    reservations_list = get_object_list(reservations_db_handle)
#    for job_step in job_steps_list :
#        if ( job_step['state'] == 'Starting' or \
#          job_step['state'] == 'Running' ) and \
#          job_step.has_key('run_at_risk_int') and \
#          job_step['run_at_risk_int'] >= 1 :
#            # Check to see if job is run_at_risk, and if so,
#            # whether its reservation conflicts with any job
#            # reservations
#            job_reservations_list = filter( lambda x,job_step=job_step : x['job_runID'] == job_step['name'], reservations_list)
#            for job_reservation in job_reservations_list :
#                job_node_list = job_reservation['node_list']
#                job_end_time = job_reservation['end_time_float']
#                job_start_time = job_reservation['start_time_float']
#            try :
#                for reservation in reservations_list :
#                    if not ( reservation.has_key('job_runID') and \
#                      jobs_dict.has_key(reservation['job_runID']) and \
#                      jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
#                      jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1 ) and \
#                      reservation['start_time_float'] <= \
#                      Now_float + RUN_AT_RISK_CLEANUP_TIME and \
#                      ( reservation['start_time_float'] < job_end_time <= reservation['end_time_float'] or \
#                      reservation['start_time_float'] <= job_start_time < reservation['end_time_float'] ) :
#                        for node in job_node_list :
#                            if node in reservation['node_list'] :
#                                input_tuple = ( job_step, )
#                                result = apply_policy_code(
#                                  reservation['job_restriction'],
#                                  input_tuple)
#                                if result != 0 :
#                                    raise "FoundConflict"
#            except "FoundConflict" :
#                # Cancel the job
#                if SERVERMODE == 'NORMAL' :
#                    try :
#                        cancel_job(job_step, events_db_handle)
#                    except 'CancelJobFailure', failed_job_step :
#                        print "cancel of %s failed" % \
#                          failed_job_step['name']
#                    except :
#                        continue
#                else :
#                    print "%s should be canceled!" % job_step['name']

def cancel_risk_reservations(reservations_db_handle,
                             jobs_db_handle,
                             events_db_handle,
                             resources_db_handle) :
#    def nonrunning_risky(this_res, jobs_dict=jobs_dict) :
#        return this_res.has_key('job_runID') and \
#          jobs_dict.has_key(this_res['job_runID']) and \
#          this_res['purpose_type_string'] != 'running' and \
#          ( not jobs_dict[this_res['job_runID']].has_key('start_count_int') or \
#          jobs_dict[this_res['job_runID']]['start_count_int'] < 1 ) and \
#          jobs_dict[this_res['job_runID']].has_key('run_at_risk_int') and \
#          jobs_dict[this_res['job_runID']]['run_at_risk_int'] >= 1

    jobs_dict = jobs_db_handle[0]
    def risky(this_res, jobs_dict=jobs_dict) :
        return this_res.has_key('job_runID') and \
          jobs_dict.has_key(this_res['job_runID']) and \
          ((jobs_dict[this_res['job_runID']].has_key('run_at_risk_int') and \
          jobs_dict[this_res['job_runID']]['run_at_risk_int'] >= 1) or \
          (this_res['purpose_type_string'] in ['running',] and jobs_dict[this_res['job_runID']].has_key('preemptible') and \
          jobs_dict[this_res['job_runID']]['preemptible'] >= 1))
    def approaching_nonrisky(this_res, jobs_dict=jobs_dict) :
        return not ( this_res.has_key('job_runID') and \
          jobs_dict.has_key(this_res['job_runID']) and \
          ((jobs_dict[this_res['job_runID']].has_key('run_at_risk_int') and \
          jobs_dict[this_res['job_runID']]['run_at_risk_int'] >= 1 ) or \
          (this_res['purpose_type_string'] in ['running',] and jobs_dict[this_res['job_runID']].has_key('preemptible') and \
          jobs_dict[this_res['job_runID']]['preemptible'] >= 1))
          )
    def get_priority_tuple(x, jobs_dict=jobs_dict) :
        return (jobs_dict[x['job_runID']]['priority'],x)
    def get_reservation_from_tuple(x) :
        return x[1]
          
    resources_dict = resources_db_handle[0]
    reservations_dict = reservations_db_handle[0]
    job_steps_list = get_object_list(jobs_db_handle)
    reservations_list = get_object_list(reservations_db_handle)
    risk_reservations = filter(risky, reservations_list)
    priority_risk_reservations = map(get_priority_tuple, risk_reservations)
    priority_risk_reservations.sort()
    priority_risk_reservations.reverse()
    # take the risk reservations highest priority first
    risk_reservations = map(get_reservation_from_tuple, priority_risk_reservations)
    nonrisk_reservations = filter(approaching_nonrisky, reservations_list)
    retained_risk_reservations = []
    for risk_res in risk_reservations :
        #print "adding risk_res (%s) purpose (%s) runID (%s)" % (risk_res['name'], risk_res['purpose_type_string'], risk_res['job_runID'])
        atrisk_res_dict = {}
        # add up cpu and memory by node for all at_risk reservations
        for node in risk_res['node_list'] :
            if risk_res.has_key('allocated_dict_list') :
                for allocated_dict in risk_res['allocated_dict_list'] :
                    if allocated_dict.has_key('type') and allocated_dict['type'] == 'node_shared' :
                        if atrisk_res_dict.has_key(node) :
                            atrisk_res_dict[node]['cpu'] = atrisk_res_dict[node]['cpu'] + allocated_dict['cpu']
                            atrisk_res_dict[node]['memory'] = atrisk_res_dict[node]['memory'] + allocated_dict['memory']
                        else :
                            atrisk_res_dict[node] = { 'cpu' : allocated_dict['cpu'], 'memory' : allocated_dict['memory'] }
                    else :
                        if atrisk_res_dict.has_key(node) :
                            atrisk_res_dict[node]['cpu'] = atrisk_res_dict[node]['cpu'] + resources_dict[node]['ConsumableCpus']
                            atrisk_res_dict[node]['memory'] = atrisk_res_dict[node]['memory'] + resources_dict[node]['ConsumableMemory']
                        else :
                            atrisk_res_dict[node] = { 'cpu' : resources_dict[node]['ConsumableCpus'], 'memory' : resources_dict[node]['ConsumableMemory'] }
                    #print "atrisk_res_dict[node]['cpu'] (%s), atrisk_res_dict[node]['memory'] (%s)" % (atrisk_res_dict[node]['cpu'], atrisk_res_dict[node]['memory'])
        # add up cpu and memory by node for nonrisky reservations that
        # conflict
        nonrisk_res_dict = {}
        for nonrisk_res in nonrisk_reservations + retained_risk_reservations :
            #print "(%s) nonrisk_res_dict.keys() (%s)" % (nonrisk_res['name'],nonrisk_res_dict.keys(),)
            if nonrisk_res['start_time_float'] <= \
              Now_float + RUN_AT_RISK_CLEANUP_TIME and \
              ( nonrisk_res['start_time_float'] < risk_res['end_time_float'] <= nonrisk_res['end_time_float'] or \
              nonrisk_res['start_time_float'] <= risk_res['start_time_float'] < nonrisk_res['end_time_float'] ) :
                #print "adding resources for nonrisk_res (%s)" % nonrisk_res
                input_tuple = ( jobs_dict[risk_res['job_runID']], )
                result = apply_policy_code(
                  nonrisk_res['job_restriction'],
                  input_tuple)
                if result == 0 :
                    continue
                if nonrisk_res.has_key('allocated_dict_list') :
                    nonrisk_allocated_dict_list = nonrisk_res['allocated_dict_list']
                else :
                    nonrisk_allocated_dict_list = []
                    for nonrisknode in nonrisk_res['node_list'] :
                        nonrisk_allocated_dict_list.append(
                          { 'nodename' : nonrisknode,
                            'type' : 'node_exclusive',
                            'cpu' : resources_dict[nonrisknode]['ConsumableCpus'],
                            'memory' : resources_dict[nonrisknode]['ConsumableMemory']
                          })
                #for nonrisknode in nonrisk_res['node_list'] :
                for nonrisk_allocated_dict in nonrisk_allocated_dict_list :
                    nonrisknode = nonrisk_allocated_dict['nodename']
                    if atrisk_res_dict.has_key(nonrisknode) :
                        if nonrisk_res_dict.has_key(nonrisknode) :
                            nonrisk_res_dict[nonrisknode]['cpu'] = nonrisk_res_dict[nonrisknode]['cpu'] + nonrisk_allocated_dict['cpu']
                            nonrisk_res_dict[nonrisknode]['memory'] = nonrisk_res_dict[nonrisknode]['memory'] + nonrisk_allocated_dict['memory']
                        else :
                            nonrisk_res_dict[nonrisknode] = { 'cpu' : allocated_dict['cpu'], 'memory' : nonrisk_allocated_dict['memory'] }
                    else :
                        if nonrisk_res_dict.has_key(nonrisknode) :
                            nonrisk_res_dict[nonrisknode]['cpu'] = nonrisk_res_dict[nonrisknode]['cpu'] + resources_dict[nonrisknode]['ConsumableCpus']
                            nonrisk_res_dict[nonrisknode]['memory'] = nonrisk_res_dict[nonrisknode]['memory'] + resources_dict[nonrisknode]['ConsumableMemory']
                        else :
                            nonrisk_res_dict[nonrisknode] = { 'cpu' : resources_dict[nonrisknode]['ConsumableCpus'], 'memory' : resources_dict[nonrisknode]['ConsumableMemory'] }
                    #print "nonrisk_res_dict[nonrisknode]['cpu'] (%s), nonrisk_res_dict[nonrisknode]['memory'] (%s)" % (nonrisk_res_dict[nonrisknode]['cpu'], nonrisk_res_dict[nonrisknode]['memory'])
        found_conflict = 0
        #print "nonrisk_res_dict.keys() (%s)" % (nonrisk_res_dict.keys(),)
        for node in nonrisk_res_dict.keys() :
            #print "checking nonrisk node (%s)" % node
            if atrisk_res_dict.has_key(node) and resources_dict.has_key(node) :
                if nonrisk_res_dict[node]['cpu'] + atrisk_res_dict[node]['cpu'] > resources_dict[node]['ConsumableCpus'] :
                    #print "found cpu conflict %s %s %s" % (nonrisk_res_dict[node]['cpu'], atrisk_res_dict[node]['cpu'], resources_dict[node]['ConsumableCpus'])
                    found_conflict = 1
                if nonrisk_res_dict[node]['memory'] + atrisk_res_dict[node]['memory'] > resources_dict[node]['ConsumableMemory'] :
                    #print "found memory conflict %s %s %s" % (nonrisk_res_dict[node]['memory'], atrisk_res_dict[node]['memory'], resources_dict[node]['ConsumableMemory'])
                    found_conflict = 1
            if found_conflict == 0 :
                #print "not deleting (%s) (%s) (%s)" % (risk_res['name'], risk_res['purpose_type_string'], risk_res['job_runID'])
                retained_risk_reservations.append(risk_res)
            else :
                if DEBUGJOB != None and risk_res['job_runID'] != None and risk_res['job_runID'] == DEBUGJOB :
                    print "deleting (%s) (%s) (%s)" % (risk_res['name'], risk_res['purpose_type_string'], risk_res['job_runID'])
                # for preempted_job reservations, need to push these back,
                # instead of deleting.  maybe pushback is getting deleted?
                #print "deleting (%s) (%s) (%s)" % (risk_res['name'], risk_res['purpose_type_string'], risk_res['job_runID'])
                if reservations_db_handle[0].has_key(risk_res['name']) :
                    #print "should delete (%s)" % risk_res['name']
                    delete_object(risk_res['name'], reservations_db_handle)
                if risk_res['purpose_type_string'] == 'running' :
                    if SERVERMODE == 'NORMAL' :
                        try :
                            if jobs_dict[risk_res['job_runID']]['preemptible'] == 0 :
                                #print "not doing preempt_job (%s)" % risk_res['job_runID']
                                cancel_job(jobs_dict[risk_res['job_runID']], events_db_handle)
                            else :
                                #print "preempt_job (%s)" % risk_res['job_runID']
                                preempt_job(jobs_dict[risk_res['job_runID']], events_db_handle)
                        except 'CancelJobFailure', failed_job_step :
                            print "cancel of %s failed" % \
                              failed_job_step['name']
                        except :
                            continue
                    else :
                        if jobs_dict[risk_res['job_runID']]['preemptible'] == 0 :
                            print "%s should be canceled!" % jobs_dict[risk_res['job_runID']]['name']
                        else :
                            print "%s should be preempted!" % jobs_dict[risk_res['job_runID']]['name']

def cancel_overrun_jobs(events_db_handle, jobs_db_handle) :
    job_steps_list = get_object_list(jobs_db_handle)
    for job_step in job_steps_list :
        Catalina_job_step_state = job_step['state']
        if Catalina_job_step_state == 'Starting' or \
          Catalina_job_step_state == 'Running' :
            # Check to see if job has exceeded its duration + overrun
            duration = job_step['wall_clock_limit']
            # If job has 'wall_clock_used', use that, otherwise
            # calculate it from Now_float - Dispatch_Time
            if job_step.has_key('wall_clock_used') and job_step['wall_clock_used'] != None :
                wall_clock_used = job_step['wall_clock_used']
            else :
                start_time = job_step['Dispatch_Time']
                wall_clock_used = Now_float - start_time
            if wall_clock_used > duration + MAXJOBOVERRUN :
                # Cancel the job
                if SERVERMODE == 'NORMAL' :
                    try :
                    # Martin W. Margo 11/30/2005 10:58 AM
                    # I will give cancel_job more information, namely the reason
                    # why I want to cancel the job.
                        cancel_job(job_step, events_db_handle, \
                            "Reason: Job killed because it exceeds wallclock time." )
                    except 'CancelJobFailure', failed_job_step :
                        print "cancel of %s failed" % \
                          failed_job_step['name']
                    except :
                        continue
                else :
                    print "%s should be canceled!" % job_step['name']

def reload_job_resource_lists(events_db_handle, jobs_db_handle, resources_db_handle) :
    """ reload_job_resource_lists()
    Refreshes every job resource list. resource list contains nodes/hosts that are
    eligible to run this job. In most cases, they stay still but sometime a node
    went out fo the queue or join the queue"""
    jobs_dict = jobs_db_handle[0]
    job_step_list = jobs_dict.values()
    for job_step in job_step_list :
        job_step_resource_dict_list = get_resource_dict_list(job_step, resources_db_handle)
        update_object_attribute('resource_dict_list', job_step_resource_dict_list, job_step, jobs_db_handle)

def update_job_resource_lists(jobs_db_handle, resources_db_handle) :
    """ for speed, this only gets the resource list for new jobs...
     there is the potential for incorrect scheduling when a node
     comes up.  reload_job_resource_lists should be run periodically..."""
    jobs_dict = jobs_db_handle[0]
    for key in jobs_dict.keys() :
        #print "update_job_resource_lists for (%s)" % key
        #Martin Margo was here 11/6/2006
        #this code can speeds up catalina tremendously
        #only update object attribute those jobs who doesn't have a resource_dict_list
        #alraeady
        
        
        #condition 1
        if jobs_dict[key].has_key('resource_dict_list') and len(jobs_dict[key]['resource_dict_list']) > 0 :
            #print "has resource_dict_list (%s), so continuing" % (jobs_dict[key]['resource_dict_list'],)
            continue
        
        #condition 2
        # For some speed, skip any jobs that were previously found to
        # have 0 resource list.  These should get checked again in
        # reload_job_resource_lists
        if jobs_dict[key]['ineligible_reason'] == 'BADRESOURCELIST' :
            #print "has ineligible_reason BADRESOURCE_LIST, so continuing"
            continue
        
        # if this job is not discovered before and has good resource list
        #print "updating job_resource_lists for (%s)" % key
        job_step_resource_dict_list = get_resource_dict_list(jobs_dict[key], resources_db_handle)
        update_object_attribute('resource_dict_list', job_step_resource_dict_list, jobs_dict[key], jobs_db_handle)

def update_job_speculative_system_queue_time(jobs_db_handle) :
    job_step_list = get_object_list(jobs_db_handle)
    for job_step in job_step_list :
        if job_step['system_queue_time'] == None :
            update_object_attribute('speculative_system_queue_time', Now_float, job_step, jobs_db_handle)
        else :
            update_object_attribute('speculative_system_queue_time', job_step['system_queue_time'], job_step, jobs_db_handle)

def sort_by_key (list, key) :
    def get_keytuple (x, key=key) :
        if x.has_key(key) :
            return (x[key], x)
        else :
            return (0, x)
    keylist = map(get_keytuple, list)
    keylist.sort()
    keylist.reverse()
    return map(lambda (value, x): x, keylist)

def get_eligible_and_running_jobs(jobs_db_handle, resources_db_handle, reservations_db_handle) :
    def get_filtered_jobs(input_tuple) :
        temp_jobs_list = input_tuple[0]
        temp_resources_list = input_tuple[1]
        temp_reservations_list = input_tuple[2]
        message_string = ''
        running_jobs_per_user = {}
        queued_jobs_per_user = {}
        running_jobs_per_account_per_qos = {}
        queued_jobs_per_account_per_qos = {}
        running_node_sec_per_user_per_qos = {}
        queued_node_sec_per_user_per_qos = {}
        running_node_sec_per_account_per_qos = {}
        queued_node_sec_per_account_per_qos = {}
        idle_jobs = []
        runningstarting_jobs = []
        other_jobs = []
        eligible_jobs = []
        ineligible_jobs = []

        for job in temp_jobs_list :
            state = job['state']
            user = job['user']
            account = job['account']
            qos = job['QOS']
            if state == 'Idle' :
                idle_jobs.append(job)
            elif state == 'Running' or state == 'Starting' or state == 'Preempted' :
                #if state == 'Preempted' :
                #    idle_jobs.append(job)
                runningstarting_jobs.append(job)
                if running_jobs_per_user.has_key(user) :
                    running_jobs_per_user[user] = \
                      running_jobs_per_user[user] + 1
                else :
                    running_jobs_per_user[user] = 1
                if running_jobs_per_account_per_qos.has_key(account) :
                    if running_jobs_per_account_per_qos[account].has_key(qos) :
                        running_jobs_per_account_per_qos[account][qos] = \
                          running_jobs_per_account_per_qos[account][qos] + 1
                    else :
                        running_jobs_per_account_per_qos[account][qos] = 1
                else :
                    running_jobs_per_account_per_qos[account] = { qos : 1, }
                if running_node_sec_per_account_per_qos.has_key(account) :
                    if running_node_sec_per_account_per_qos[account].has_key(qos) :
                        running_node_sec_per_account_per_qos[account][qos] = \
                          running_node_sec_per_account_per_qos[account][qos] + job['resource_amount_int'] * job['wall_clock_limit']
                    else :
                        running_node_sec_per_account_per_qos[account][qos] = job['resource_amount_int'] * job['wall_clock_limit']
                else :
                    running_node_sec_per_account_per_qos[account] = { qos : job['resource_amount_int'] * job['wall_clock_limit'], }
            else :
                 other_jobs.append(job)

        if FIFOSCREEN == 'ON' :
            idle_jobs = sort_by_key(idle_jobs, 'SubmitTime')
            idle_jobs.reverse()
        else :
            idle_jobs = sort_by_key(idle_jobs, 'priority')
        for job in idle_jobs :
            user = job['user']
            account = job['account']
            qos = job['QOS']
            if BADRESOURCELIST == 'ON' :
                if not job.has_key('resource_dict_list') :
                    if DEBUGJOB != None and job['job_runID'] != None and job['job_runID'] == DEBUGJOB :
                        print "no resource_dict_list"
                    job['ineligible_reason'] = 'BADRESOURCELIST'
                    ineligible_jobs.append(job)
                    continue
                positive_resource_list = 0
                for resource_dict in job['resource_dict_list'] :
                    if len(resource_dict['resource_dict'].keys()) > 0 :
                        positive_resource_list = 1
                        break
                if positive_resource_list == 0 :
                    if DEBUGJOB != None and job['name'] != None and job['name'] == DEBUGJOB :
                        print "positive_resource_list == 0"
                    job['ineligible_reason'] = 'BADRESOURCELIST'
                    ineligible_jobs.append(job)
                    continue
                #if len(job['requested_resource_list']) > job['resource_dict_list'][0]['amount_int'] :
                #if len(job['resource_dict_list'][0]['resource_dict'].keys()) < job['resource_dict_list'][0]['amount_int'] :
                #    if DEBUGJOB != None and job['name'] != None and job['name'] == DEBUGJOB :
                #        print "too few keys in resource_dict_list"
                #        print "job['resource_dict_list'][0]['resource_dict'] (%s) job['resource_dict_list'][0]['amount_int'] (%s)" % (job['resource_dict_list'][0]['resource_dict'],job['resource_dict_list'][0]['amount_int'])
                #    job['ineligible_reason'] = 'BADRESOURCELIST'
                #    ineligible_jobs.append(job)
                #    continue
            if MAXJOBPERUSERPOLICY != None and MAXJOBPERUSERPOLICY == 'ON' :
                if running_jobs_per_user.has_key(user) :
                    if running_jobs_per_user[user] >= \
                      MAXJOBPERUSERCOUNT :
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'MAXJOBPERUSERPOLICY'
                        ineligible_jobs.append(job)
                        continue
            if QOS_MAXJOBPERUSERPOLICY_dict[job['QOS']] != None :
                if running_jobs_per_user.has_key(user) :
                    if running_jobs_per_user[user] >= \
                      QOS_MAXJOBPERUSERPOLICY_dict[job['QOS']] :
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'QOSMAXJOBPERUSERPOLICY'
                        ineligible_jobs.append(job)
                        continue
            if QOS_MAXJOBPERACCOUNTPOLICY_dict.has_key(job['QOS']) and QOS_MAXJOBPERACCOUNTPOLICY_dict[job['QOS']] != None :
                if running_jobs_per_account_per_qos.has_key(account) :
                    if running_jobs_per_account_per_qos[account].has_key(job['QOS']) :
                        if running_jobs_per_account_per_qos[account][qos] >= \
                          QOS_MAXJOBPERACCOUNTPOLICY_dict[job['QOS']] :
                            job['system_queue_time'] = None
                            job['ineligible_reason'] = 'MAXJOBPERACCOUNTPOLICY'
                            ineligible_jobs.append(job)
                            continue
            if MAXJOBQUEUEDPERUSERPOLICY != None and MAXJOBQUEUEDPERUSERPOLICY == 'ON' :
                if queued_jobs_per_user.has_key(user) :
                    if queued_jobs_per_user[user] >= \
                      MAXJOBQUEUEDPERUSERCOUNT :
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'MAXJOBQUEUEDPERUSERPOLICY'
                        ineligible_jobs.append(job)
                        continue
            if QOS_MAXJOBQUEUEDPERUSERPOLICY_dict[job['QOS']] != None :
                if queued_jobs_per_user.has_key(user) :
                    if queued_jobs_per_user[user] >= \
                      QOS_MAXJOBQUEUEDPERUSERPOLICY_dict[job['QOS']] :
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'QOSMAXJOBQUEUEDPERUSERPOLICY'
                        ineligible_jobs.append(job)
                        continue
            if QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict.has_key(job['QOS']) and QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict[job['QOS']] != None :
                if queued_jobs_per_account_per_qos.has_key(account) :
                    if queued_jobs_per_account_per_qos[account].has_key(job['QOS']) :
                        if queued_jobs_per_account_per_qos[account].has_key(qos) :
                            if queued_jobs_per_account_per_qos[account][qos] >= \
                              QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict[job['QOS']] :
                                job['system_queue_time'] = None
                                job['ineligible_reason'] = 'QOSMAXJOBQUEUEDPERACCOUNTPOLICY'
                                ineligible_jobs.append(job)
                                continue
            if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict') and \
              QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict.has_key(job['QOS']) and \
              QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job['QOS']] != None :
                if running_node_sec_per_user_per_qos.has_key(user) :
                    if running_node_sec_per_user_per_qos[user].has_key(job['QOS']) :
                        if running_node_sec_per_user_per_qos[user][qos] >= \
                          QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job['QOS']] :
                            job['system_queue_time'] = None
                            job['ineligible_reason'] = 'QOSMAXNODESECRUNNINGPERUSERPOLICY'
                            ineligible_jobs.append(job)
                            continue
            if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict') and \
              QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict.has_key(job['QOS']) and \
              QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict[job['QOS']] != None :
                if queued_node_sec_per_user_per_qos.has_key(user) :
                    if queued_node_sec_per_user_per_qos[user].has_key(job['QOS']) :
                        if queued_node_sec_per_user_per_qos[user].has_key(qos) :
                            if queued_node_sec_per_user_per_qos[user][qos] >= \
                              QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict[job['QOS']] :
                                job['system_queue_time'] = None
                                job['ineligible_reason'] = 'QOSMAXNODESECQUEUEDPERUSERPOLICY'
                                ineligible_jobs.append(job)
                                continue
            if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict') and \
              QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict.has_key(job['QOS']) and \
              QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job['QOS']] != None :
                if running_node_sec_per_account_per_qos.has_key(account) :
                    if running_node_sec_per_account_per_qos[account].has_key(job['QOS']) :
                        if running_node_sec_per_account_per_qos[account][qos] >= \
                          QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job['QOS']] :
                            job['system_queue_time'] = None
                            job['ineligible_reason'] = 'QOSMAXNODESECRUNNINGPERACCOUNTPOLICY'
                            ineligible_jobs.append(job)
                            continue
            if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict') and \
              QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict.has_key(job['QOS']) and \
              QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict[job['QOS']] != None :
                if queued_node_sec_per_account_per_qos.has_key(account) :
                    if queued_node_sec_per_account_per_qos[account].has_key(job['QOS']) :
                        if queued_node_sec_per_account_per_qos[account].has_key(qos) :
                            if queued_node_sec_per_account_per_qos[account][qos] >= \
                              QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict[job['QOS']] :
                                job['system_queue_time'] = None
                                job['ineligible_reason'] = 'QOSMAXNODESECQUEUEDPERACCOUNTPOLICY'
                                ineligible_jobs.append(job)
                                continue
            job['ineligible_reason'] = 'Eligible'
            job['system_queue_time'] = job['speculative_system_queue_time']
            eligible_jobs.append(job)
            if queued_jobs_per_user.has_key(user) :
                queued_jobs_per_user[user] = queued_jobs_per_user[user] + 1
            else :
                queued_jobs_per_user[user] = 1
            if queued_jobs_per_account_per_qos.has_key(account) :
                if queued_jobs_per_account_per_qos[account].has_key(job['QOS']) :
                    queued_jobs_per_account_per_qos[account][qos] = \
                      queued_jobs_per_account_per_qos[account][qos] + 1
                else :
                    queued_jobs_per_account_per_qos[account][qos] = 1
            else :
                queued_jobs_per_account_per_qos[account] = { qos : 1, }
            if queued_node_sec_per_account_per_qos.has_key(account) :
                if queued_node_sec_per_account_per_qos[account].has_key(job['QOS']) :
                    queued_node_sec_per_account_per_qos[account][qos] = \
                      queued_node_sec_per_account_per_qos[account][qos] + job['resource_amount_int'] * job['wall_clock_limit']
                else :
                    queued_node_sec_per_account_per_qos[account][qos] = job['resource_amount_int'] * job['wall_clock_limit']
            else :
                queued_node_sec_per_account_per_qos[account] = { qos : job['resource_amount_int'] * job['wall_clock_limit'], }
        result = (eligible_jobs, ineligible_jobs, runningstarting_jobs, message_string)
        return result

    temp_jobs_list = get_object_list(jobs_db_handle)
    temp_resources_list = get_object_list(resources_db_handle)
    temp_reservations_list = get_object_list(reservations_db_handle)
    input_tuple = ( temp_jobs_list, temp_resources_list, temp_reservations_list )
    (eligible_jobs, ineligible_jobs, runningstarting_jobs, message_string) = \
      get_filtered_jobs(input_tuple)
    if FIFOSCREEN == 'ON' :
        eligible_jobs = sort_by_key(eligible_jobs, 'priority')
    for job in eligible_jobs :
        insert_new_object(job, jobs_db_handle)
    for job in ineligible_jobs :
        insert_new_object(job, jobs_db_handle)
    return (eligible_jobs, runningstarting_jobs)

def spec_string_to_start_dict(object) :
    # takes an object with a start_spec_string
    # returns a dictionary with times
    time_dict = {
      'minutes' : None,
      'hours' :   None,
      'mdays' :   None,
      'months'  : None,
      'wdays' :   None
    }
    minute, hour, mday, mon, wday = string.split(
      object['start_spec_string']
    )
    # convert each of these into a list of ints
    if minute == '*' :
        min_list = range(60)
    else :
        string_min_list = string.split(minute,',')
        min_list = map(string.atoi, string_min_list)
    if hour == '*' :
        hour_list = range(24)
    else :
        string_hour_list = string.split(hour,',')
        hour_list = map(string.atoi, string_hour_list)
    if mday == '*' :
        mday_list = range(1,32)
    else :
        string_mday_list = string.split(mday,',')
        mday_list = map(string.atoi, string_mday_list)
    if mon == '*' :
        mon_list = range(1,13)
    else :
        string_mon_list = string.split(mon,',')
        mon_list = map(string.atoi, string_mon_list)
    if wday == '*' :
        wday_list = range(7)
    else :
        string_wday_list = string.split(wday,',')
        wday_list = map(string.atoi, string_wday_list)
    time_dict['minutes'] = min_list
    time_dict['hours'] = hour_list
    time_dict['mdays'] = mday_list
    time_dict['months'] = mon_list
    time_dict['wdays'] = wday_list
    return time_dict

def get_new_candidate_time(start_time_dict, candidate_epoch, standing_TZ=None) :
    # gets start_time_dict, and current_candidate_epoch
    # returns new_candidate_epoch, new_candidate_tuple
    if os.environ.has_key('TZ') :
        old_TZ = os.environ['TZ']
    else :
        old_TZ = None
    if standing_TZ == None :
        if FORCETZ == 'NOFORCE' :
            os.environ['TZ'] = 'GMT0'
        else :
            os.environ['TZ'] = FORCETZ
    else :
        os.environ['TZ'] = standing_TZ
    mon_list = start_time_dict['months']
    mday_list = start_time_dict['mdays']
    hour_list = start_time_dict['hours']
    min_list = start_time_dict['minutes']
    wday_list = start_time_dict['wdays']
    found = 0
    #new_candidate_tuple = time.localtime(candidate_epoch)
    while found == 0 :
        candidate_tuple = time.localtime(candidate_epoch)
        #new_candidate_tuple = time.localtime(candidate_epoch)
        candidate_list = []
        for item in candidate_tuple :
            candidate_list.append(item)
        candidate_mon = candidate_list[1]
        candidate_mday = candidate_list[2]
        candidate_hour = candidate_list[3]
        candidate_min = candidate_list[4]
        candidate_wday = candidate_list[6]
        #print "Checking month"
        if not candidate_mon in mon_list :
            # go to the start of the next month
            # increment candidate_epoch by 24 * 3600
            new_candidate_mon = candidate_mon
            while new_candidate_mon == candidate_mon :
                candidate_epoch = candidate_epoch + 24 * 3600
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_mon = new_candidate_tuple[1]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[2] = 1
            new_candidate_list[3] = 0
            new_candidate_list[4] = 0
            new_candidate_list[5] = 0
            new_candidate_list[6] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        #print "Checking wday"
        # In Python, Monday is 0, so add 1 to conform to cron
        if not (candidate_wday + 1) % 7 in wday_list :
            # go to next wday
            # increment candidate_epoch by 24 * 3600
            new_candidate_wday = candidate_wday
            while new_candidate_wday == candidate_wday :
                candidate_epoch = candidate_epoch + 24 * 3600
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_wday = new_candidate_tuple[6]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[3] = 0
            new_candidate_list[4] = 0
            new_candidate_list[5] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        #print "Checking mday"
        if not candidate_mday in mday_list :
            # go to next mday
            # increment candidate_epoch by 24 * 3600
            new_candidate_mday = candidate_mday
            while new_candidate_mday == candidate_mday :
                candidate_epoch = candidate_epoch + 24 * 3600
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_mday = new_candidate_tuple[2]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[3] = 0
            new_candidate_list[4] = 0
            new_candidate_list[5] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        #print "Checking hour"
        if not candidate_hour in hour_list :
            #print "going to next hour"
            #print "hour_list is (%s)" % hour_list
            #print "candidate_hour is (%s)" % candidate_hour
            # go to next hour
            # increment candidate_epoch by 3600
            new_candidate_hour = candidate_hour
            while new_candidate_hour == candidate_hour :
                candidate_epoch = candidate_epoch + 3600
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_hour = new_candidate_tuple[3]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[4] = 0
            new_candidate_list[5] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        #print "Checking min"
        if not candidate_min in min_list :
            #print "going to next min"
            #print "min_list is (%s)" % min_list
            #print "candidate_min is (%s)" % candidate_min
            # go to next min
            # increment candidate_epoch by 60
            new_candidate_min = candidate_min
            while new_candidate_min == candidate_min :
                candidate_epoch = candidate_epoch + 60
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_min = new_candidate_tuple[4]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[5] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        # Found start time
        found = 1
        #print "Start time found, returning"
    # If reached here without continuing, then month, day,
    # hour, minute, weekday match, so return this time
    if old_TZ != None :
        os.environ['TZ'] = old_TZ
    return (candidate_epoch, candidate_tuple)

def migrate_shortpools(jobs_db_handle, resources_db_handle, reservations_db_handle) :
    # Look for standing reservations with a 'latency_float' attribute
    # for each standing reservation with a latency_float attribute,
    # create a lookahead reservation with earliest start =
    # Now_float + latency_float, duration = duration - ( Now_float - start ),
    # mode = lookahead, all other attributes same as standing res instance.
    # update standing res instance to have the node_list of the lookahead
    # reservation.
    # This should result in shortpool behaviour, with reservations migrating
    # to short-running nodes, if possible.
    existing_reservations = get_object_list(reservations_db_handle)
    job_reservations = filter( lambda reservation : \
      reservation['purpose_type_string'] == 'job' and \
      (reservation['start_count_int'] == None or \
      reservation['start_count_int'] == 0) , \
      existing_reservations )
    ignorable_job_reservations = []
    for job_reservation in job_reservations :
        ignorable_job_reservations.append(job_reservation['job_runID'])
    shortpool_reservations = filter( lambda reservation : \
      reservation['purpose_type_string'] == 'standing_reservation'\
      and reservation['latency_float'] != None , \
      existing_reservations )
    for reservation in shortpool_reservations :
        if reservation['start_time_float'] <= Now_float < reservation['end_time_float'] :
            earliest_start_float = Now_float + reservation['latency_float']
            duration_float = reservation['duration_float'] - \
              ( Now_float - reservation['start_time_float'] )
            latest_end_float = earliest_start_float + duration_float
            ignore_reservations_list=reservation['ignore_reservations_list']
            node_usage=reservation['node_usage']
            requested_resource_list=reservation['requested_resource_list']
            if ignore_reservations_list == None :
                ignore_reservations_list = [ reservation['name'] ]
            else :
                ignore_reservations_list.append( reservation['name'] )
            ignore_reservations_list = ignore_reservations_list + ignorable_job_reservations
            try :
                new_reservation = create_reservation(
                  resources_db_handle=resources_db_handle,
                  reservations_db_handle=reservations_db_handle,
                  jobs_db_handle=jobs_db_handle,
                  earliest_start_float=earliest_start_float,
                  latest_end_float=latest_end_float,
                  duration_float=duration_float,
                  resource_amount_int=reservation['resource_amount_int'],
                  node_usage=reservation['node_usage'],
                  requested_resource_list=reservation['requested_resource_list'],
                  job_restriction=reservation['job_restriction'],
                  node_restriction=reservation['node_restriction'],
                  node_sort_policy=reservation['node_sort_policy'],
                  conflict_policy=reservation['conflict_policy'],
                  purpose_type_string=reservation['purpose_type_string'],
                  ignore_reservations_list=ignore_reservations_list,
                  comment_string=reservation['comment_string'],
                  mode='lookahead'
                )
            except 'InsufficientNodes', new_res :
                continue
            else :
                if new_reservation['node_list'] != None :
                    if DEBUGJOB != None :
                        print "for reservation (%s) setting node_list to (%s)" % (reservation['name'],new_reservation['node_list'])
                    update_object_attribute('node_list',
                      new_reservation['node_list'],
                      reservation,
                      reservations_db_handle
                    )
                    # for shared nodes, need to update allocated_dict_list
                    update_object_attribute('allocated_dict_list',
                      new_reservation['allocated_dict_list'],
                      reservation,
                      reservations_db_handle
                    )

def update_standing_reservations(
  events_db_handle,
  jobs_db_handle,
  resources_db_handle,
  reservations_db_handle,
  standing_reservations_db_handle) :
    # conflict policy should allow conflicts with running jobs, choosing
    # nodes that are firstavailable or idle.  conflict policy should not
    # allow overlap with user reservations.
    # Wipe out the old standing reservations
    # filtering out job reservations this way allows overlap of nonjob
    # reservations.  The reservation ids for jobs are put into the 
    # ignore_reservations_list.  The first create_reservation call
    # for a standing_reservation wipes out all the job reservations.
    # Standing reservations can subsequently be given the old job reservation
    # ids.  The ignore_reservations_list still contains these old job
    # reservation ids, so these get ignored in create_reservation, causing
    # the new reservation to overlap the older ones...
    # Should either wipe out the old job reservations, or remake ignore
    # reservations list each time... Should be okay to pass in a none
    # ignore_reservations_list.  This will cause the job reservations
    # to get wiped out in create_reservation.  We want to get blocked
    # by the running reservations anyway...
    # Actually, we need to ignore reservations for running jobs
    # that pass job restriction for each running reservation.
    # Otherwise, the standing res instance can't get remade over
    # running jobs it allowed to run.
    jobs_dict = jobs_db_handle[0]
    reservations_dict = reservations_db_handle[0]
    standing_reservations_dict = standing_reservations_db_handle[0]
    running_reservations_list = []
    for reservation in reservations_dict.values() :
        if reservation['purpose_type_string'] == 'running' or \
          (reservation['purpose_type_string'] == 'job' and \
          reservation.has_key('job_runID') and \
          reservation['job_runID'] != None and \
          jobs_dict.has_key(reservation['job_runID']) and \
          jobs_dict[reservation['job_runID']].has_key('start_count_int') and \
          jobs_dict[reservation['job_runID']]['start_count_int'] != None and \
          jobs_dict[reservation['job_runID']]['start_count_int'] >= 1) :
            running_reservations_list.append(reservation)
    standing_reservations_list = get_object_list(
      standing_reservations_db_handle)
    for standing_reservation in standing_reservations_list :
        # Starting from Now_float - duration_float,
        # find each of the next instances
        # Convert the potential starting time to a time tuple
        # Check to see if the values of the tuple match the lists
        # from the start_spec_string.  If all values match, then
        # attempt to create a standing reservation instance at that
        # start time.  If any of the tuple elements fails, increment
        # the potential starting time by the number of seconds appropriate
        # for that time tuple, unless month fails, in which case increment
        # by one day's worth of seconds.  This is to catch the first day
        # of each month without logic to determine days in a month.
        # Set all other tuple elements to their first possible value.
        # iterate until depth number of instances have been created.
        # Convert the start_spec_string into separate fields
        # move to function spec_string_to_start_dict
	# Check to see if there are enough accepted nodes to
        # ever fulfull this reservation
        for reservation in reservations_dict.values() :
            if reservation['purpose_type_string'] == \
              'standing_reservation' and \
              reservation['creator_string'] == standing_reservation['name']:
                try :
                    delete_object(reservation['name'],
                      reservations_db_handle)
                except 'NameNotInDB', name :
                    print "Standing Reservation to be deleted (%s) not found" % \
                      name
        accepted_nodes_list = get_accepted_nodes_list(standing_reservation['node_restriction'], resources_db_handle)
        if len(accepted_nodes_list) < standing_reservation['resource_amount_int'] :
            print "InsufficientAcceptedNodes"
            continue
        start_time_dict = spec_string_to_start_dict(standing_reservation)
        if standing_reservation.has_key('TZ_string') :
            standing_TZ = standing_reservation['TZ_string']
        else :
            standing_TZ = None
        number_found = 0
        depth = standing_reservation['depth']
        resource_amount_int = standing_reservation['resource_amount_int']
        #print "resource_amount_int (%s)" % resource_amount_int
        if standing_reservation.has_key('requested_resource_list') :
            requested_resource_list = standing_reservation['requested_resource_list']
        else :
            requested_resource_list = []
            req_dict_dict = {'type' : 'node_exclusive',
                             'req_list' : []
                            }
            for index in range(resource_amount_int) :
                requested_resource_list.append(req_dict_dict)
        if standing_reservation.has_key('node_usage') and standing_reservation['node_usage'] != None :
            node_usage = standing_reservation['node_usage']
        else :
            node_usage = 'node_exclusive'
        #print "node_usage (%s)" % node_usage
        if standing_reservation.has_key('requested_resource_list') :
            requested_resource_list = standing_reservation['requested_resource_list']
        else :
            requested_resource_list = []
        #print "requested_resource_list (%s)" % (requested_resource_list,)
        latency_float = standing_reservation['latency_float']
        job_restriction = standing_reservation['job_restriction']
        node_restriction = standing_reservation['node_restriction']
        conflict_policy = standing_reservation['conflict_policy']
        duration_float = standing_reservation['duration_float']
        if standing_reservation['sort_policy'] == None :
            #sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', '[]', LAST_AVAILABLE_IGNORED_FIRST)
            sort_policy = LAST_AVAILABLE_IGNORED_FIRST
        else :
            sort_policy = standing_reservation['sort_policy']
        affinity_calculation = standing_reservation['affinity_calculation']
        purpose_type_string = 'standing_reservation'
        creator_string = standing_reservation['name']
        if standing_reservation.has_key('creator_string') and type(standing_reservation['creator_string']) is str :
            comment_string = 'standing reservation: ' + standing_reservation['name'] + ' ;' + standing_reservation['comment_string'] + ' ;' + standing_reservation['creator_string'] + ' ;'
        else :
            comment_string = 'standing reservation: ' + standing_reservation['name'] + ' ;' + standing_reservation['comment_string'] + ' ;'
        candidate_epoch = Now_float - duration_float
        ignore_reservations_list = []
        if standing_reservation.has_key('overlap_running_int') and \
          standing_reservation['overlap_running_int'] != None and \
          standing_reservation['overlap_running_int'] >= 1 :
            #print "overlap_running_int found for (%s)" % standing_reservation['name']
            for reservation in running_reservations_list :
                if not jobs_dict.has_key(reservation['job_runID']) :
                    continue
                input_tuple = ( jobs_dict[reservation['job_runID']], )
                result = apply_policy_code(job_restriction, input_tuple)
                if result == 0 :
                    ignore_reservations_list.append(reservation['name'])
        #original_nodes = []
        #found_original_nodes = 0
        low_duration = 0
        while number_found < depth and low_duration == 0 :
            duration_float = standing_reservation['duration_float']
            ( candidate_epoch, new_candidate_tuple ) = \
              get_new_candidate_time( start_time_dict, candidate_epoch, standing_TZ=standing_TZ )

            earliest_start_float = candidate_epoch
            latest_end_float = candidate_epoch + duration_float
            #print "standing_reservation (%s)" % standing_reservation['name']
            #print "earliest_start_float (%s)" % time.asctime(time.localtime(earliest_start_float))
            if earliest_start_float >= END_OF_SCHEDULING :
                #print "end of scheduling"
                break
            try :
                new_res = create_reservation(
                  resources_db_handle=resources_db_handle,
                  reservations_db_handle=reservations_db_handle,
                  jobs_db_handle=jobs_db_handle,
                  earliest_start_float=earliest_start_float,
                  latest_end_float=latest_end_float,
                  duration_float=duration_float,
                  latency_float=latency_float,
                  resource_amount_int=resource_amount_int,
                  node_usage=node_usage,
                  requested_resource_list=requested_resource_list,
                  job_restriction=job_restriction,
                  node_restriction=node_restriction,
                  node_sort_policy=sort_policy,
                  conflict_policy=conflict_policy,
                  affinity_calculation=affinity_calculation,
                  purpose_type_string=purpose_type_string,
                  creator_string=creator_string,
                  comment_string=comment_string,
                  ignore_reservations_list=ignore_reservations_list,
                  accepted_nodes_list=accepted_nodes_list,
                  mode='real'
                )
            except 'InsufficientNodes', failed_res :
                #print "full size, full duration failed"
                # grab as many nodes as possible (since it failed
                # due to insufficient nodes, we should not get more
                # than we asked for before...
                # At this point, it would also be possible to
                # create small sub reservations between earliest_start
                # and latest_end, first finding all sub reservations
                # with the full resource amount.  This would entail
                # reducing duration by small chunks (15 min.) and
                # doing trial reservations, keeping track of end_times
                # after finding all full resource amount sub reservations
                # could find all smaller than full amount sub reservations
                # by decrementing resource amount iteratively...
                # For now, just bail.
                partial_found = 0
                minimal_resource_amount_int = None
                #if earliest_start_float < Now_float :
                #    max_duration_float = min(duration_float,standing_reservation['duration_float'] - (Now_float - earliest_start_float))
                #else :
                #    max_duration_float = duration_float
                #latest_end_float = candidate_epoch + max_duration_float
                try :
                    #print "trying a partial..."
                    new_res = create_reservation(
                      resources_db_handle=resources_db_handle,
                      reservations_db_handle=reservations_db_handle,
                      jobs_db_handle=jobs_db_handle,
                      earliest_start_float=earliest_start_float,
                      latest_end_float=latest_end_float,
                      duration_float=duration_float,
                      latency_float=latency_float,
                      resource_amount_int=minimal_resource_amount_int,
                      node_usage=node_usage,
                      requested_resource_list=requested_resource_list,
                      job_restriction=job_restriction,
                      node_restriction=node_restriction,
                      node_sort_policy=sort_policy,
                      conflict_policy=conflict_policy,
                      affinity_calculation=affinity_calculation,
                      purpose_type_string=purpose_type_string,
                      creator_string=creator_string,
                      comment_string=comment_string,
                      accepted_nodes_list=accepted_nodes_list,
                      ignore_reservations_list=ignore_reservations_list,
                      mode='real'
                    )
                except 'InsufficientNodes' :
                    if DEBUG == 'usr' :
                        print "No resource_amount Standing reservation instance failed"
                    print "No resource_amount Standing reservation instance failed"
                else :
                    print "No resource_amount Standing reservation instance succeeded"
                    partial_found = new_res['resource_amount_int']
                    #if found_original_nodes == 0 :
                    #    found_original_nodes = 1
                    #    original_nodes = new_res['node_list']
                    #    sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', `original_nodes`, LAST_AVAILABLE_IGNORED_FIRST)
                # Find the next full instance
                full_found = 0
                if earliest_start_float < Now_float :
                    full_earliest_start_float = Now_float
                    #max_duration_float = min(duration_float,standing_reservation['duration_float'] - (Now_float - earliest_start_float))
                else :
                    full_earliest_start_float = earliest_start_float
                    #max_duration_float = duration_float
                last_change = - duration_float/2
                duration = duration_float
                #while full_found == 0 and duration_float > FUDGE_FACTOR :
                while full_found == 0 or abs(last_change) > FUDGE_FACTOR :
                    # should do a binary search, with FUDGE_FACTOR resolution
                    full_amount = resource_amount_int - partial_found
                    duration = max(1,duration + last_change)
                    try :
                        print "trying partial with duration (%s) last_change (%s)" % (duration,last_change)
                        new_res = create_reservation(
                          resources_db_handle=resources_db_handle,
                          reservations_db_handle=reservations_db_handle,
                          jobs_db_handle=jobs_db_handle,
                          earliest_start_float=full_earliest_start_float,
                          latest_end_float=latest_end_float,
                          duration_float=duration_float,
                          latency_float=latency_float,
                          resource_amount_int=full_amount,
                          node_usage=node_usage,
                          requested_resource_list=requested_resource_list,
                          job_restriction=job_restriction,
                          node_restriction=node_restriction,
                          node_sort_policy=sort_policy,
                          conflict_policy=conflict_policy,
                          affinity_calculation=affinity_calculation,
                          purpose_type_string=purpose_type_string,
                          creator_string=creator_string,
                          comment_string=comment_string,
                          accepted_nodes_list=accepted_nodes_list,
                          ignore_reservations_list=ignore_reservations_list,
                          mode='real'
                        )
                    except 'InsufficientNodes' :
                        # Set duration smaller by FUDGE_FACTOR and try again
                        #duration_float = duration_float - FUDGE_FACTOR
                        print "full, short duration failed"
                        if duration < FUDGE_FACTOR :
                            print "standing res instance failed, no time!"
                            #low_duration = 1
                            break
                        if last_change < 0 :
                            last_change = last_change
                        else :
                            last_change = - last_change/2
                    else :
                        print "full, short duration succeeded"
                        if abs(last_change) < FUDGE_FACTOR :
                            full_found = 1
                            break
                        if last_change < 0 :
                            last_change = - last_change/2
                        else :
                            last_change = last_change
                        #if found_original_nodes < 2 :
                        #    found_original_nodes = found_original_nodes + 1
                        #    original_nodes = original_nodes + new_res['node_list']
                        #    sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', `original_nodes`, LAST_AVAILABLE_IGNORED_FIRST)
                if partial_found > 0 or full_found > 0 :
                    number_found = number_found + 1
            else :
                #print "full size, full duration succeeded"
                # Successful creation of standing reservation instance
                number_found = number_found + 1
                #if found_original_nodes == 0 :
                #    found_original_nodes = 2
                #    original_nodes = new_res['node_list']
                #    sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', `original_nodes`, LAST_AVAILABLE_IGNORED_FIRST)
            # increment time by 60 sec for next iteration,
            # so we don't keep choosing the same period
            new_candidate_min = new_candidate_tuple[4]
            original_candidate_min = new_candidate_tuple[4]
            while new_candidate_min == original_candidate_min :
                candidate_epoch = candidate_epoch + 60
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_min = new_candidate_tuple[4]
            temp_tuple = new_candidate_tuple
            new_candidate_tuple = (
              temp_tuple[0],
              temp_tuple[1],
              temp_tuple[2],
              temp_tuple[3],
              new_candidate_min,
              0,
              temp_tuple[6],
              temp_tuple[7],
              temp_tuple[8],
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)

def update_running_reservations(runningstarting_jobs, reservations_db_handle, resources_db_handle, jobs_db_handle, events_db_handle) :
    # need to clean this up
    resources_dict = resources_db_handle[0]
    existing_reservations = get_object_list(reservations_db_handle)
    save_jobs_list = []
    runningstarting_names_list = get_object_names_list(runningstarting_jobs)
    for reservation in existing_reservations :
        if reservation['purpose_type_string'] in ['running', 'preempted_job'] :
            delete_object(reservation['name'], reservations_db_handle)
        elif reservation['purpose_type_string'] in ['pushback',] :
            if not (jobs_db_handle[0].has_key(reservation['job_runID']) and jobs_db_handle[0][reservation['job_runID']]['state'] in ['Preempted', 'Running']) :
                delete_object(reservation['name'], reservations_db_handle)
                
    # need to do something differenct for node_usage:node_shared jobs.
    # take their resources_requested_list and create reservations according
    # to that list.  The easy way here would be to add a cpu_map and
    # mb_map to this reservation.  Then, when open_windows gets found,
    # use these, in addition to the node_list to find open_windows.
    # Maybe don't need cpu_map and mb_map.  generate those from
    # resources_requested_list?
    preempted_jobs_list = []
    for job in runningstarting_jobs :
        if job['state'] == 'Running' :
            purpose_type_string = 'running'
        elif job['state'] == 'Preempted' :
            preempted_jobs_list.append(copy.deepcopy(job))
            continue
        else :
            purpose_type_string = job['state']
            continue
        submit_time = job['SubmitTime']
        new_res_name = `int(submit_time)`
        while reservations_db_handle[0].has_key(new_res_name) :
            submit_time = submit_time + 1
            new_res_name = `int(submit_time)`
        new_res = initialize_reservation(new_res_name)
        if job.has_key('wall_clock_used') and job['wall_clock_used'] != None :
            wall_clock_used = job['wall_clock_used']
        else :
            wall_clock_used = Now_float - job['Dispatch_Time']
        # for preempted job, here is the tricky part.  The reservation
        # duration should be from start time to end of last job
        # on any of the preempted nodes + remaining job time
        # would it work to: make all the running reservations first,
        # restrict preempt job reservations to the preemted nodes,
        # set duration to wall clock remaining,
        # create the preempt reservations, set duration and start to now.
        if job['wall_clock_limit'] < wall_clock_used :
            duration_float = wall_clock_used + FUDGE_FACTOR
        else :
            duration_float = job['wall_clock_limit'] + FUDGE_FACTOR
        #start_time_float = job['Dispatch_Time']
        start_time_float = Now_float - wall_clock_used
        # FIXTHIS: if a node gets allocated twice in node_shared,
        # then the node will only be listed once in 'allocated_hosts'
        # for LoadL.  Should this be fixed in Catalina_LL.py?
        # Or should resource_amount_int mean number of nodes?
        # Should base allocated_hosts off of taskmachines in Catalina_LL.py
        # How to map task_hosts to initmap/requested_resource_list?
        # There should be one requested_resource_list for each node
        # of initmap, and one req_list entry for each task on that node.
        # for every task_host, should reserve a req_list.
        # are the sequences of task_hosts, initmap and requested_resource_list
        # matched for both PBS and LoadL?  For now, assume that they are.
        task_hosts = job['task_hosts']
        allocated_hosts = job['allocated_hosts']
        # If an allocated_host does not exist in the resource_dict,
        # I don't know how to make the reservation.
        unknown_hosts = 0
        for allocated_host in allocated_hosts :
            if not resources_db_handle[0].has_key(allocated_host) :
                unknown_hosts = unknown_hosts + 1
        if unknown_hosts > 0 :
            continue
        node_list = allocated_hosts
        resource_amount_int = job['resource_amount_int']
        new_res['duration_float'] = duration_float
        new_res['start_time_float'] = start_time_float
        new_res['end_time_float'] = start_time_float + duration_float
        new_res['node_list'] = node_list
        new_res['job_restriction'] = 'result = 1'
        new_res['resource_amount_int'] = resource_amount_int
        new_res['purpose_type_string'] = purpose_type_string
        new_res['job_runID'] = job['name']
        task_host_index = 0
        if job.has_key('requested_resource_list') :
            node_requested_list = filter(lambda x : x['type'] in ('node_shared', 'node_exclusive'), job['requested_resource_list'])
            #if len(node_list) != len(node_requested_list) :
            #    print "%s: node_list (%s) != requested_resource_list (%s)!" % \
            #      (job['name'], node_list, requested_resource_list)
            #for reqres in node_requested_list :
            allocated_dict_list = []
            # This only supports node cpu/memory.  To support floating
            # licenses, storage, etc. need to add these in as types.
            #for index in range(min(len(node_requested_list), len(node_list))) :
            for index in range(len(node_requested_list)) :
                # allocated_dict_list
                # This is a little tricky.  In create_reservation, need to
                # make sure that the node_list syncs up with
                # requested_resource_list...
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "node_requested_list (%s)" % (node_requested_list,)
                    #continue_var = raw_input("continue? ")
                #print "node_requested_list (%s)" % (node_requested_list,)
                node_usage = node_requested_list[index]['type']
                if node_requested_list[index]['type'] == 'node_shared' :
                    node_amount = 0
                    cpus = 0
                    memory = 0
                    if node_requested_list[index].has_key('req_list') :
                        nodename = task_hosts[task_host_index]
                        if nodename == '' :
                            # for serial jobs, taskinstancemachinemap is
                            # apparently left empty.  So, take the first
                            # allocated host, assuming this is a one node
                            # serial job
                            nodename = allocated_hosts[0]
                        for req in node_requested_list[index]['req_list'] :
                            if req.has_key('cpu') :
                                cpus = cpus + req['cpu']
                            if req.has_key('memory') :
                                memory = memory + req['memory']
                            task_host_index = task_host_index + 1
                    else :
                        #cpus = resources_dict[node_list[index]]['ConsumableCpus']
                        #memory = resources_dict[node_list[index]]['ConsumableMemory']
                        cpus = 0
                        memory = 0
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "node_shared cpus(%s) memory (%s)" % (cpus, memory)
                else :
                    node_amount = 1
                    cpus = resources_dict[node_list[index]]['ConsumableCpus']
                    memory = resources_dict[node_list[index]]['ConsumableMemory']
                    nodename = node_list[index]
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "node_exclusive cpus (%s) memory (%s)" % (cpus, memory)
                allocated_dict = { 'nodename' : nodename,
                                   'type' : node_usage,
                                   'node' : node_amount,
                                   'cpu' : cpus,
                                   'memory' : memory
                                 }
                      #{ 'nodename' : node,
                      #  'type' : 'node_exclusive',
                      #  'node' : 1,
                      #  'cpu' : resources_dict[node]['ConsumableCpus'],
                      #  'memory' : resources_dict[node]['ConsumableMemory'] }
                allocated_dict_list.append(allocated_dict)
        else :
            # if no requested_resource_list was found, assume that all
            # resources for the node were requested
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "no requested_resource_list"
            allocated_dict_list = []
            for index in range(len(node_list)) :
                cpus = resources_dict[node_list[index]]['ConsumableCpus']
                memory = resources_dict[node_list[index]]['ConsumableMemory']
                allocated_dict = { 'nodename' : node_list[index],
                                   'type' : 'node_exclusive',
                                   'cpu' : cpus,
                                   'memory' : memory
                                 }
                allocated_dict_list.append(allocated_dict)
        new_res['allocated_dict_list'] = allocated_dict_list
        insert_new_object(new_res, reservations_db_handle)
    for preempted_job in preempted_jobs_list :
        #print "creating preempted_job reservation for (%s)" % preempted_job['name']
        #def create_job_reservation(
          #resources_db_handle,
          #reservations_db_handle,
          #jobs_db_handle,
          #job_step,
          #events_db_handle
          #) :
        # create a job reservation with remaining wall_clock duration
        # at the end of any running reservations
        new_res = create_job_reservation(resources_db_handle,
                               reservations_db_handle,
                               jobs_db_handle,
                               preempted_job,
                               events_db_handle
                               )
        if new_res != None :
            #print "preempted_job reservation succeeded for (%s)" % preempted_job['name']
            del_pushback_list = []
            for res_key in reservations_db_handle[0].keys() :
                if reservations_db_handle[0][res_key]['purpose_type_string'] == "pushback" and reservations_db_handle[0][res_key]['job_runID'] == preempted_job['name'] :
                    del_pushback_list.append(res_key)
            for pushback_key in del_pushback_list :
                #print "deleting pushback res (%s)" % pushback_key
                delete_object(pushback_key, reservations_db_handle)
        else :
            pass
            #print "preempted_job reservation failed for (%s)" % preempted_job['name']
            

def get_open_windows_for_reservation_list(reservations, nodes) :
    # take a list of reservations
    # return a list of open_windows, sorted in ascending start order
    # (start, end, node_name)
    def sort_by_reservation_start(first, second) :
        if first['start_time_float'] < second['start_time_float'] :
            return -1
        if first['start_time_float'] == second['start_time_float'] :
            return 0
        if first['start_time_float'] > second['start_time_float'] :
            return 1
    reservations.sort(sort_by_reservation_start)
    open_windows_list = []
    for reservation in reservations :
        start_time_float = reservation['start_time_float']
        end_time_float = reservation['end_time_float']
        for node in reservation['node_list'] :
            if node in nodes :
                open_windows_list.append(
                  ( start_time_float, end_time_float, node )
                )
    return open_windows_list

def create_standing_reservation(
  standing_reservations_db_handle,
  start_spec_string,
  standing_TZ,
  duration_float,
  resource_amount_int,
  requested_resource_list,
  node_usage,
  affinity_calculation,
  sort_policy,
  depth,
  mode,
  comment='',
  conflict_policy=None,
  creator_string=username_string,
  latency_float=None,
  job_restriction='result = 1',
  node_restriction='result = 1',
  overlap_running_int = 0
  ) :
    if start_spec_string == None :
        raise 'NoStartSpec'
    if duration_float == None :
        raise 'NoDuration'
    if duration_float < 0 :
        raise 'NegativeDuration'
    if resource_amount_int == None :
        raise 'MissingAmount'
    if depth == None :
        raise 'NoDepth'
    start_spec_reo = re.compile(r"((\*|(\d+,?)+)\s*){5}")
    if not start_spec_reo.match(start_spec_string) :
        raise 'BadStartSpec', start_spec_string
    new_standing_reservation_name = get_new_db_key(standing_reservations_db_handle)
    new_standing_reservation = initialize_reservation(new_standing_reservation_name)
    new_standing_reservation['start_spec_string'] = start_spec_string
    new_standing_reservation['TZ_string'] = standing_TZ
    new_standing_reservation['duration_float'] = duration_float
    new_standing_reservation['latency_float'] = latency_float
    new_standing_reservation['creator_string'] = creator_string
    new_standing_reservation['resource_amount_int'] = resource_amount_int
    new_standing_reservation['requested_resource_list'] = requested_resource_list
    new_standing_reservation['node_usage'] = node_usage
    new_standing_reservation['job_restriction'] = job_restriction
    new_standing_reservation['node_restriction'] = node_restriction
    new_standing_reservation['conflict_policy'] = conflict_policy
    new_standing_reservation['sort_policy'] = sort_policy
    new_standing_reservation['depth'] = string.atoi(depth)
    new_standing_reservation['comment_string'] = comment
    new_standing_reservation['overlap_running_int'] = overlap_running_int
    insert_new_object(
      new_object=new_standing_reservation,
      db_handle=standing_reservations_db_handle
    )
    return new_standing_reservation
    
def create_job_reservation(
  resources_db_handle,
  reservations_db_handle,
  jobs_db_handle,
  job_step,
  events_db_handle
) :
    # Create a job reservation by calling create_reservation and
    # passing in runID, resource_amount_int, duration_float, purpose_type_string='job'
    job_runID = job_step['name']
    preempted_ignore_list = []
    if job_step.has_key('run_at_risk_int') and job_step['run_at_risk_int'] >= 1 :
        duration_float = RUN_AT_RISK_MIN_RUNTIME + FUDGE_FACTOR
    elif job_step['state'] == 'Preempted' :
        # schedule preemptible jobs at full wallclock time,
        # we schedule at full wallclock, since the schedule
        # will be wrong, if no preemption occurs.
        #duration_float = RUN_AT_RISK_MIN_RUNTIME + FUDGE_FACTOR
        # for preempted jobs, need to restrict to the nodes the job
        # currently occupies, should put this into the Catalina_RM
        # module...
        #print "setting duration_float for Preempted job to (%s)" % (job_step['wall_clock_limit'] - job_step['wall_clock_used'] + FUDGE_FACTOR,)
        duration_float = job_step['wall_clock_limit'] - job_step['wall_clock_used'] + FUDGE_FACTOR
        # ignore any 'pushback' reservations with the same 'job_runID'
        # delete the pushback reservations, if the create_res is successful
        for pushback_key in reservations_db_handle[0].keys() :
            if reservations_db_handle[0][pushback_key]['purpose_type_string'] == 'pushback' and \
              reservations_db_handle[0][pushback_key]['job_runID'] == job_step['name'] :
                preempted_ignore_list.append(pushback_key)
    else :
        duration_float = job_step['wall_clock_limit'] + FUDGE_FACTOR
    resource_amount_int = job_step['resource_amount_int']
    if job_step.has_key('node_usage') and job_step['node_usage'] == 'node_shared' :
        node_dict_type = 'node_shared'
    else :
        node_dict_type = 'node_exclusive'
    if job_step.has_key('requested_resource_list') :
        requested_resource_list = job_step['requested_resource_list']
    else :
        requested_resource_list = []
    if job_step['state'] == 'Preempted' :
        purpose_type_string = 'preempted_job'
    else :
        purpose_type_string = 'job'
    node_restriction = None
    conflict_policy = None
    node_sort_policy = None
    job_restriction = "if input_tuple[0]['name'] == '%s' : result = 0" % job_step['name']
    new_res = create_reservation(
      resources_db_handle=resources_db_handle,
      reservations_db_handle=reservations_db_handle,
      jobs_db_handle=jobs_db_handle,
      duration_float=duration_float,
      resource_amount_int=resource_amount_int,
      requested_resource_list=requested_resource_list,
      ignore_reservations_list=preempted_ignore_list,
      node_sort_policy=node_sort_policy,
      node_usage=node_dict_type,
      node_restriction = node_restriction,
      conflict_policy=conflict_policy,
      job_runID=job_runID,
      job_restriction=job_restriction,
      purpose_type_string=purpose_type_string,
      job_step=job_step,
      mode='real'
      )
    #if job_step['state'] == 'Preempted' :
    #    # should set start_time_float to Now_float, and duration accordingly
    #    # def update_object_attribute(name, value, object, db_handle) :
    #    start_time_float = Now_float
    #    update_object_attribute('start_time_float', start_time_float, new_res, reservations_db_handle)
    #    end_time_float = Now_float + duration_float
    #    update_object_attribute('end_time_float', end_time_float, new_res, reservations_db_handle)
    #    update_object_attribute('duration_float', duration_float, new_res, reservations_db_handle)

    return new_res


def create_job_reservations(eligible_jobs, resources_db_handle, reservations_db_handle, jobs_db_handle, events_db_handle) :
    reservations_created = 0
    jobs_dict = jobs_db_handle[0]
    running_jobs_per_user = {}
    running_jobs_per_account_per_qos = {}
    running_node_sec_per_account_per_qos = {}
    running_node_sec_per_user_per_qos = {}
    for job_name in jobs_dict.keys() :
        state = jobs_dict[job_name]['state']
        user = jobs_dict[job_name]['user']
        account = jobs_dict[job_name]['account']
        qos = jobs_dict[job_name]['QOS']
        if state in ['Running','Starting'] :
            if running_jobs_per_user.has_key(user) :
                running_jobs_per_user[user] = running_jobs_per_user[user] + 1
            else :
                running_jobs_per_user[user] = 1
            if running_jobs_per_account_per_qos.has_key(account) :
                if running_jobs_per_account_per_qos[account].has_key(qos) :
                    running_jobs_per_account_per_qos[account][qos] = running_jobs_per_account_per_qos[account][qos] + 1
                else :
                    running_jobs_per_account_per_qos[account][qos] = 1
            else :
                running_jobs_per_account_per_qos[account] = { qos : 1, }
            if running_node_sec_per_user_per_qos.has_key(user) :
                if running_node_sec_per_user_per_qos[user].has_key(qos) :
                    running_node_sec_per_user_per_qos[user][qos] = running_node_sec_per_user_per_qos[user][qos] + jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit']
                else :
                    running_node_sec_per_user_per_qos[user][qos] = jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit']
            else :
                running_node_sec_per_user_per_qos[user] = { qos : jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit'], }
            if running_node_sec_per_account_per_qos.has_key(account) :
                if running_node_sec_per_account_per_qos[account].has_key(qos) :
                    running_node_sec_per_account_per_qos[account][qos] = running_node_sec_per_account_per_qos[account][qos] + jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit']
                else :
                    running_node_sec_per_account_per_qos[account][qos] = jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit']
            else :
                running_node_sec_per_account_per_qos[account] = { qos : jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit'], }
    # Potential race condition.  If a job start was initiated last
    # iteration, but the RM does not report the job as Running or
    # Starting, then it won't contribute to the running_jobs_per_user
    # count.  This will allow jobs past the limit...
    # Consider keeping a list of all active job reservations for
    # that user...
    active_reservations_per_user = {}
    active_reservations_per_account_per_qos = {}
    active_reserved_node_sec_per_account_per_qos = {}
    active_reserved_node_sec_per_user_per_qos = {}
    existing_reservations = get_object_list(reservations_db_handle)
    for reservation in existing_reservations :
        if reservation['purpose_type_string'] == 'job' and \
          ( not reservation.has_key('start_count_int') or \
          not reservation['start_count_int'] >= 1 or \
          Now_float > reservation['start_time_float'] + JOB_START_TIME_LIMIT or \
          ( jobs_dict.has_key(reservation['job_runID']) and \
          jobs_dict[reservation['job_runID']]['state'] in ['Running', 'Removed', 'Canceled', 'Completed']) ):
            delete_object(reservation['name'], reservations_db_handle)
            if ( (jobs_dict.has_key(reservation['job_runID']) and \
                not jobs_dict[reservation['job_runID']]['state'] in ['Unknown']) \
              or LOST_JOB_WARN == 'TRUE' ) and \
              ( jobs_dict.has_key(reservation['job_runID']) and \
                not jobs_dict[reservation['job_runID']]['state'] in ['Removed', 'Canceled', 'Completed'] ) and \
              SERVERMODE == 'NORMAL' and \
              Now_float > reservation['start_time_float'] + JOB_START_TIME_LIMIT \
              and ( JOB_START_WARN_LIMIT == None or \
              jobs_dict[reservation['job_runID']]['job_start_warns_int'] < JOB_START_WARN_LIMIT ) :
                message = "job (%s) has exceeded JOB_START_TIME_LIMIT, reservation deleted" % reservation['job_runID']
                recipient = MAIL_RECIPIENT
                subject = "Catalina job reservation timed out"
                if jobs_dict[reservation['job_runID']].has_key('job_start_warns_int') :
                    warn_count = jobs_dict[reservation['job_runID']]['job_start_warns_int'] + 1
                else :
                    warn_count = 1
                warn(message, subject, recipient)
                catsyslog(message,'notice')
                update_object_attribute('job_start_warns_int', warn_count, jobs_dict[reservation['job_runID']], jobs_db_handle)
    existing_reservations = get_object_list(reservations_db_handle)
    for reservation in existing_reservations :
        if ( reservation['purpose_type_string'] == 'job' \
          or reservation['purpose_type_string'] == 'running' ) \
          and jobs_dict.has_key(reservation['job_runID']) :
            if (Now_float + 1) >= reservation['start_time_float'] :
                active_res_owner = jobs_dict[reservation['job_runID']]['user']
                active_res_account = jobs_dict[reservation['job_runID']]['account']
                if active_reservations_per_user.has_key(active_res_owner) :
                    active_reservations_per_user[active_res_owner].append(reservation['job_runID'])
                else :
                    active_reservations_per_user[active_res_owner] = [reservation['job_runID'],]
                if active_reservations_per_account_per_qos.has_key(active_res_account) :
                    if active_reservations_per_account_per_qos[active_res_account].has_key(jobs_dict[reservation['job_runID']]['QOS']) :
                        active_reservations_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']].append(reservation['job_runID'])
                    else :
                        active_reservations_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']] = [reservation['job_runID'],]
                else :
                    active_reservations_per_account_per_qos[active_res_account] = { jobs_dict[reservation['job_runID']]['QOS'] : [reservation['job_runID'],] , }
                if active_reserved_node_sec_per_user_per_qos.has_key(active_res_owner) :
                    if active_reserved_node_sec_per_user_per_qos[active_res_owner].has_key(jobs_dict[reservation['job_runID']]['QOS']) :
                        active_reserved_node_sec_per_user_per_qos[active_res_owner][jobs_dict[reservation['job_runID']]['QOS']] = active_reserved_node_sec_per_user_per_qos[active_res_owner][jobs_dict[reservation['job_runID']]['QOS']] + jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit']
                    else :
                        active_reserved_node_sec_per_user_per_qos[active_res_owner][jobs_dict[reservation['job_runID']]['QOS']] = jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit']
                else :
                    active_reserved_node_sec_per_user_per_qos[active_res_owner] = { jobs_dict[reservation['job_runID']]['QOS'] : jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit'], }
                if active_reserved_node_sec_per_account_per_qos.has_key(active_res_account) :
                    if active_reserved_node_sec_per_account_per_qos[active_res_account].has_key(jobs_dict[reservation['job_runID']]['QOS']) :
                        active_reserved_node_sec_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']] = active_reserved_node_sec_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']] + jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit']
                    else :
                        active_reserved_node_sec_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']] = jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit']
                else :
                    active_reserved_node_sec_per_account_per_qos[active_res_account] = { jobs_dict[reservation['job_runID']]['QOS'] : jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit'], }
    for job_step in eligible_jobs :
        if RESERVATION_DEPTH != None and reservations_created >= RESERVATION_DEPTH :
            break
        if MAXJOBPERUSERPOLICY != None and \
           MAXJOBPERUSERPOLICY == 'ON' and \
          ( running_jobs_per_user.has_key(job_step['user']) and \
          running_jobs_per_user[job_step['user']] >= \
          MAXJOBPERUSERCOUNT or \
          active_reservations_per_user.has_key(job_step['user']) and \
          len(active_reservations_per_user[job_step['user']]) >= \
          MAXJOBPERUSERCOUNT ) :
            update_object_attribute(
              'ineligible_reason',
              'MAXJOBPERUSERPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        if QOS_MAXJOBPERUSERPOLICY_dict.has_key(job_step['QOS']) and \
           QOS_MAXJOBPERUSERPOLICY_dict[job_step['QOS']] != None and \
          ( running_jobs_per_user.has_key(job_step['user']) and \
          running_jobs_per_user[job_step['user']] >= \
          QOS_MAXJOBPERUSERPOLICY_dict[job_step['QOS']] or \
          active_reservations_per_user.has_key(job_step['user']) and \
          len(active_reservations_per_user[job_step['user']]) >= \
          QOS_MAXJOBPERUSERPOLICY_dict[job_step['QOS']] ) :
            update_object_attribute(
              'ineligible_reason',
              'QOSMAXJOBPERUSERPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        if QOS_MAXJOBPERACCOUNTPOLICY_dict.has_key(job_step['QOS']) and \
           QOS_MAXJOBPERACCOUNTPOLICY_dict[job_step['QOS']] != None and \
          ( running_jobs_per_account_per_qos.has_key(job_step['account']) and \
          running_jobs_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) and \
          running_jobs_per_account_per_qos[job_step['account']][job_step['QOS']] >= \
          QOS_MAXJOBPERACCOUNTPOLICY_dict[job_step['QOS']] or \
          active_reservations_per_account_per_qos.has_key(job_step['account']) and \
          active_reservations_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) and \
          len(active_reservations_per_account_per_qos[job_step['account']][job_step['QOS']]) >= \
          QOS_MAXJOBPERACCOUNTPOLICY_dict[job_step['QOS']] ) :
            update_object_attribute(
              'ineligible_reason',
              'MAXJOBPERACCOUNTPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict') and \
           QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict.has_key(job_step['QOS']) and \
           QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job_step['QOS']] != None and \
          ( running_node_sec_per_user_per_qos.has_key(job_step['user']) and \
          running_node_sec_per_user_per_qos[job_step['user']].has_key(job_step['QOS']) and \
          running_node_sec_per_user_per_qos[job_step['user']][job_step['QOS']] >= \
          QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job_step['QOS']] or \
          active_reserved_node_sec_per_user_per_qos.has_key(job_step['user']) and \
          active_reserved_node_sec_per_user_per_qos[job_step['user']].has_key(job_step['QOS']) and \
          active_reserved_node_sec_per_user_per_qos[job_step['user']][job_step['QOS']] >= QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job_step['QOS']] ) :
            update_object_attribute(
              'ineligible_reason',
              'QOSMAXNODESECRUNNINGPERUSERPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict') and \
           QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict.has_key(job_step['QOS']) and \
           QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job_step['QOS']] != None and \
          ( running_node_sec_per_account_per_qos.has_key(job_step['account']) and \
          running_node_sec_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) and \
          running_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] >= \
          QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job_step['QOS']] or \
          active_reserved_node_sec_per_account_per_qos.has_key(job_step['account']) and \
          active_reserved_node_sec_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) and \
          active_reserved_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] >= QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job_step['QOS']] ) :
            update_object_attribute(
              'ineligible_reason',
              'QOSMAXNODESECRUNNINGPERACCOUNTPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        try :
            for reservation in existing_reservations :
                if reservation['purpose_type_string'] == 'job' and \
                  reservation.has_key('job_runID') and \
                  reservation['job_runID'] == job_step['name'] :
                    raise 'FoundJob'
        except 'FoundJob' :
            continue
        try :
            new_res = create_job_reservation(
              resources_db_handle=resources_db_handle,
              reservations_db_handle=reservations_db_handle,
              jobs_db_handle=jobs_db_handle,
              job_step=job_step,
              events_db_handle=events_db_handle
            )
            if (Now_float + 1) >= new_res['start_time_float'] and \
              (Now_float - new_res['start_time_float']) <= FUDGE_FACTOR/2 :
                if running_jobs_per_user.has_key(job_step['user']) :
                    running_jobs_per_user[job_step['user']] = running_jobs_per_user[job_step['user']] + 1
                else :
                    running_jobs_per_user[job_step['user']] = 1
                if running_jobs_per_account_per_qos.has_key(job_step['account']) :
                    if running_jobs_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) :
                        running_jobs_per_account_per_qos[job_step['account']][job_step['QOS']] = running_jobs_per_account_per_qos[job_step['account']][job_step['QOS']] + 1
                    else :
                        running_jobs_per_account_per_qos[job_step['account']][job_step['QOS']] = 1
                else :
                    running_jobs_per_account_per_qos[job_step['account']] = { job_step['QOS'] : 1 }
                if running_node_sec_per_account_per_qos.has_key(job_step['account']) :
                    if running_node_sec_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) :
                        running_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] = running_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] + job_step['resource_amount_int'] * job_step['wall_clock_limit']
                    else :
                        running_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] = job_step['resource_amount_int'] * job_step['wall_clock_limit']
                else :
                    running_node_sec_per_account_per_qos[job_step['account']] = { job_step['QOS'] : job_step['resource_amount_int'] * job_step['wall_clock_limit'] }
                if active_reservations_per_user.has_key(job_step['user']) :
                    if not job_step['name'] in active_reservations_per_user[job_step['user']] :
                        active_reservations_per_user[job_step['user']].append(job_step['name'])
                else :
                    active_reservations_per_user[job_step['user']] = [job_step['name'],]
                if active_reservations_per_account_per_qos.has_key(job_step['account']) :
                    if active_reservations_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) :
                        if not job_step['name'] in active_reservations_per_account_per_qos[job_step['account']][job_step['QOS']] :
                            active_reservations_per_account_per_qos[job_step['account']][job_step['QOS']].append(job_step['name'])
                    else :
                        active_reservations_per_account_per_qos[job_step['account']][job_step['QOS']] = [job_step['name'],]
                else :
                    active_reservations_per_account_per_qos[job_step['account']] = { job_step['QOS'] : [job_step['name'],], }
        except 'InsufficientNodes', new_res :
            message = "job reservation (%s) failed for runID (%s) due to insufficient resources" % ( new_res['name'], new_res['job_runID'] )
            print message
            catsyslog(message,'notice')
        except :
            try :
                info_tuple = sys.exc_info()
                print "(%s) (%s) (%s)" % info_tuple
                traceback.print_tb(info_tuple[2])
            except :
                print "print of sys.exc_info() failed"
        else :
            reservations_created = reservations_created + 1

def set_system_priority(job_step, system_priority_int, jobs_db_handle) :
    if system_priority_int == 0 :
        update_object_attribute('system_priority_int', None, job_step, jobs_db_handle)
        update_object_attribute('system_priority_mark_string', None, job_step, jobs_db_handle)
    else :
        update_object_attribute('system_priority_int', system_priority_int, job_step, jobs_db_handle)
        update_object_attribute('system_priority_mark_string', '*', job_step, jobs_db_handle)

def set_resource_usability(resources_db_handle, jobs_db_handle) :
    resources_dict = resources_db_handle[0]
    jobs_dict = jobs_db_handle[0]
    for resource_name in resources_dict.keys() :
        node_seconds_int = 0
        for job_name in jobs_dict.keys() :
            if jobs_dict[job_name].has_key('resource_dict_list') :
                for element in jobs_dict[job_name]['resource_dict_list'] :
                    if element['resource_dict'].has_key(resource_name) :
                        node_seconds_int = node_seconds_int + long(element['amount_int']) * long(jobs_dict[job_name]['wall_clock_limit'])
        update_object_attribute('resource_usability_int', node_seconds_int, resources_dict[resource_name], resources_db_handle)
        if DEBUGJOB != None :
            print "%s resource_usability (%s)" % (resource_name, node_seconds_int)

def update_resources(resources_db_handle, cfg_resources_db_handle,jobs_db_handle) :
    resource_shelf = resources_db_handle[0]
    old_resources = get_object_list(resources_db_handle)
    old_resources_names = get_object_names_list(old_resources)
    resource_list = get_resources_list()
    cfg_resources_shelf = cfg_resources_db_handle[0]
    cfg_resources_list = get_configured_resources_list(resources_db_handle)
    cfg_resources_names = get_object_names_list(cfg_resources_list)
    found_resource_dict = {}
    found_resource_names = []
    for resource in resource_list :
        found_resource_dict[resource['name']] = resource
        found_resource_names.append(resource['name'])
        if resource_shelf.has_key(resource['name']) :
            for key in resource.keys() :
                # This is kind of convoluted...
                if key == 'State_Change_Time' :
                    continue
                if key == 'State' :
                    if resource['State'] == 'Down':
                        #print "Down state for (%s)" % resource['name']
                        # Delay before considering a node really Down.
                        if resource['speculative_state'] != resource_shelf[resource['name']]['speculative_state'] :
                            #print "New Down state for (%s)" % resource['name']
                            update_object_attribute('State_Change_Time', Now_float, resource_shelf[resource['name']], resources_db_handle)
                            update_object_attribute('speculative_state', resource[key], resource_shelf[resource['name']], resources_db_handle)
                        else :
                            #print "Old Down state for (%s)" % resource['name']
                            # node has reported Down again
                            if Now_float - resource_shelf[resource['name']]['State_Change_Time'] > RESOURCE_DOWN_TIME_LIMIT :
                                #print "RESOURCE_DOWN_TIME_LIMIT exceeded for (%s)" % resource['name']
                                update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                    else :
                        # No delay for non-Down nodes
                        if resource['State'] != resource_shelf[resource['name']]['State'] :
                            update_object_attribute('State_Change_Time', Now_float, resource_shelf[resource['name']], resources_db_handle)
                            update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                        if resource['State'] != resource_shelf[resource['name']]['speculative_state'] :
                            update_object_attribute('speculative_state', resource[key], resource_shelf[resource['name']], resources_db_handle)
                        
                    #if resource['State'] != resource_shelf[resource['name']]['State'] :
                    #    # Could put in a delay here to work around
                    #    # bogus Down nodes from LoadLeveler.
                    #    # 
                    #    update_object_attribute('State_Change_Time', Now_float, resource_shelf[resource['name']], resources_db_handle)
                    #    if resource['State'] != 'Down' :
                    #        update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                    #    else :
                    #        update_object_attribute('speculative_state', resource[key], resource_shelf[resource['name']], resources_db_handle)
                    #else :
                    #    if resource['State'] == 'Down' :
                    #        if Now_float - resource_shelf[resource['name']]['State_Change_Time'] > LOST_NODE_LIMIT :
                    #            update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                else :
                    update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
        else :
            insert_new_object(resource, resources_db_handle)
    lost_resource_names_list = []
    down_resource_names_dict = {}
    for resource in old_resources :
        if not resource['State'] in ['Idle', 'Running'] :
            continue
        if not resource['name'] in found_resource_names :
            lost_resource_names_list.append(resource['name'])
        elif not found_resource_dict[resource['name']]['State'] in ['Idle', 'Running'] :
            # put delay in here, too
            if Now_float - resource_shelf[resource['name']]['State_Change_Time'] > RESOURCE_DOWN_TIME_LIMIT :
                down_resource_names_dict[resource['name']] = found_resource_dict[resource['name']]['State']
    down_resource_names_list = map(lambda x, down_resource_names_dict=down_resource_names_dict : "%s - %s" % (x, down_resource_names_dict[x]), down_resource_names_dict.keys())

    #Martin Margo: Just a hack for now, remove the broken nodes from the job eligible nodes
    # KKY: took this out, so that there would be a delay for transiently
    # down nodes.
    # Should probably put the delay down here too.
    filter_out_down_nodes( down_resource_names_dict.keys() + lost_resource_names_list, jobs_db_handle)

    if len(lost_resource_names_list) > 0 or len(down_resource_names_list) > 0 :
        recipient = MAIL_RECIPIENT
        subject = "Available node count has decreased"
        message = """The following nodes are not found: 
%s

The following nodes are no longer in Idle or Running state:
%s""" % (lost_resource_names_list, down_resource_names_list)
        warn(message, subject, recipient)
        catsyslog(message,'notice')
    for resource in cfg_resources_list :
        if not resource['name'] in found_resource_names :
            if resource['name'] in resource_shelf.keys() :
                for key in resource.keys() :
                    update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                if resource_shelf[resource['name']]['State'] != None :
                    update_object_attribute('State', None,
                      resource_shelf[resource['name']], resources_db_handle)
                    update_object_attribute('State_Change_Time', Now_float,
                      resource_shelf[resource['name']], resources_db_handle)
            else :
                insert_new_object(resource, resources_db_handle)
    for resource_name in old_resources_names :
        if resource_name not in cfg_resources_names and \
          resource_name not in found_resource_names :
            delete_object(resource_name, resources_db_handle)

def update_max_pushback(jobs_db_handle, reservations_db_handle, resources_db_handle) :
    # update 'max_pushback' for each preemptible job
    # create a dictionary, with nodenames as keys.  Put in a list
    # of non-job reservations for each nodename
    # for each running, preemptible job reservation, for each node allocated to
    # that job, see if there are any non-job reservations.  Find the earliest
    # one, and set max_pushback to the start of the earliest non-job
    # reservation on any of those nodes.
    jobs_dict = jobs_db_handle[0]
    reservations_dict = reservations_db_handle[0]
    resources_dict = resources_db_handle[0]
    node_res_dict = {}
    for res_key in reservations_dict.keys() :
        #if reservations_dict[res_key]['purpose_type_string'] == 'job' :
        #    continue
        for allocated_dict in reservations_dict[res_key]['allocated_dict_list'] :
            if node_res_dict.has_key(allocated_dict['nodename']) :
                node_res_dict[allocated_dict['nodename']].append(copy.deepcopy(reservations_dict[res_key]))
            else :
                node_res_dict[allocated_dict['nodename']] = [copy.deepcopy(reservations_dict[res_key]),]
    for node_key in node_res_dict.keys() :
        # create an earliest-to-latest event list for each node
        event_list = []
        for res in node_res_dict[node_key] :
            node_resource_list = filter(lambda x : x.has_key('nodename') and x['nodename'] == node_key, res['allocated_dict_list'])
            event_list.append((res['start_time_float'],
                              'decrement',
                              copy.deepcopy(node_resource_list)
                             ))
            event_list.append((res['end_time_float'],
                              'add',
                              copy.deepcopy(node_resource_list)
                             ))
        temp_list = []
        for event in event_list :
            temp_list.append((event[0], event))
        temp_list.sort()
        new_list = map(lambda x : x[1], temp_list)
                              
        node_res_dict[node_key] = new_list
    
    for res_key in reservations_dict.keys() :
        if reservations_dict[res_key]['purpose_type_string'] != 'running' \
          or jobs_dict[reservations_dict[res_key]['job_runID']]['preemptible'] == None \
          or jobs_dict[reservations_dict[res_key]['job_runID']]['preemptible'] < 1 :
            update_object_attribute('max_pushback_float', Now_float, reservations_dict[res_key], reservations_db_handle)
            continue
        max_pushback = END_OF_SCHEDULING
        for allocated_dict in reservations_dict[res_key]['allocated_dict_list'] :
            allocated_node = 0
            allocated_cpu = 0
            allocated_memory = 0
            required_node_node = allocated_dict['node']
            required_node_cpu = allocated_dict['cpu']
            required_node_memory = allocated_dict['memory']
            configured_node_node = 1
            configured_node_cpu = resources_dict[allocated_dict['nodename']]['ConsumableCpus']
            configured_node_memory = resources_dict[allocated_dict['nodename']]['ConsumableMemory']
            if node_res_dict.has_key(allocated_dict['nodename']) :
                for event in node_res_dict[allocated_dict['nodename']] :
                    if max_pushback < event[0] :
                        break
                    event_node = event[2]['node']
                    event_cpu = event[2]['cpu']
                    event_memory = event[2]['memory']
                    if event[1] == 'decrement' :
                        allocated_node = allocated_node + event[2]['node']
                        allocated_cpu = allocated_cpu + event[2]['cpu']
                        allocated_memory = allocated_memory + event[2]['memory']
                    elif event[1] == 'add' :
                        allocated_node = allocated_node - event[2]['node']
                        allocated_cpu = allocated_cpu - event[2]['cpu']
                        allocated_memory = allocated_memory - event[2]['memory']
                    if event[2]['type'] == 'node_exclusive' \
                      or configured_node_node < allocated_node \
                      or configured_node_cpu < allocated_cpu \
                      or configured_node_memory < allocated_memory :
                        max_pushback = event[0] - 1
                        break
        update_object_attribute('max_pushback_float', max_pushback, reservations_dict[res_key], reservations_db_handle)

def schedule_jobs(events_db_handle, jobs_db_handle, resources_db_handle, reservations_db_handle, cfg_resources_db_handle, standing_reservations_db_handle) :
    # the jobs and reservations handles should be write, the resources handle
    # can be read
    # update speculative_system_queue_time for each job
    #  - if system_queue_time = None : speculative_system_queue_time = Now_float
    #    else : speculative_system_queue_time = system_queue_time
    # update priority for all jobs
    # screen jobs through policies.  Set new system_queue_time
    # for newly eligible jobs.  Set system_queue_time to None for ineligible
    # jobs
    # create new job reservations, in priority order
    print "update_resources and filter out bad nodes from job list"
    update_resources(resources_db_handle, cfg_resources_db_handle,jobs_db_handle)
    print "update_job_info"
    update_job_info(jobs_db_handle)
    print "cancel_overrun_jobs"
    cancel_overrun_jobs(events_db_handle, jobs_db_handle)
    print "cancel_bad_jobs"
    cancel_bad_jobs(jobs_db_handle, resources_db_handle, events_db_handle)
    print "update_job_resource_lists"
    update_job_resource_lists(jobs_db_handle, resources_db_handle)
    print "update_resource_usability"
    set_resource_usability(resources_db_handle, jobs_db_handle)
    print "update_job_speculative_system_queue_time"
    update_job_speculative_system_queue_time(jobs_db_handle)
    print "update_job_priorities"
    update_job_priorities(jobs_db_handle)
    print "get_eligible_and_running_jobs"
    (eligible_jobs, runningstarting_jobs) = get_eligible_and_running_jobs(jobs_db_handle,
        resources_db_handle, reservations_db_handle)
    print "update_running_reservations"
    update_running_reservations(runningstarting_jobs, reservations_db_handle, resources_db_handle, jobs_db_handle, events_db_handle)
    print "updating standing reservations"
    update_standing_reservations(
      events_db_handle=events_db_handle,
      jobs_db_handle=jobs_db_handle,
      resources_db_handle=resources_db_handle,
      reservations_db_handle=reservations_db_handle,
      standing_reservations_db_handle=standing_reservations_db_handle)
    print "migrating shortpools"
    migrate_shortpools(jobs_db_handle, resources_db_handle, reservations_db_handle)
    #print "update_max_pushback"
    #update_max_pushback(jobs_db_handle, reservations_db_handle, resources_db_handle)
    print "create_job_reservations"
    create_job_reservations(eligible_jobs, resources_db_handle,
        reservations_db_handle, jobs_db_handle, events_db_handle)
    print "cancel run-at-risk jobs"
    #cancel_risk_jobs(reservations_db_handle=reservations_db_handle,
    #                 jobs_db_handle=jobs_db_handle,
    #                 events_db_handle=events_db_handle
    #                 resources_db_handle=resources_db_handle)
    cancel_risk_reservations(reservations_db_handle=reservations_db_handle,
                     jobs_db_handle=jobs_db_handle,
                     events_db_handle=events_db_handle,
                     resources_db_handle=resources_db_handle)
    print "run_jobs"
    run_jobs(events_db_handle, jobs_db_handle, resources_db_handle, reservations_db_handle)

# To implement multiple resource scheduling,
# - get the multiple requirement spec from the RM
#   like PBS: tg-c002+16:ppn=1:compute
#   this should be req_list = [{tasks : <ppn>, req_code : <requirements code>}...]
# - in get_resource_list, use this to set the jobs resource_list_list,
#   a list of screened resources for each element of req_list
# - in create_reservation, run through get_screened_nodes, get_open_windows_list,
#   get_sized_windows_list, get_sorted_windows_list, get_chosen_nodes_list
#   and accompanying code once for each req_list element.
# - consolidate the several reservation windows into one.
# Naah, this won't work.  the separate reservations need to by synced
# - get the multiple requirement spec from the RM
#   like PBS: tg-c002+16:ppn=1:compute
#   this should be req_list = [{'tasks' : <ppn>, 'req_code' : <requirements code>}...]
# - in get_resource_list, use this to set the jobs resource_list_dict,
#   a dict of screened resources for each element of req_list and the
#   number of nodes for that element:
#   resource_list_dict = [{'amount' : <number of nodes for that element>,
#                          'resource_list' : <list of node names for that element>},
#                         ...
#                         ]
#   sort the list in increasing length of resource_list.
# - in get_sized_windows_list, use resource_list_dict to search for
#   a set of nodes filling the amount requirement for each element
#   of req_list.
# - in get_chosen_nodes_list, select nodes according to the amounts
#   in resource_list_dict
def create_reservation(
  resources_db_handle=None,
  reservations_db_handle=None,
  jobs_db_handle=None,
  old_res_id=None,
  earliest_start_float=None,
  latest_end_float=None,
  duration_float=None,
  latency_float=None,
  resource_amount_int=None,
  resource_amount_requested_int=None,
  requested_resource_list=[],
  job_restriction='result = 1',
  node_restriction=None,
  node_usage='node_exclusive',
  resource_dict_list=None,
  node_sort_policy=None,
  conflict_policy=None,
  affinity_calculation=None,
  purpose_type_string='generic',
  account_string=None,
  creator_string=username_string,
  job_runID=None,
  job_step=None,
  ignore_reservations_list=None,
  blocking_reservations_list=[],
  accepted_nodes_list=None,
  comment_string=None,
  notify_string=None,
  mode='lookahead',
  max_resource_int=None
) :
    if duration_float == None :
        raise 'NoDuration'
    if duration_float < 0 :
        raise 'NegativeDuration'
    if resource_amount_int == None and earliest_start_float == None :
        raise 'MissingAmountAndStart'
    if earliest_start_float != None and earliest_start_float < 0 :
        raise 'NegativeStart'
    if latest_end_float != None and latest_end_float < 0 :
        raise 'NegativeEnd'
    if earliest_start_float != None and \
      latest_end_float != None and \
      earliest_start_float > latest_end_float :
        raise 'StartAfterEnd'
    if resource_amount_int != None and max_resource_int != None and \
      resource_amount_int > max_resource_int :
        raise 'RequestGTMax'
    if purpose_type_string == 'user_set' and account_string == None :
        raise 'NoAccount'
    if job_restriction == None :
        job_restriction = 'result = 1'

    #print "create_res: earliest_start_float (%s) (%s)" % (time.asctime(time.localtime(earliest_start_float)), earliest_start_float)
    #print "create_res: latest_end_float (%s) (%s)" % (time.asctime(time.localtime(latest_end_float)), latest_end_float)
    existing_reservations = get_object_list(reservations_db_handle)

    # If this is not a job reservation, delete all job reservations
    # so that the
    # runjob routine doesn't start jobs within the new reservation
    # due to an old overlapping job reservation
    # The reservations db should be write-locked, so the runjobs
    # routine won't run while this is going on...this would need to
    # be changed if the db went to a non-locking implementation...
    # Do not delete job reservations here.  Let create_job_reservations
    # do it.  To prevent run_jobs from running old reservations,
    # do not run run_jobs, except within schedule_jobs, when the
    # reservations db is locked.

    if old_res_id != None :
        saved_reservations = []
        for reservation in existing_reservations :
            if reservation['name'] != old_res_id :
                saved_reservations.append(reservation)
        existing_reservations = saved_reservations
    # Filter out any reservations named in ignore_reservations_list
    if ignore_reservations_list != None :
        filtered_reservations = filter(
          lambda reservation, list=ignore_reservations_list :
              not reservation['name'] in list,
              existing_reservations )
        existing_reservations = filtered_reservations

    if job_step != None :
        submit_time = job_step['SubmitTime']
        reservation_name = `int(submit_time)`
        #if job_step.has_key('node_usage') :
            #node_usage = job_step['node_usage']
        while reservations_db_handle[0].has_key(reservation_name) :
            submit_time = submit_time + 1
            reservation_name = `int(submit_time)`
    elif purpose_type_string == 'standing_reservation' :
        name_start_time = earliest_start_float
        reservation_name = `int(name_start_time)`
        while reservations_db_handle[0].has_key(reservation_name) :
            name_start_time = name_start_time + 1
            reservation_name = `int(name_start_time)`
    else :
        reservation_name = get_new_db_key(reservations_db_handle)
    new_res = initialize_reservation(reservation_name)
    new_res['job_restriction'] = 'result = 1'
    new_res['purpose_type_string'] = 'unknown'
    new_res['earliest_start_float'] = earliest_start_float
    new_res['latest_end_float'] = latest_end_float
    new_res['duration_float'] = duration_float
    new_res['latency_float'] = latency_float
    new_res['resource_amount_int'] = resource_amount_int
    new_res['resource_amount_requested_int'] = resource_amount_int
    new_res['requested_resource_list'] = requested_resource_list
    new_res['max_resource_int'] = max_resource_int
    new_res['purpose_type_string'] = purpose_type_string
    new_res['comment_string'] = comment_string
    new_res['account_string'] = account_string
    new_res['creator_string'] = creator_string
    new_res['conflict_policy'] = conflict_policy
    new_res['node_restriction'] = node_restriction
    new_res['node_sort_policy'] = node_sort_policy
    new_res['node_usage'] = node_usage
    new_res['affinity_calculation'] = affinity_calculation
    new_res['job_restriction'] = job_restriction
    new_res['job_runID'] = job_runID
    new_res['notify_string'] = notify_string
    new_res['start_count_int'] = 0

    # dummy values, so that we can re-enter this function and
    # not overwrite an outer loop reservation_name.  insert
    # will actually create a placeholder reservation, job_rest
    # should be correct.
    #if re.match(r"^temp_pushback_for", purpose_type_string) :
    new_res['start_time_float'] = earliest_start_float
    new_res['end_time_float'] = latest_end_float
    new_res['node_list'] = []
    new_res['allocated_dict_list'] = []
    insert_new_object_with_key(reservation_name, new_res, reservations_db_handle
)


    if accepted_nodes_list == None :
        accepted_nodes_list = get_accepted_nodes_list(node_restriction, resources_db_handle)
        #print "accepted_nodes_list (%s)" % (accepted_nodes_list,)
    if resource_dict_list == None :
        if purpose_type_string in ['job', 'preempted_job'] :
            resource_dict_list = job_step['resource_dict_list']
        else :
            resource_dict_list = [{
              'amount_int' : resource_amount_int,
              'resource_dict' : {}
              }]
            for accepted_node in accepted_nodes_list :
                #resource_dict_list[0]['resource_dict'][accepted_node] = resources_db_handle[0][accepted_node]
                resource_dict_list[0]['resource_dict'][accepted_node] = {}
    new_res['requested_resource_list'] = requested_resource_list
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "len(accepted_nodes_list) (%s)" % len(accepted_nodes_list)
    #print "len(accepted_nodes_list) (%s)" % len(accepted_nodes_list)
    # set jobs_dict
    if jobs_db_handle != None :
        jobs_dict = jobs_db_handle[0]
    else :
        raise "NoJobsDict"
    #if purpose_type_string in ['job','preempted_job'] :
    if purpose_type_string in ['job', 'temp_pushback'] or re.match(r"^temp_pushback_for", purpose_type_string) :
        #if purpose_type_string in ['preempted_job',] :
        #    print "doing preempted_job (%s)" % job_runID
        #print "doing job (%s)" % job_runID
        if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
            print "doing job (%s)" % job_runID
        # filter accepted_nodes_list with resource_dict_list[0]['resource_dict']
        old_accepted_nodes_list = copy.copy(accepted_nodes_list)
        accepted_nodes_list = filter(lambda x,resource_dict_list=resource_dict_list : resource_dict_list[0]['resource_dict'].has_key(x), old_accepted_nodes_list)
        # If the job is bound to a list of reservations, meaning
        # it is only supposed to run in that list, filter
        # accepted nodes list with that set of reservations
        # How should jobs be bound to cpu and memory?  You don't
        # want jobs running on the other cpus or memory of a node...
        # A reservation can request just nodes, or nodes with cpus/memory
        # A job bound to the reservation should be bound to either just
        # the node or to the node and the requested cpus/memory...
        if job_step['reservation_binding'] != None \
          and job_step['reservation_binding'] != [] :
            bound_nodes_list = []
            bound_nodes_dict = {}
            bound_reservations_list = []
            for reservation_name in job_step['reservation_binding'] :
                try :
                    bound_reservation = get_object(reservation_name, reservations_db_handle)
                except :
                    continue
                else :
                    bound_reservations_list.append(bound_reservation)
            for reservation in bound_reservations_list :
                bound_res_node_list = reservation['node_list']
                if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
                    print "for reservation (%s) bound_res_node_list (%s)" % (reservation['name'], bound_res_node_list)
                # what if reservation has no allocated_dict_list?
                if reservation.has_key('allocated_dict_list') :
                    allocated_dict_list = reservation['allocated_dict_list']
                else :
                    allocated_dict_list = []
                    for nodename in reservation['node_list'] :
                        allocated_resource = {'nodename' : nodename,
                                              'type' : 'node_exclusive',
                                              'cpu' : resources_db_handle[0][nodename]['ConsumableCpus'],
                                              'memory' : resources_db_handle[0][nodename]['ConsumableMemory']
                                              }
                        allocated_dict_list.append(allocated_resource)
                for allocated_resource in allocated_dict_list :
                    nodename = allocated_resource['nodename']
                    if bound_nodes_dict.has_key(nodename) :
                        bound_nodes_dict[nodename]['cpu'] = \
                          bound_nodes_dict[nodename]['cpu'] + \
                            allocated_resource['cpu']
                        bound_nodes_dict[nodename]['memory'] = \
                          bound_nodes_dict[nodename]['memory'] + \
                            allocated_resource['memory']
                    else :
                        bound_nodes_dict[nodename] = {}
                        bound_nodes_dict[nodename]['cpu'] = \
                            allocated_resource['cpu']
                        bound_nodes_dict[nodename]['memory'] = \
                            allocated_resource['memory']
                for node_name in bound_res_node_list :
                    if node_name not in bound_nodes_list :
                        bound_nodes_list.append(node_name)
                # Do I need to also assign cpu and memory here?
                # Yes, bound_nodes_list needs to be bound_nodes_dict
                # with associated cpus and memory.  Also need to allow duplicate
                # nodes to allow sums of cpus and memory...
            if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
                print "bound_nodes_list (%s)" % (bound_nodes_list,)
            #print "bound_nodes_list (%s)" % (bound_nodes_list,)
            accepted_nodes_list = filter(lambda node_name, bound_nodes_list=bound_nodes_list : node_name in bound_nodes_list, accepted_nodes_list)
            if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
                print "reservation_binding (%s)" % job_step['reservation_binding']
                print "after filter, len(accepted_nodes_list) (%s)" % len(accepted_nodes_list)
            # Create reservations to block off times on the bound nodes
            # to prevent the job from running at the wrong time
            temp_blocking_reservations_list = []
            new_blocking_res_number = 0
            blocking_node_list = []
            side_allocated_dict_list = []
            prepost_allocated_dict_list = []
            for nodename in bound_nodes_dict.keys() :
                # For each relevant node, create two reservations,
                # one to sit before and one to sit after all bound
                # reservations on the node.
                # for cpu-memory scheduling, also need to create
                # a side reservation for unbound cpu/memory on the node
                # This should be one reservation, with a node_list and
                # allocated_dict_list
                #pre_blocking_res['node_list'] = [node_name,]
                blocking_node_list.append(nodename)
                side_allocated_dict_list.append(
                  { 'nodename' : nodename,
                    'type' : 'node_shared',
                    'cpu' : resources_db_handle[0][nodename]['ConsumableCpus'] - bound_nodes_dict[nodename]['cpu'],
                    'memory' : resources_db_handle[0][nodename]['ConsumableMemory'] - bound_nodes_dict[nodename]['memory']
                  } )
                # create allocated_dict_list for pre-blocking-res
                prepost_allocated_dict_list.append(
                  { 'nodename' : nodename,
                    'type' : 'node_exclusive',
                    'cpu' : resources_db_handle[0][nodename]['ConsumableCpus'],
                    'memory' : resources_db_handle[0][nodename]['ConsumableMemory']
                  } )
            new_blocking_res_number = new_blocking_res_number + 1
            pre_blocking_res_name = 'tempblock' + "%s" % new_blocking_res_number
            pre_blocking_res = initialize_reservation(pre_blocking_res_name)
            pre_blocking_res['start_time_float'] = 0.0
            pre_blocking_res['end_time_float'] = END_OF_SCHEDULING
            pre_blocking_res['allocated_dict_list'] = prepost_allocated_dict_list
            pre_blocking_res['node_list'] = blocking_node_list
            # create allocated_dict_list for post-blocking-res
            new_blocking_res_number = new_blocking_res_number + 1
            post_blocking_res_name = 'tempblock' + "%s" % new_blocking_res_number
            post_blocking_res = initialize_reservation(post_blocking_res_name)
            post_blocking_res['node_list'] = blocking_node_list
            post_blocking_res['start_time_float'] = Now_float
            post_blocking_res['end_time_float'] = END_OF_SCHEDULING
            post_blocking_res['allocated_dict_list'] = prepost_allocated_dict_list
            post_blocking_res['node_list'] = blocking_node_list
            # Do I need to create a side_blocking_res for unreserved cpus
            # and memory?  Shouldn't really need to, since cpu and
            # memory are tracked by count rather than named entity...
            for reservation in bound_reservations_list :
                # For each bound reservation, adjust the pre res
                # end time and the post res start time to reflect
                # the bound res start and end times, if they extend
                # the times.
                if reservation['start_time_float'] < pre_blocking_res['end_time_float'] :
                    pre_blocking_res['end_time_float'] = reservation['start_time_float']
                if reservation['end_time_float'] > post_blocking_res['start_time_float'] :
                    post_blocking_res['start_time_float'] = reservation['end_time_float']
            # instead of placing these kludgy temp blocking reservations,
            # can I just set earliest_start and latest_end to the start and
            # end of the blocking reservations?  This probably does not
            # handle L-shaped reservation sets properly.
            temp_blocking_reservations_list.append(pre_blocking_res)
            temp_blocking_reservations_list.append(post_blocking_res)
            if DEBUGJOB != None and job_runID != None and job_runID == DEBUGJOB :
                print "blocking reservations:"
                for blocking_reservation in temp_blocking_reservations_list :
                    print "blocking_reservation (%s, %s, %s)" % (time.asctime(time.localtime(blocking_reservation['start_time_float'])), time.asctime(time.localtime(blocking_reservation['end_time_float'])), blocking_reservation['node_list'])
        #screened_nodes = get_screened_nodes(
        #    job_step['name'], accepted_nodes_list, jobs_db_handle,
        #    resources_db_handle)
        #if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        #    print "len(screened_nodes) (%s)" % len(screened_nodes)
        # If reservation_binding is specified, use only those open_windows_list
        # appropriate for the listed reservations
        # Need to check here that job meets reservation['job_restriction'] and
        # that there are no conflicts with other jobs bound to the same reservation
        # build list of nodes with "pushback" or "preempted_job" reservations
        preempted_nodes_list = []
        for reservation in existing_reservations :
            if reservation['purpose_type_string'] in ['preempted_job', 'pushback'] :
               for node_name in reservation['node_list'] :
                   if not node_name in preempted_nodes_list :
                       preempted_nodes_list.append(node_name)
        blocking_reservations = []
        for reservation in existing_reservations :
            input_tuple = ( job_step, )
            result = apply_policy_code(reservation['job_restriction'],
                input_tuple)
            if result != 0 :
                if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
                    print "appending blocking reservation (%s)" % reservation['name']
                if job_step.has_key('run_at_risk_int') and \
                  job_step['run_at_risk_int'] >= 1 :
                    blocking_reservations.append(reservation)
                else :
                    #print "checking blocking reservation (%s) for non-run_at_risk job (%s)" % (reservation['name'], job_step['name'])
                    # why was there an instance of a job having a
                    # reservation, but not existing in the jobs dict?
                    try :
                        if reservation.has_key('job_runID') and \
                          reservation['job_runID'] != None and \
                          reservation['job_runID'] != job_step['name'] and \
                          ((jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                          jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1) or \
                          (job_step.has_key('preempting') and \
                           job_step['preempting'] >= 1 and \
                          jobs_dict[reservation['job_runID']].has_key('preemptible') and \
                          jobs_dict[reservation['job_runID']]['preemptible'] >= 1)) :
                            #print "checking run_at_risk/preemptible reservation (%s) with job (%s) for non-run_at_risk job (%s)" % (reservation['name'], reservation['job_runID'], job_step['name'])
                            #temp_res = copy.deepcopy(reservation)
                            temp_res = initialize_reservation(reservation['name'] + '.temp')
                            temp_res['node_list'] = copy.deepcopy(reservation['node_list'])
                            temp_res['allocated_dict_list'] = copy.deepcopy(reservation['allocated_dict_list'])
                            temp_res['start_time_float'] = copy.copy(reservation['start_time_float'])
                            temp_res['end_time_float'] = copy.copy(reservation['end_time_float'])
                            existing_preempted_job = 0
                            if reservation['purpose_type_string'] == 'running' or \
                              (reservation['purpose_type_string'] in ['job'] and \
                              jobs_dict[reservation['job_runID']].has_key('start_count_int') and \
                              jobs_dict[reservation['job_runID']]['start_count_int'] != None and \
                              jobs_dict[reservation['job_runID']]['start_count_int'] >= 1) \
                              :
                                for node_name in reservation['node_list'] :
                                    if node_name in preempted_nodes_list :
                                        #print "node_name (%s) in preempted_nodes_list (%s)" % (node_name, preempted_nodes_list)
                                        existing_preempted_job = 1
                            if ((reservation['purpose_type_string'] in ['running', ] or \
                              (reservation['purpose_type_string'] == 'job' and \
                              jobs_dict.has_key(reservation['job_runID']) and \
                              jobs_dict[reservation['job_runID']].has_key('start_count_int') and \
                              jobs_dict[reservation['job_runID']]['start_count_int'] != None and \
                              jobs_dict[reservation['job_runID']]['start_count_int'] >= 1)) and \
                              existing_preempted_job == 0) or \
                              reservation['purpose_type_string'] in ['preempted_job', 'pushback'] :
                                #print "checking running/job reservation (%s) for non-run_at_risk job (%s)" % (reservation['name'], job_step['name'])
                                if job_step.has_key('preempting') and job_step['preempting'] >= 1 and jobs_dict[reservation['job_runID']].has_key('preemptible') and jobs_dict[reservation['job_runID']]['preemptible'] >= 1 :
                                    if job_step['priority'] > jobs_dict[reservation['job_runID']]['priority'] \
                                      and reservation['end_time_float'] - Now_float > PREEMPT_MIN_RUNTIME :
                                        #print "checking preemptible reservation (%s) for non-run_at_risk job (%s)" % (reservation['name'], job_step['name'])
                                        # additional check for preemptible jobs.
                                        # if new_res is for a preempting job,
                                        # check to see if wallclock limit would
                                        # push back past max_pushback_float.
                                        # if it can fit, truncate the reservation.
                                        # if the nodes in the pushed back reservation
                                        # get used for a job, will subsequent jobs
                                        # be scheduled after truncation?  Do I
                                        # need to create the pushed back reservation?
                                        # does this change with other job reservations?
                                        # yes, I need to create the pushed-back
                                        # reservation, so that new jobs don't get
                                        # filled in.  This also means that max
                                        # pushback changes with each new job.
                                        # if the job reservation for a preemptible
                                        # job can be pushed back, then truncate it,
                                        # otherwise leave it full length.
                                        # Could do this by invoking create_reservation
                                        # to check if a reservation is possible
                                        # in the "pushed back" time, but that might
                                        # be expensive.  Perhaps it would be just
                                        # as good to check for any non-job reservations
                                        # on the same nodes, at the same time, that
                                        # would push nodes, cpus, memory over the
                                        # configured values.  To do that for each
                                        # reservation would be almost as much time
                                        # and a lot more code.
                                        #print "trying to create pushback_res of (%s) (%s) for (%s)" % (reservation['job_runID'], reservation['purpose_type_string'], job_step['name'])
                                        #print "before try"
                                        try :
                                            #print "inside try"
                                            job_restriction = "if input_tuple[0]['name'] in ['%s','%s'] : result = 0" % (jobs_dict[reservation['job_runID']]['name'], new_res['job_runID'])
                                            pushback_node_restriction = "if input_tuple[0]['name'] in %s : result = 0" % (reservation['node_list'], )
                                            pushback_duration_float=jobs_dict[reservation['job_runID']]['wall_clock_limit'] - jobs_dict[reservation['job_runID']]['wall_clock_used'] + FUDGE_FACTOR
                                            if reservation['purpose_type_string'] in ['preempted_job', 'pushback'] :
                                                pushback_earliest_start_float=reservation['start_time_float'] + new_res['duration_float']
                                                pushback_latest_end_float=copy.copy(reservation['start_time_float'] + new_res['duration_float'] + jobs_dict[reservation['job_runID']]['wall_clock_limit'] - jobs_dict[reservation['job_runID']]['wall_clock_used'] + 1 + FUDGE_FACTOR)
                                            else :
                                                pushback_earliest_start_float=Now_float + new_res['duration_float']
                                                pushback_latest_end_float=copy.copy(Now_float + new_res['duration_float'] + jobs_dict[reservation['job_runID']]['wall_clock_limit'] - jobs_dict[reservation['job_runID']]['wall_clock_used'] + 1 + FUDGE_FACTOR)
                                            pushback_res = create_reservation(
                                              resources_db_handle=resources_db_handle,
                                              reservations_db_handle=reservations_db_handle,
                                              jobs_db_handle=jobs_db_handle,
                                              job_step=jobs_db_handle[0][reservation['job_runID']],
                                              mode='real',
                                              job_restriction=job_restriction,
                                              node_restriction=pushback_node_restriction,
                                              job_runID=reservation['job_runID'],
                                              ignore_reservations_list=[reservation['name']],
                                              purpose_type_string="temp_pushback_for_%s" % new_res['job_runID'],
                                              earliest_start_float=pushback_earliest_start_float,
                                              duration_float=pushback_duration_float,
                                              latest_end_float=pushback_latest_end_float,
                                              resource_amount_int = len(jobs_dict[reservation['job_runID']]['requested_resource_list']),
                                              requested_resource_list = jobs_dict[reservation['job_runID']]['requested_resource_list']
                                            )
                                            #print "after create_reservation (%s) (%s)" % (reservation['job_runID'], pushback_res['name'])
                                            #print "latest_end_float (%s)" % time.asctime(time.localtime(latest_end_float))
                                        except 'InsufficientNodes', pushback_res :
                                            # pushback reservation failed, so don't
                                            # try to preempt this one
                                            if DEBUGJOB != None :
                                                print "pushback of preemptible job (%s) not possible due to reservation (%s) blocking!" % (pushback_res['job_runID'], reservation['name'])
                                            #print "pushback of preemptible job (%s) for (%s) not possible due to reservation (%s) blocking!" % (reservation['job_runID'], new_res['job_runID'], reservation['name'])
                                            blocking_reservations.append(reservation)
                                        except :
                                            #print "some other pushback res exception occurred"
                                            info_tuple = sys.exc_info()
                                            info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                                            traceback.print_tb(info_tuple[2])
                                            tb_list = traceback.format_tb(info_tuple[2])
                                            info_list = info_list + tb_list
                                            tb_text = string.join(info_list)
                                            message = tb_text
                                            print "message (%s)" % message

                                            blocking_reservations.append(reservation)
                                        else :
                                            # in get_chosen, if any of these
                                            # reservations nodes are chosen,
                                            # then a real pushback_res needs
                                            # to be made, to block any new job
                                            # reservations.
                                            # pushback res for duration of
                                            # both preempting and preemptible
                                            # jobs successful.  mode was real,
                                            # so this should be in the db.
                                            # do not append to blocking res
                                            # list.
                                            #print "pushback of preemptible job (%s) succeeded for reservation (%s) pushback_res['name'] (%s) !" % (pushback_res['job_runID'], reservation['name'], pushback_res['name'])
                                            #blocking_reservations.append(pushback_res)
                                            temp_res['start_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                                            temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                                            # seemed to work without this...
                                            blocking_reservations.append(pushback_res)
                                        #print "after try of pushback of (%s) (%s) (%s) res" % (reservation['purpose_type_string'], reservation['job_runID'], reservation['name'])
                                    else :
                                        #print "priority not overriding or PREEMPT_MIN_RUNTIME too small"
                                        blocking_reservations.append(reservation)
                                else :
                                    #print "not preemptible reservation (%s) for non-run_at_risk job (%s)" % (reservation['name'], job_step['name'])
                                    #if job_step['preempting'] >= 1 and jobs_dict[reservation['job_runID']].has_key('preemptible') and jobs_dict[reservation['job_runID']]['preemptible'] >= 1 :
                                    #if job_step['preempting'] >= 1 :
                                    #    print "job_step['preempting'] >= 1"
                                    #if jobs_dict[reservation['job_runID']].has_key('preemptible') :
                                    #    print "jobs_dict[reservation['job_runID']].has_key('preemptible')"
                                    #if jobs_dict[reservation['job_runID']]['preemptible'] >= 1 :
                                    #    print "jobs_dict[reservation['job_runID']]['preemptible'] >= 1"
                                    #print "job_step['preempting'] (%s), job_step['priority'] (%s), jobs_dict[reservation['job_runID']]['priority'] (%s), jobs_dict[reservation['job_runID']]['preemptible']" % (job_step['preempting'], job_step['priority'], jobs_dict[reservation['job_runID']]['priority'], jobs_dict[reservation['job_runID']]['preemptible'])
                                    #blocking_reservations.append(reservation)
                                    temp_res['start_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                                    temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                                #print "before goofy"
                            else :
                                # not getting here, not appending nonrunning
                                # reservations...
                                # This is goofy.  This will create an
                                # end_time_float before the start time.
                                # Did I do this for a reason?  It might
                                # be this way to choose run_at_risk occupied
                                # windows to be chosen last via last_available
                                #print "failed check for running/job reservation (%s) for non-run_at_risk job (%s)" % (reservation['name'], job_step['name'])
                                if jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                                  jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1 and \
                                  existing_preempted_job == 0 :
                                    temp_res['start_time_float'] = Now_float
                                    temp_res['end_time_float'] = Now_float
                                #else :
                                #    temp_res['start_time_float'] = Now_float
                                #    temp_res['end_time_float'] = Now_float
                            #print "appending temp_res (%s) (%s) (%s)" % (temp_res['name'], temp_res['purpose_type_string'], temp_res['job_runID'])
                            blocking_reservations.append(temp_res)
                        else :
                            #print "appending unmodified reservation (%s)" % reservation['name']
                            blocking_reservations.append(reservation)
                        #print "before except"
                    except :
                        #print "in exception after 'before except'"
                        info_tuple = sys.exc_info()
                        info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                        traceback.print_tb(info_tuple[2])
                        tb_list = traceback.format_tb(info_tuple[2])
                        info_list = info_list + tb_list
                        tb_text = string.join(info_list)
                        message = tb_text
                        print "message (%s)" % message
                        #if not jobs_dict.has_key(reservation['job_runID']) :
                        #    print "job (%s) has reservation (%s), but does exist in jobs_dict!" % (reservation['job_runID'], reservation['name'])
                    #print "len(blocking_reservations) (%s)" % len(blocking_reservations)
                    #print "after except"
        if job_step['reservation_binding'] != None \
          and job_step['reservation_binding'] != [] :
            blocking_reservations = blocking_reservations + temp_blocking_reservations_list + blocking_reservations_list
        #if purpose_type_string in ['preempted_job',] :
        #    print "blocking_reservations (%s)" % (blocking_reservations_list,)

        open_windows_list = get_open_windows_list(accepted_nodes_list, new_res, blocking_reservations, resources_db_handle)
        if job_step.has_key('resource_dict_list') :
            resource_dict_list = job_step['resource_dict_list']
        else :
            raise NoResourceDictList
        if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
            print "checking open_windows_list before passing off to get_sized_windows"
            for new_window in open_windows_list :
                print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
                  (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])
    # purpose_type_string != 'job', so use accepted_nodes_list to find open_windows_list
    else :
        # If this is not a job reservation, filter out all the job
        # reservations, so they don't block this one.  There may be
        # overlapping reservations until the job reservations get updated.
        # Leave blocking job reservations with startcount > 0
        if new_res['purpose_type_string'] != 'user_set' :
            non_job_reservations = filter(
              lambda reservation : reservation['purpose_type_string'] \
              != 'job' or reservation['start_count_int'] > 0, existing_reservations)
            # create a dictionary of reservations by node
            #reservations_by_node = {}
            #for reservation in non_job_reservations :
            #    for node in reservation
            truncated_reservations = []
            for reservation in non_job_reservations :
                temp_res = copy.deepcopy(reservation)
                if reservation['purpose_type_string'] in ['pushback', 'preempted_job'] :
                    temp_res['start_time_float'] = Now_float
                if reservation['purpose_type_string'] == 'running' and \
                  jobs_dict.has_key(reservation['job_runID']) and \
                  (jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                  jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1) :
                    #temp_res = copy.deepcopy(reservation)
                    if reservation['purpose_type_string'] == 'running' or \
                      (reservation['purpose_type_string'] == 'job' and \
                      jobs_dict[reservation['job_runID']]['start_count_int'] != None and \
                      jobs_dict[reservation['job_runID']]['start_count_int'] >= 1) :
                        # need to check here for sufficient time before
                        # start of the next res...
                        # actually, did this earlier in the pushback section
                        temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                    else :
                        temp_res['end_time_float'] = Now_float
                    #truncated_reservations.append(temp_res)
                else :
                    pass
                    #truncated_reservations.append(reservation)
                truncated_reservations.append(temp_res)
            existing_reservations = truncated_reservations
        if new_res['purpose_type_string'] == 'preempted_job' :
            old_accepted_nodes_list = copy.copy(accepted_nodes_list)
            accepted_nodes_list = filter(lambda x,resource_dict_list=resource_dict_list : resource_dict_list[0]['resource_dict'].has_key(x), old_accepted_nodes_list)
        existing_reservations = existing_reservations + blocking_reservations_list
        #if purpose_type_string in ['preempted_job',] :
        #    for existing_reservation in existing_reservations :
        #        print "existing_reservation (%s)" % (existing_reservation['name'],)
        open_windows_list = get_open_windows_list(
          accepted_nodes_list, new_res, existing_reservations, resources_db_handle)
        #print "open_windows_list (%s)" % (open_windows_list,)
        resource_dict_list = None
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "len(open_windows_list) (%s)" % len(open_windows_list)
    big_windows_list = get_big_windows_list(open_windows_list, new_res)
    #if purpose_type_string in ['preempted_job',] :
    #    print "len(open_windows_list) (%s)" % len(open_windows_list)
    #    for window in open_windows_list :
    #        print "window[0] (%s) window[1] (%s) window[2] (%s)" % (time.asctime(time.localtime(window[0])), time.asctime(time.localtime(window[1])), window[2])
    #print "len(open_windows_list) (%s)" % len(open_windows_list)
    #print "requested_resource_list (%s)" % (requested_resource_list)
    (sized_windows_list, current_start_time_float, counting_dict_list, node_bin_dict) = get_sized_windows_list(
      new_res, big_windows_list, resources_db_handle, jobs_db_handle, resource_dict_list=resource_dict_list, requested_resource_list = requested_resource_list)
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "final len(sized_windows_list) (%s)" % len(sized_windows_list)
    #print "len(sized_windows_list) (%s)" % len(sized_windows_list)
    new_res['start_time_float'] = current_start_time_float
    relevant_nodes_list = []
    for window in sized_windows_list :
        relevant_nodes_list.append(window[2]['nodename'])
    active_reservations = []
    for reservation in existing_reservations :
        if (reservation['start_time_float'] <= current_start_time_float < reservation['end_time_float']) or (reservation['start_time_float'] < (current_start_time_float + duration_float) <= reservation['end_time_float']) :
            #if job_step.has_key('run_at_risk_int') and \
            #  job_step['run_at_risk_int'] < 1 and \
            if reservation.has_key('job_runID') and \
              reservation['job_runID'] != None and \
              jobs_dict.has_key(reservation['job_runID']) and \
              ((jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
              jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1) or \
              (jobs_dict[reservation['job_runID']].has_key('preemptible') and \
              jobs_dict[reservation['job_runID']]['preemptible'] >= 1)) and \
              ( reservation['purpose_type_string'] == 'running' or \
              ( reservation['purpose_type_string'] == 'job' and \
              reservation['start_count_int'] > 0 )) :
                #temp_res = copy.deepcopy(reservation)
                #temp_res['affinity_calculation'] = "result = %s" % (- jobs_dict[reservation['job_runID']]['priority'],)
                #temp_res['job_restriction'] = 'result = 0'
                reservation['affinity_calculation'] = "result = %s" % (- jobs_dict[reservation['job_runID']]['priority'],)
                #active_reservations.append(temp_res)
                active_reservations.append(reservation)
            else :
                active_reservations.append(reservation)
            if reservation['purpose_type_string'] not in ('job', 'running') \
              and (reservation['start_time_float'] <= current_start_time_float \
              < reservation['end_time_float'] or \
              reservation['start_time_float'] < current_start_time_float + duration_float <= reservation['end_time_float']) :
                active_reservations.append(reservation)
    windows_and_reservations_list = []
    if not (purpose_type_string in ['job', 'preempted_job', 'temp_pushback'] or re.match(r"^temp_pushback_for", purpose_type_string) ):
        job_step = initialize_job_step('temp_job_step')
    for window in sized_windows_list :
        relevant_reservations_list = []
        for reservation in active_reservations :
            if window[2]['nodename'] in reservation['node_list'] :
                relevant_reservations_list.append(reservation)
        windows_and_reservations_list.append((window, relevant_reservations_list, job_step))
    sorted_windows_list = get_sorted_windows_list( sized_windows_list, new_res,
      accepted_nodes_list, node_sort_policy, resources_db_handle, reservations_db_handle, jobs_db_handle, windows_and_reservations_list )
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "len(sorted_windows_list) (%s)" % len(sorted_windows_list)
    #print "len(sorted_windows_list) (%s)" % len(sorted_windows_list)
    chosen_nodes_list, allocated_dict_list = get_chosen_nodes_list(new_res, sorted_windows_list, counting_dict_list, node_bin_dict, resources_db_handle )
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "after get_chosen"
    #print "len(chosen_nodes_list) (%s)" % len(chosen_nodes_list)
    #print "allocated_dict_list before (%s)" % (allocated_dict_list,)
    #if purpose_type_string == 'job' :
    #    for allocated_dict in allocated_dict_list :
    #        if allocated_dict.has_key('nodename') :
    #            if not allocated_dict.has_key('type') :
    #                if job_step.has_key('node_usage') and job_step['node_usage'] == 'SHARED' :
    #                    allocated_dict['type'] = 'node_shared'
    #                else :
    #                    allocated_dict['type'] = 'node_exclusive'
    #                    if allocated_dict.has_key('cpu') :
    #                        allocated_dict['cpu'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableCpus']
    #                    if allocated_dict.has_key('memory') :
    #                        allocated_dict['memory'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableMemory']
    #else :
    #    for allocated_dict in allocated_dict_list :
    #        if allocated_dict.has_key('nodename') :
    #            if not allocated_dict.has_key('type') :
    #                allocated_dict['type'] = 'node_exclusive'
    #                if allocated_dict.has_key('cpu') :
    #                    allocated_dict['cpu'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableCpus']
    #                if allocated_dict.has_key('memory') :
    #                    allocated_dict['memory'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableMemory']
    #print "allocated_dict_list after (%s)" % (allocated_dict_list,)
    delete_object(new_res['name'], reservations_db_handle)
    if resource_amount_int != None and len(chosen_nodes_list) < resource_amount_int or len(chosen_nodes_list) == 0 :
        if DEBUGJOB != None and job_runID != None and job_runID == DEBUGJOB :
            print "len(chosen_nodes_list) (%s)" % len(chosen_nodes_list)
            print "resource_amount_int (%s)" % resource_amount_int
        if new_res.has_key('job_runID') and new_res['job_runID'] != None :
            # remove all "temp_pushback" reservations
            #print "didn't get all nodes for job (%s) res, removing all temp_pushback reservations" % (new_res['job_runID'],)
            pushback_res_list = []
            for res_key in reservations_db_handle[0] :
                if reservations_db_handle[0][res_key]['purpose_type_string'] == "temp_pushback_for_%s" % new_res['job_runID'] :
                    pushback_res_list.append(res_key)
                    #delete_object(res_key, reservations_db_handle)
            for res_key in pushback_res_list :
                #print "deleting temp_pushback reservations (%s)" % res_key
                delete_object(res_key, reservations_db_handle)
        raise 'InsufficientNodes', new_res
    else :
        node_list = []
        for allocated_dict in allocated_dict_list :
            if new_res['node_usage'] == 'node_exclusive' :
                allocated_dict['node'] = 1
                if resources_db_handle[0].has_key(allocated_dict['nodename']) :
                    if resources_db_handle[0][allocated_dict['nodename']].has_key('Cpus') :
                        allocated_dict['pcpus'] = resources_db_handle[0][allocated_dict['nodename']]['Cpus']
                        allocated_dict['cpu'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableCpus']
                    if resources_db_handle[0][allocated_dict['nodename']].has_key('Memory') :
                        allocated_dict['pmemory'] = resources_db_handle[0][allocated_dict['nodename']]['Memory']
                        allocated_dict['memory'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableMemory']
            node_list.append(allocated_dict['nodename'])
        new_res['node_list'] = node_list
        new_res['end_time_float'] = current_start_time_float + duration_float
        if new_res.has_key('job_runID') and new_res['job_runID'] != None :
        #if new_res['purpose_type_string'] != "temp_pushback" :
            # remove any "pushback" reservations that don't overlap
            # the new reservation
            # if the job can't be immediately scheduled, the temp_pushback
            # reservations may interfere with a later scheduling slot.
            # will probably have to do a second non-pushback scheduling
            # run for the job, to see if it could be scheduled earlier.
            # should ignore the temp_pushback reservations, truncate the
            # running, then convert to persistent pushback, if it
            # interferes with the new reservation.
            #print "node_list (%s)" % (node_list,)
            pushback_del_list = []
            pushback_keep_list = []
            for res_key in reservations_db_handle[0].keys() :
                #if reservations_db_handle[0][res_key]['purpose_type_string'] == "temp_pushback" :
                if reservations_db_handle[0][res_key]['purpose_type_string'] == "temp_pushback_for_%s" % new_res['job_runID'] :
                    #pushback_res_list.append(res_key)
                    #print "checking temp_pushback (%s)" % res_key
                    # find the end time of the running reservation for the
                    # pushback res
                    # actually, need to do this for 'job' with start of 1,
                    # 'pushback' and 'preempted_job' reservations
                    overlap_found = 0
                    running_end_time = 0
                    running_start_time = END_OF_SCHEDULING
                    for running_res_key in reservations_db_handle[0].keys() :
                        if (
                          reservations_db_handle[0][running_res_key]['purpose_type_string'] in ['running', 'pushback', 'preempted_job'] \
                          or reservations_db_handle[0][running_res_key]['purpose_type_string'] == 'job' and reservations_db_handle[0][running_res_key]['start_count_int'] >= 1) \
                          and reservations_db_handle[0][running_res_key]['job_runID'] == reservations_db_handle[0][res_key]['job_runID'] :
                            running_end_time = reservations_db_handle[0][running_res_key]['end_time_float']
                            running_start_time = reservations_db_handle[0][running_res_key]['start_time_float']
                    if new_res['start_time_float'] < running_end_time and \
                       running_start_time < new_res['end_time_float'] :
                        # there is overlap
                        overlap_found = 1
                        #print "overlap_found for (%s)" % res_key
                        #update_object_attribute('purpose_type_string', 'pushback', reservations_db_handle[0][res_key], reservations_db_handle)
                    else :
                        pass
                        #print "no overlap for (%s)" % res_key
                        #pushback_list.append(res_key)
                        #continue
                        # no overlap
                    node_matched = 0
                    for allocated_dict in reservations_db_handle[0][res_key]['allocated_dict_list'] :
                        #print "job_runID (%s) allocated_dict (%s)" % (reservations_db_handle[0][res_key]['job_runID'], allocated_dict)
                        if allocated_dict['nodename'] in node_list :
                            #print "node_matched for (%s)" % res_key
                            node_matched = 1
                    if overlap_found == 0 or node_matched == 0 :
                        if not res_key in pushback_del_list :
                            #print "no overlap_found or no node_matched for (%s)" % res_key
                            pushback_del_list.append(res_key)
                        #delete_object(res_key, reservations_db_handle)
                    else :
                        #print "overlap_found and node_matched for (%s)" % res_key
                        update_object_attribute('purpose_type_string', 'pushback', reservations_db_handle[0][res_key], reservations_db_handle)
                        if not res_key in pushback_keep_list :
                            pushback_keep_list.append(res_key)
            for new_key in pushback_keep_list :
                for old_key in reservations_db_handle[0].keys() :
                    if reservations_db_handle[0][old_key]['purpose_type_string'] in ['pushback', 'preempted_job'] \
                      and old_key != new_key \
                      and reservations_db_handle[0][old_key]['start_time_float'] <= reservations_db_handle[0][new_key]['start_time_float'] \
                      and reservations_db_handle[0][new_key]['job_runID'] == reservations_db_handle[0][old_key]['job_runID'] :
                        if not old_key in pushback_del_list :
                            #print "appending to pushback_del_list old_key (%s) for new_key (%s)" % (old_key, new_key)
                            pushback_del_list.append(old_key)
            for del_key in pushback_del_list :
                #print "deleting pushback (%s) for job (%s) purpose (%s)" % (del_key, reservations_db_handle[0][del_key]['job_runID'], reservations_db_handle[0][del_key]['purpose_type_string'])
                delete_object(del_key, reservations_db_handle)
        #new_res['node_list'] = chosen_nodes_list
        new_res['allocated_dict_list'] = allocated_dict_list
        new_res['end_time_float'] = current_start_time_float + duration_float
        new_res['resource_amount_int'] = len(chosen_nodes_list)
        if DEBUGJOB != None and job_runID != None and job_runID == DEBUGJOB :
            print "allocated_dict_list (%s)" % (allocated_dict_list,)
            print "node_list (%s)" % (node_list,)
            print "chosen_nodes_list (%s)" % (chosen_nodes_list,)
        if mode != 'lookahead' :
            insert_new_object(new_res, reservations_db_handle)
        if old_res_id != None and mode == 'real' :
            delete_object(old_res_id, reservations_db_handle)

        return new_res

def roll_logs(events_db_handle, old_jobs_db_handle, old_reservations_db_handle) :
    # move old jobs, old reservations, events to date-stamped archive files
    # check size of db file, if db file exceeds DBSIZE_LIMIT, the
    # open a datestamped version in ARCHIVE_DIR, dump the db to that,
    # dump an empty dictionary to the production file.
    archive_stat = os.statvfs(ARCHIVE_DIR)
    # Check for 100,000 free blocks
    if archive_stat[statvfs.F_BAVAIL] < 100000 :
        raise 'InsufficientBlocks'
    # Check for 10 free inodes
    if archive_stat[statvfs.F_FAVAIL] != -1 and archive_stat[statvfs.F_FAVAIL] < 100000 :
        raise 'InsufficientINodes'
    db_names_list = (events_db_handle[1][1], old_jobs_db_handle[1][1], old_reservations_db_handle[1][1])
    db_list = (events_db_handle, old_jobs_db_handle, old_reservations_db_handle)
    uid_int = pwd.getpwnam(CAT_LOCK_OWNER)[2]
    gid_int = grp.getgrnam(CAT_LOCK_GROUP)[2]
    for db in db_list :
        db_name = db[1][1]
        file_stat = os.stat(DBDIR + '/' + db_name)
        size_bytes = file_stat[stat.ST_SIZE]
        if size_bytes > DBSIZE_LIMIT :
            archive_file_mode = None
            date_stamp = int(Now_float)
            while archive_file_mode == None :
                archive_file_name = ARCHIVE_DIR + '/' + db_name + '.' + "%s" % date_stamp
                if os.path.exists(archive_file_name) :
                    date_stamp = date_stamp + 1
                    continue
                break
        else :
            continue
        FO = open(archive_file_name, 'wb')
        db_dict = db[0]
        pickle = cPickle.Pickler(FO,1)
        pickle.fast = 1
        pickle.dump(db_dict)
        FO.close()
        os.chmod(archive_file_name,0664)
        os.chown(archive_file_name,uid_int,gid_int)
        for key in db_dict.keys() :
            del db_dict[key]

def log_event(event, events_db_handle) :
    # What should events be?  Dictionary, with 'name' and any
    # other relevant attributes
    event_name = get_new_db_key(events_db_handle)
    insert_new_object_with_key(event_name, event, events_db_handle)

def warn(message, subject, recipient) :
    if SERVERMODE not in ['NORMAL','TEST'] :
        print "warning in testmode"
        print "%s" % recipient
        print "%s" % subject
        print "%s" % message
    else :
        cmd = ECHO + ' ' + "'%s'" % message + ' | ' + MAILX + ' -s ' + '"%s"' % subject + " %s" % recipient
        os.system(cmd)

def catsyslog(message, priority) :
    # Could do this with the syslog module, but this way, the sys admin
    # can insert his/her own logger wrapper.
    if SERVERMODE not in ['NORMAL',] :
        cmd = LOGGER + ' -t CATALINA_DEBUG' + ' -p ' + LOGGER_FACILITY + '.debug ' + "'" + message + "'"
        os.system(cmd)
    else :
        cmd = LOGGER + ' -t CATALINA_NORMAL' + ' -p ' + LOGGER_FACILITY + '.' + priority + ' ' + "'" + message + "'"
        os.system(cmd)
    
####################
# convertToAbbreviatedForm()
# A function to convert nodelist string into abbreviated form. This string is
# passsed via Unix to RUNJOB. 
# previously to run a job, Catalina calls
# rj_LL ds355 888 99 0 ds355 ds355 ds355 ds355 ds355 ds366 ds366
# 
# However, there is a problem where this can be too long. Harkness has
# a job with 4096 tasks across 256 nodes, and it didn't get scheduled. UNIX
# limitation prevents us from passing 32KB worth of argument. Therefore, I 
# propose a compacted / abbreviated form as follows
# rj_LL ds355 888 99 0 ds355#5 ds3466#2
# 
# This is easier to read in show_events, consumed less space, and allows large
# jobs to run. The conversion takes very little time and the list is expanded
# once again in RUNJOB C program.
#
##
#incporporated feature request #560, the task geometry stuff
#Do not lump everything together, but keep the order in tact
#e.g. ds355 ds366 ds366 ds355 ds355 ds355 node list array 
# will be passed in as
# ds355#1 ds366#2 ds355#3
# previously, (the incorrect way) was
# ds355#4 ds366#2

######################
def convertToAbbreviatedForm(input_str):
    #split the input string by whitespace
    input_str = input_str.split(" ")
    ret_str = ""
    last_seen = input_str[0]
    last_seen_count = 0
    
    for word in input_str :
        if word == last_seen :
            last_seen_count +=1
        else :
            #save it to ret_str
            ret_str = ret_str + ' ' + last_seen + '#' + str(last_seen_count)
            #reset last_seen and last_seen_count
            last_seen = word
            last_seen_count=1
    #end of for
    
    #don't forget to add the last element. This is not done in the loop because
    #there is no different item after the last element. The data is still saved
    #in last_seen and last_seen_count
    ret_str = ret_str + ' ' + last_seen + '#' + str(last_seen_count)
    
    return ret_str.lstrip()
    
    
def filter_out_down_nodes(broken_node_list,jobs_db_handle):
    """ A function to take out the down nodes from a job resource_dict_list's 
    resource_dict. Only applies if the job state is not running. When a job is
    already running, removing its entry is too late"""
    
    object_list = get_object_list(jobs_db_handle)
    
    for broken in broken_node_list :
        
        #for every job in database
        for job in range(len(object_list)) :
            
            #if job state is running, it's too late
            if object_list[job]['state'] == 'Running' :
                continue
            
            #for every resource_list in each job 
            for res_list in range(len(object_list[job]['resource_dict_list'])) :
                
                try :
                    del object_list[job]['resource_dict_list'][res_list]['resource_dict'][broken]
                    
                except :
                    continue
    
    
#end of file





@


1.175
log
@convert MACHINE_REFRESH_INTERVAL to int, or failover to 1
@
text
@d2735 1
a2735 1
            if DEBUGJOB != None and risk_res['job_runID'] != None and risk_res['job_runID'] == DEBUGJOB :
d5838 4
@


1.174
log
@set system_queue_time to None for Hold jobs
wait for RESOURCE_DOWN_TIME_LIMIT to set nodes to Down
@
text
@d190 8
@


1.173
log
@move old jobs, if it is not in new list _or_ if it is not Idle Running or
Starting
@
text
@d2665 1
a2665 1
          or not jobs_dict[job_step['name']]['state'] in ['Idle', 'Running', 'Starting'] :
d2725 7
d3192 1
d3201 1
d3209 1
d3213 1
d4790 36
a4825 3
                    if resource['State'] != resource_shelf[resource['name']]['State'] :
                        update_object_attribute('State_Change_Time', Now_float, resource_shelf[resource['name']], resources_db_handle)
                update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
d4836 3
a4838 1
            down_resource_names_dict[resource['name']] = found_resource_dict[resource['name']]['State']
d4842 3
d4846 1
@


1.172
log
@fix ticket 0082788
mmargo
@
text
@d2665 1
a2665 1
          and not jobs_dict[job_step['name']]['state'] in ['Idle', 'Running', 'Starting'] :
@


1.171
log
@New database format.
All subroutine which involves Databases are updated.
mmargo
@
text
@d129 1
d268 1
d316 2
d2797 1
d2835 10
d2915 1
d2928 2
a2929 2
	update_object_attribute('priority', priority, temp_job, jobs_db_handle)
	update_object_attribute('priority_element_dict', priority_element_dict, temp_job, jobs_db_handle)
d3169 4
d3180 3
a3182 3
    # for speed, this only gets the resource list for new jobs...
    # there is the potential for incorrect scheduling when a node
    # comes up.  reload_job_resource_lists should be run periodically...
d3185 11
a3195 2
        #if job_step.has_key('resource_dict_list') and len(job_step['resource_dict_list']) > 0 :
        #    continue
d3199 4
a3202 2
        #if job_step['ineligible_reason'] == 'BADRESOURCELIST' :
        #    continue
d4760 1
a4760 1
def update_resources(resources_db_handle, cfg_resources_db_handle) :
d4794 3
d4916 2
a4917 2
    print "update_resources"
    update_resources(resources_db_handle, cfg_resources_db_handle)
d5871 10
a5884 8
    task_hash = {}
    for word in input_str :
        if task_hash.has_key(word):
            current_count = task_hash[word]
            current_count +=1   #increment count
            task_hash[word] = current_count
        else:
            task_hash[word]=1
d5886 2
d5889 10
a5898 4
    # WORKAROUND
    # sort the hashtables based on their values and output them in
    # ascending order. Else, LoadL doesn't like it
    task_items = sorted(task_hash.items(),key=itemgetter(1),reverse=True)
d5900 4
a5903 2
    for pair in task_items :
        ret_str = ret_str + " " + pair[0] + "#" + str(pair[1])
a5904 1
    #strip leading whitespace
d5907 27
@


1.170
log
@contains Kenneth's fix for leaking reservations
mmargo
@
text
@d568 1
a568 1
    FO = open(FILE, 'w')
d570 5
a574 1
    pickle = cPickle.Pickler(FO)
d588 1
a588 1
    ROFO = open(ROFILE, 'w')
d590 5
a594 1
    pickle = cPickle.Pickler(ROFO)
d608 1
a608 1
            FO = open(REMOTEFILE, 'r')
d668 1
a668 1
            FO = open(ROFILE, 'r')
d672 3
a674 2
            FO = open(REMOTEFILE, 'w')
            pickle = cPickle.Pickler(FO)
d678 1
a678 1
            FO = open(REMOTEFILE, 'r')
d732 1
a732 1
            ROFO = open(ROFILE, 'r')
d763 3
a765 2
        FO = open(REMOTEFILE, 'w')
        pickle = cPickle.Pickler(FO)
d5784 1
a5784 1
        FO = open(archive_file_name, 'w')
d5786 2
a5787 1
        pickle = cPickle.Pickler(FO)
d5867 1
a5867 1
   @


1.169
log
@fix lstrip()
does not accept an empty string
mmargo
@
text
@d68 2
d86 1
d964 9
d5652 1
d5655 1
d5839 8
a5846 2
    for node in task_hash.keys():
        ret_str = ret_str + " " + node + "#" + str(task_hash[node])
d5849 8
a5856 1
    return ret_str.lstrip()@


1.168
log
@I made a mistake. Instead of placing placeholder, I hardcoded the path to Catalina DB DIR and Catalina HOMEDIR.
This is describing the error in version 1.166. Version 1.167 has no other difference than the fix
mmargo
@
text
@d5829 1
a5829 1
    return ret_str.lstrip(" ")@


1.167
log
@Add a new function called convertToAbbreviatedForm()
mmargo
@
text
@d52 16
a67 16
from Catalina_LL import get_job_steps_dict
from Catalina_LL import get_resources_list
from Catalina_LL import get_configured_resources_list
from Catalina_LL import get_resource_dict_list
#from Catalina_LL import get_resource_list
#from Catalina_LL import get_resource_name_list
from Catalina_LL import run_jobs
from Catalina_LL import get_job_step_state
from Catalina_LL import cancel_job
from Catalina_LL import preempt_job
from Catalina_LL import cancel_bad_jobs
from Catalina_LL import initialize_job_step
from Catalina_LL import get_scheduler_time
from Catalina_LL import JOB_UPDATE_ATTRIBUTE_list
from Catalina_LL import JOBSUFFIX
from Catalina_LL import SUBMIT_OUTPUT_PATTERN
d152 2
a153 2
HOMEDIR = '/users/loadl/catalina.20060621'
DBDIR = '/users/loadl/catalina.20060621/db'
d157 2
a158 2
  'HOMEDIR' : '/users/loadl/catalina.20060621',
  'DBDIR' : '/users/loadl/catalina.20060621/db'
@


1.166
log
@add logic to separate out jobs to p655 and p690 depending on their memory req'd
fix by Kenneth
mmargo
@
text
@d52 16
a67 16
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_job_steps_dict
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resources_list
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_configured_resources_list
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resource_dict_list
#from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resource_list
#from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resource_name_list
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import run_jobs
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_job_step_state
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import cancel_job
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import preempt_job
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import cancel_bad_jobs
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import initialize_job_step
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_scheduler_time
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import JOB_UPDATE_ATTRIBUTE_list
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import JOBSUFFIX
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import SUBMIT_OUTPUT_PATTERN
d152 2
a153 2
HOMEDIR = '___HOMEDIR_PLACEHOLDER___'
DBDIR = '___DBDIR_PLACEHOLDER___'
d157 2
a158 2
  'HOMEDIR' : '___HOMEDIR_PLACEHOLDER___',
  'DBDIR' : '___DBDIR_PLACEHOLDER___'
d5796 34
@


1.165
log
@speed up Catalina iteration. non-queued job fifo order
@
text
@a1888 2
                    #print "'node_usage' == 'node_exclusive'"
                    # what if amount_int == None?
a1889 2
                    requested_cpu = None
                    requested_memory = None
d1891 7
a1897 7
                    requested_node = 1
                    for indexb in range(len(dict['req_list'])) :
                        requested_resource = dict['req_list'][indexb]
                        if requested_resource.has_key('cpu') :
                            requested_cpu = requested_cpu + requested_resource['cpu']
                        if requested_resource.has_key('memory') :
                            requested_memory = requested_memory + requested_resource['memory']
d1917 6
a1922 17
                    if dict['node_usage'] == 'node_exclusive' :
                        if resources_db_handle[0][nodename].has_key('ConsumableCpus') :
                            acConsumableCpus = resources_db_handle[0][nodename]['ConsumableCpus']
                        else :
                            acConsumableCpus = 0
                        if resources_db_handle[0][nodename].has_key('ConsumableMemory') :
                            acConsumableMemory = resources_db_handle[0][nodename]['ConsumableMemory']
                        else :
                            acConsumableMemory = 0
                        if acConsumableCpus <= node_bin_dict[nodename]['cpu'] :
                            sufficient_cpu = 1
                        else :
                            sufficient_cpu = 0
                        if acConsumableMemory <= node_bin_dict[nodename]['memory'] :
                            sufficient_memory = 1
                        else :
                            sufficient_memory = 0
d1924 1
a1924 8
                        if requested_cpu <= node_bin_dict[nodename]['cpu'] :
                            sufficient_cpu = 1
                        else :
                            sufficient_cpu = 0
                        if requested_memory <= node_bin_dict[nodename]['memory'] :
                            sufficient_memory = 1
                        else :
                            sufficient_memory = 0
d2009 16
a2024 34
                            if dict['node_usage'] == 'node_exclusive' :
                                #if dict['requested_node'] <= node_bin_dict[window[1]]['node'] and dict['requested_cpu'] <= node_bin_dict[window[1]]['cpu'] and dict['requested_memory'] <= node_bin_dict[window[1]]['memory'] :
                                if dict['requested_node'] <= node_bin_dict[window[1]]['node'] - assigned_node and resources_db_handle[0][window[1]]['ConsumableCpus'] - assigned_cpu <= node_bin_dict[window[1]]['cpu'] and resources_db_handle[0][window[1]]['ConsumableMemory'] <= node_bin_dict[window[1]]['memory'] - assigned_memory :
                                    if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                                        print "appending node (%s)" % window[1]
                                    #print "appending node (%s)" % window[1]
                                    assigned_nodes_list.append(window[1])
                                    dict['saved_windows_list'].append(window)
                                    assigned_node = assigned_node + dict['requested_node']
                                    assigned_cpu = assigned_cpu + node_bin_dict[window[1]]['cpu']
                                    assigned_memory = assigned_memory + node_bin_dict[window[1]]['memory']
                                    continue
                                else :
                                    #print "not appending (%s)" % (window,)
                                    #print "dict['requested_node'] (%s)" % dict['requested_node']
                                    #print "node_bin_dict[window[1]]['node'] (%s)" % node_bin_dict[window[1]]['node']
                                    #print "assigned_node (%s)" % assigned_node
                                    #print "resources_db_handle[0][window[1]]['ConsumableCpus'] (%s)" % resources_db_handle[0][window[1]]['ConsumableCpus']
                                    #print "assigned_cpu (%s)" % assigned_cpu
                                    #print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                    #print "resources_db_handle[0][window[1]]['ConsumableMemory'] (%s)" % resources_db_handle[0][window[1]]['ConsumableMemory']
                                    #print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
                                    #print "assigned_memory (%s)" % assigned_memory
                                    #if dict['requested_node'] <= node_bin_dict[window[1]]['node'] - assigned_node and resources_db_handle[0][window[1]]['ConsumableCpus'] - assigned_cpu <= node_bin_dict[window[1]]['cpu'] and resources_db_handle[0][window[1]]['ConsumableMemory'] <= node_bin_dict[window[1]]['memory'] - assigned_memory :
                                    #print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                    #print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                    #print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                    #print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
                                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "not appending (%s)" % (window,)
                                        print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                        print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                        print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                        print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
d2026 13
a2038 28
                                # The assigned_nodes_list check is to prevent
                                # scheduling on the same node twice for shared
                                # jobs.  I think this should be allowed, but
                                # LoadLeveler, at least, complains...
                                if not window[1] in assigned_nodes_list and dict['requested_node'] <= node_bin_dict[window[1]]['node'] - assigned_node and dict['requested_cpu'] <= node_bin_dict[window[1]]['cpu'] - assigned_cpu and dict['requested_memory'] <= node_bin_dict[window[1]]['memory'] - assigned_memory :
                                    if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                                        print "appending node (%s)" % window[1]
                                    #print "appending node (%s)" % window[1]
                                    assigned_nodes_list.append(window[1])
                                    dict['saved_windows_list'].append(window)
                                    #assigned_node = assigned_node + dict['requested_node']
                                    #assigned_node = assigned_node + 1
                                    assigned_cpu = assigned_cpu + dict['requested_cpu']
                                    assigned_memory = assigned_memory + dict['requested_memory']
                                    continue
                                else :
                                    #print "not appending (%s)" % (window,)
                                    #print "not appending (%s)" % (window,)
                                    #print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                    #print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                    #print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                    #print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
                                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "not appending (%s)" % (window,)
                                        print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                        print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                        print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                        print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
d2062 28
a2089 31
                if new_res['node_usage'] == 'node_shared' :
                    enough_cpu = 0
                    enough_memory = 0
                    accrued_cpu = 0
                    accrued_memory = 0
                    used_nodes_list = []
                    if dict.has_key('req_list') :
                        if dict['req_list'][0].has_key('cpu') :
                            requested_cpu = dict['req_list'][0]['cpu']
                        else :
                            requested_cpu = 0
                        if dict['req_list'][0].has_key('memory') :
                            requested_memory = dict['req_list'][0]['memory']
                        else :
                            requested_memory = 0
                    for saved_window in dict['saved_windows_list'] :
                        nodename = saved_window[1]
                        if nodename in used_nodes_list :
                            continue
                        if node_bin_dict.has_key(nodename) :
                            if node_bin_dict[nodename].has_key('cpu') :
                                accrued_cpu = node_bin_dict[nodename]['cpu']
                            if node_bin_dict[nodename].has_key('memory') :
                                accrued_memory = node_bin_dict[nodename]['memory']
                        if accrued_cpu >= requested_cpu and accrued_memory >= requested_memory :
                            used_nodes_list.append(nodename)
                            enough_cpu = 1
                            enough_memory = 1
                            break
                    if enough_cpu == 0 or enough_memory == 0 :
                        need_more = 1
d2091 3
a2093 36
                else :
                    if len(dict['saved_windows_list']) < dict['amount_int'] :
                        # FIXTHIS!  This is a bad test of whether counting_dict is fulfilled...
                        # Need to look at cpu and memory for each node in node_bin_dict
                        # and see if there is a set for each dict['req_list']
                        #print "len(dict['saved_windows_list']) < dict['amount_int'], need_more = 1"
                        #print "len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                        #print "dict['saved_windows_list'] (%s)" % (dict['saved_windows_list'],)
                        #print "dict['amount_int'] (%s)" % dict['amount_int']
                        #print "len(sized_windows_list) (%s)" % len(sized_windows_list)
                        if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                            print "len(dict['saved_windows_list']) < dict['amount_int'], need_more = 1"
                            print "len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                            print "dict['amount_int'] (%s)" % dict['amount_int']
                            print "len(sized_windows_list) (%s)" % len(sized_windows_list)
                        #print "dict['amount_int'] (%s)" % dict['amount_int']
                        #print "len(sized_windows_list) (%s)" % len(sized_windows_list)
                        #print "setting need_more = 1"
                        need_more = 1
                        break
                    #else :
                    #    print "insufficient saved_windows_list (%s) dict['amount_int'] (%s)" % (len(dict['saved_windows_list']), dict['amount_int'])
            #if \
            #  ( max_resource_int != None \
            #  and len(sized_windows_list) >= max_resource_int \
            #  and new_window[0] > current_start_time_float ) \
            #  or ( resource_amount_int != None \
            #  and len(sized_windows_list) >= resource_amount_int \
            #  and new_window[0] > current_start_time_float ) \
            #  :
            #    if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
            #        print "breaking because we have reached target amount"
            #    break
    
            # End of MOVE THIS

a2412 3
            if requested_node != None and requested_node > 0 and counting_dict['node_usage'] == 'node_exclusive' :
                requested_cpu = resources_db_handle[0][nodename]['ConsumableCpus']
                requested_memory = resources_db_handle[0][nodename]['ConsumableMemory']
@


1.164
log
@Added email support message
Martin W.Margo
With this, overrun jobs will give more information to the cancel_job()
mmargo
@
text
@d254 2
d3255 11
a3267 10
        def sort_by_key (list, key) :
            def get_keytuple (x, key=key) :
                if x.has_key(key) :
                    return (x[key], x)
                else :
                    return (0, x)
            keylist = map(get_keytuple, list)
            keylist.sort()
            keylist.reverse()
            return map(lambda (value, x): x, keylist)
d3321 5
a3325 1
        idle_jobs = sort_by_key(idle_jobs, 'priority')
d3485 2
@


1.163
log
@update current_start_time_float and clean out too-short windows before
next set of windows
@
text
@d3209 5
a3213 1
                        cancel_job(job_step, events_db_handle)
@


1.162
log
@fix calculation of wall_clock_used
@
text
@d1793 2
d1800 2
d2202 11
@


1.161
log
@report errors on open_db
@
text
@d4150 1
a4150 1
            wall_clock_used = job['Dispatch_Time'] - Now_float
@


1.160
log
@enable garbage collection, add pcpus and pmemory to allocated_dict_list
@
text
@d612 13
@


1.159
log
@record username_string for creator_string of every reservation
@
text
@d82 3
d601 1
d605 1
d643 1
d655 18
d2627 1
a2627 1
    temp_object = copy.deepcopy(dict[object['name']])
d5690 5
d5779 1
@


1.158
log
@add PREEMPT_MIN_RUNTIME check
@
text
@d456 2
d3862 4
a3865 1
        comment_string = 'standing reservation: ' + standing_reservation['name'] + ' ;' + standing_reservation['comment_string'] + ' ;'
d4308 1
d4333 1
d4984 1
a4984 1
  creator_string=None,
@


1.157
log
@delete dummy reservation
@
text
@d217 14
d5329 2
a5330 1
                                    if job_step['priority'] > jobs_dict[reservation['job_runID']]['priority'] :
a5358 1
                                        # def update_max_pushback(jobs_db_handle, reservations_db_handle, resources_db_handle) :
a5359 17
# Looks like create_res changes the reservations_dict, and python doesn't
# like that.  Do I need to send in a copy of reservations_db_handle?
# Or set mode='pushback'?
#trying to create pushback_res for (ds270.258.0)
#inside try
#some other pushback res exception occurred
#  File "/dsgpfs2/kenneth/test/catalina/install/Catalina.py", line 5256, in create_reservation
#    requested_resource_list = jobs_dict[reservation['job_runID']]['requested_resource_list']
#  File "/dsgpfs2/kenneth/test/catalina/install/Catalina.py", line 5466, in create_reservation
#    for res_key in reservations_db_handle[0] :
#message (exceptions.RuntimeError dictionary changed size during iteration 
#   File "/dsgpfs2/kenneth/test/catalina/install/Catalina.py", line 5256, in create_reservation
#    requested_resource_list = jobs_dict[reservation['job_runID']]['requested_resource_list']
#   File "/dsgpfs2/kenneth/test/catalina/install/Catalina.py", line 5466, in create_reservation
#    for res_key in reservations_db_handle[0] :
#)

d5430 1
a5430 1
                                        #print "priority not overriding"
@


1.156
log
@insert dummy reservation right after finding reservation_name
@
text
@d5640 1
@


1.155
log
@fix running start_time_float for preempted and resumed jobs
@
text
@d5066 14
@


1.154
log
@check for job in db
@
text
@d4106 2
a4107 1
        start_time_float = job['Dispatch_Time']
@


1.153
log
@check has_key('preempting') before using
@
text
@d4065 1
a4065 1
            if not jobs_db_handle[0][reservation['job_runID']]['state'] in ['Preempted', 'Running'] :
@


1.152
log
@add preemption
@
text
@d5268 2
a5269 1
                          (job_step['preempting'] >= 1 and \
d5299 1
a5299 1
                                if job_step['preempting'] >= 1 and jobs_dict[reservation['job_runID']].has_key('preemptible') and jobs_dict[reservation['job_runID']]['preemptible'] >= 1 :
@


1.151
log
@interactivebug
@
text
@d61 1
d892 2
d1492 2
d1568 1
d1615 1
a1615 1
                #print "ran out of requested_resource_list, setting req_list to []"
d1714 1
a1714 1
        #print "len(sized_windows_list) (%s)" % len(sized_windows_list)
d1986 10
d2043 1
a2043 1
            #    print "saved_windows_list (%s) amount (%s) resource_dict (%s)" % (counting_dict['saved_windows_list'],counting_dict['amount_int'], counting_dict['resource_dict'].keys())
d2211 1
d2247 1
d2251 1
a2251 1
        #print "need_more == 1"
d2260 1
a2260 1
def get_sorted_windows_list(sized_windows_list, new_res, accepted_nodes_list, sort_policy_code, resources_db_handle, windows_and_reservations_list=None) :
d2294 2
d2297 1
d2302 8
d2327 1
a2327 1
                sort_tuple = ( 0, -(window[0]), long(resource_sort_int), window)
d2329 1
a2329 1
                sort_tuple = (-(affinity), -(window[0]), long(resource_sort_int), window)
d2345 3
a2347 1
    input_tuple = (sized_windows_list,temp_nodes_list,new_res,Now_float,windows_and_reservations_list,resources_dict)
d2587 1
a2587 1
    temp_object = copy.copy(dict[object['name']])
d2987 4
a2990 2
          jobs_dict[this_res['job_runID']].has_key('run_at_risk_int') and \
          jobs_dict[this_res['job_runID']]['run_at_risk_int'] >= 1
d2994 5
a2998 2
          jobs_dict[this_res['job_runID']].has_key('run_at_risk_int') and \
          jobs_dict[this_res['job_runID']]['run_at_risk_int'] >= 1 )
d3017 1
d3040 1
d3080 1
d3082 1
d3085 1
a3085 1
                    print "found cpu conflict %s %s %s" % (nonrisk_res_dict[node]['cpu'], atrisk_res_dict[node]['cpu'], resources_dict[node]['ConsumableCpus'])
d3088 1
a3088 1
                    print "found memory conflict %s %s %s" % (nonrisk_res_dict[node]['memory'], atrisk_res_dict[node]['memory'], resources_dict[node]['ConsumableMemory'])
d3091 1
d3096 2
d3099 3
a3101 1
                delete_object(risk_res['name'], reservations_db_handle)
d3105 6
a3110 1
                            cancel_job(jobs_dict[risk_res['job_runID']], events_db_handle)
d3117 4
a3120 1
                        print "%s should be canceled!" % jobs_dict[risk_res['job_runID']]['name']
d3130 8
a3137 2
            start_time = job_step['Dispatch_Time']
            if Now_float - start_time - MAXJOBOVERRUN > duration :
d3218 3
a3220 1
            elif state == 'Running' or state == 'Starting' :
d4055 1
a4055 1
def update_running_reservations(runningstarting_jobs, reservations_db_handle, resources_db_handle) :
a4058 1
    purpose_type_string = 'running'
d4062 1
a4062 1
        if reservation['purpose_type_string'] == 'running' :
d4064 4
d4075 1
d4077 8
d4091 13
a4103 2
        if job['Dispatch_Time'] + job['wall_clock_limit'] < Now_float :
            duration_float = Now_float - job['Dispatch_Time'] + FUDGE_FACTOR
d4220 30
d4338 1
d4341 16
d4368 4
a4371 1
    purpose_type_string = 'job'
d4383 1
d4390 1
a4390 1
      purpose_type_string='job',
d4394 9
d4775 81
d4887 1
a4887 1
    update_running_reservations(runningstarting_jobs, reservations_db_handle, resources_db_handle)
d4897 2
d5069 1
a5069 1
        if purpose_type_string == 'job' :
d5088 4
a5091 1
    if purpose_type_string == 'job' :
d5240 7
d5259 1
d5265 7
a5271 2
                          jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                          jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1 :
d5274 5
a5278 4
                            temp_res['node_list'] = reservation['node_list']
                            temp_res['allocated_dict_list'] = reservation['allocated_dict_list']
                            temp_res['start_time_float'] = reservation['start_time_float']
                            temp_res['end_time_float'] = reservation['end_time_float']
d5280 10
d5292 1
d5294 139
a5432 3
                              jobs_dict[reservation['job_runID']]['start_count_int'] >= 1) :
                                temp_res['start_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                                temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
d5434 2
d5441 10
a5450 2
                                temp_res['start_time_float'] = Now_float
                                temp_res['end_time_float'] = Now_float
d5453 1
d5455 1
d5457 11
a5467 2
                        if not jobs_dict.has_key(reservation['job_runID']) :
                            print "job (%s) has reservation (%s), but does exist in jobs_dict!" % (reservation['job_runID'], reservation['name'])
d5469 1
d5473 2
d5496 4
d5502 3
d5507 3
a5509 3
                  jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                  jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1 :
                    temp_res = copy.deepcopy(reservation)
d5514 3
d5520 1
a5520 1
                    truncated_reservations.append(temp_res)
d5522 3
a5524 1
                    truncated_reservations.append(reservation)
d5526 3
d5530 3
d5540 4
d5557 1
a5557 3
        if reservation['start_time_float'] <= current_start_time_float \
          < reservation['end_time_float'] or \
          reservation['start_time_float'] < current_start_time_float + duration_float <= reservation['end_time_float'] :
d5563 4
a5566 2
              jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
              jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1 and \
d5584 1
a5584 1
    if purpose_type_string != 'job' :
d5593 1
a5593 1
      accepted_nodes_list, node_sort_policy, resources_db_handle, windows_and_reservations_list )
d5628 11
d5647 74
@


1.150
log
@print node state in unavailable email
@
text
@d2649 3
@


1.149
log
@commented out a print
@
text
@d4592 1
a4592 1
    down_resource_names_list = []
d4599 2
a4600 1
            down_resource_names_list.append(resource['name'])
@


1.148
log
@in get_sized, do better check for fulfillment of counting_dict with
shared req_list
@
text
@d3037 1
a3037 1
                    print "nonrisk_res_dict[nonrisknode]['cpu'] (%s), nonrisk_res_dict[nonrisknode]['memory'] (%s)" % (nonrisk_res_dict[nonrisknode]['cpu'], nonrisk_res_dict[nonrisknode]['memory'])
@


1.147
log
@radical changes to getlongest...
@
text
@d1558 1
d1561 1
d1570 1
d1736 1
d1899 10
a1908 5
            if len(saved_sorting_list) > 0 :
                if getlongest == 1 :
                    candidate_windows_list = [current_start_time_float, sized_windows_list, sized_dict_list]
                    if len(sized_windows_list) > 0 and (new_res['node_usage'] =='node_shared' or (sufficient_node == 1 and sufficient_cpu == 1 and sufficient_memory == 1)) :
                        sized_windows_lists_list.append(candidate_windows_list)
d2026 2
d2030 1
d2037 59
a2095 20
                    need_more = 1
                    break
                if len(dict['saved_windows_list']) < dict['amount_int'] :
                    #print "len(dict['saved_windows_list']) < dict['amount_int'], need_more = 1"
                    #print "len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                    #print "dict['saved_windows_list'] (%s)" % (dict['saved_windows_list'],)
                    #print "dict['amount_int'] (%s)" % dict['amount_int']
                    #print "len(sized_windows_list) (%s)" % len(sized_windows_list)
                    if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        print "len(dict['saved_windows_list']) < dict['amount_int'], need_more = 1"
                        print "len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                        print "dict['amount_int'] (%s)" % dict['amount_int']
                        print "len(sized_windows_list) (%s)" % len(sized_windows_list)
                    #print "dict['amount_int'] (%s)" % dict['amount_int']
                    #print "len(sized_windows_list) (%s)" % len(sized_windows_list)
                    #print "setting need_more = 1"
                    need_more = 1
                    break
                #else :
                #    print "insufficient saved_windows_list (%s) dict['amount_int'] (%s)" % (len(dict['saved_windows_list']), dict['amount_int'])
d2112 17
a2128 6
                #print "breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(current_start_time_float)))
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "need_more == 0"
                break
            #else :
                #print "not breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(current_start_time_float)))
d2209 1
a2209 1
                    candidate_tuple_list.append((window_count, -(copy.copy(current_start_time_float)), copy.deepcopy(counting_dict_list)))
d2212 1
d2214 1
d2239 1
d2488 3
a2490 3
                #    print "not enough node (%s) cpu (%s) memory (%s)" % (node_cpu_memory_dict[nodename]['node'] - consumed_node, node_cpu_memory_dict[nodename]['cpu'] - consumed_cpu, node_cpu_memory_dict[nodename]['memory'] - consumed_memory)
                #    print "node_cpu_memory_dict (%s)" % node_cpu_memory_dict
                #    print "requested_node (%s) requested_cpu (%s) requested_memory (%s)" % (requested_node, requested_cpu, requested_memory)
d2512 1
d3052 1
a3052 1
                print "deleting (%s) (%s) (%s)" % (risk_res['name'], risk_res['purpose_type_string'], risk_res['job_runID'])
d3809 1
a3809 1
                print "end of scheduling"
@


1.146
log
@revert to 1.144, since sharedmap fails
@
text
@d1721 10
a1730 5
            # If resource_amount_int == None, save sized_windows_list with
            # current start time, so that the largest of the sets can be returned
            if getlongest == 1 :
                candidate_windows_list = [current_start_time_float, sized_windows_list, sized_dict_list]
                sized_windows_lists_list.append(candidate_windows_list)
d1733 2
d1895 5
d2075 2
a2076 1
        sized_windows_lists_list.append(candidate_windows_list)
d2099 3
d2130 5
a2134 1
                        continue
d2141 7
a2147 1
                    candidate_tuple_list.append((none_count, copy.deepcopy(counting_dict_list)))
d2150 2
a2151 1
                counting_dict_list = candidate_tuple_list[-1][1]
@


1.145
log
@if getting longest in get_sized, break.  Also, don't append to
sized_dict_lists_list, if sized_list is 0
@
text
@d1725 1
a1725 6
                if len(sized_windows_list) > 0 :
                    sized_windows_lists_list.append(candidate_windows_list)
            # Should current_start_time_float be set to the next window?
            # No, leave like this.  We are evaluating the current windows,
            # and current_start_time_float should reflect the current set,
            # rather than the next set.
a1727 2
            #if new_window_index != last_window_index and open_windows_list[new_window_index + 1][0] > current_start_time_float :
            #    current_start_time_float = open_windows_list[new_window_index + 1][0]
d2063 1
a2063 2
        if len(sized_windows_list) > 0 :
            sized_windows_lists_list.append(candidate_windows_list)
a2085 3
                if len(window_list) == 0 :
                    continue
                current_start_time_float = start_and_list[0]
a2121 1
                    break
@


1.144
log
@different -l for NORMAL and not NORMAL sys logging.
catsyslog for insufficient job resources
@
text
@d1725 6
a1730 1
                sized_windows_lists_list.append(candidate_windows_list)
d1733 2
d2070 2
a2071 1
        sized_windows_lists_list.append(candidate_windows_list)
d2094 3
d2133 1
@


1.143
log
@send warnings to syslog
@
text
@d4439 3
a4441 1
            print "job reservation (%s) failed for runID (%s) due to insufficient resources" % ( new_res['name'], new_res['job_runID'] )
d5173 1
a5173 1
        cmd = LOGGER + ' -t CATALINA' + ' -p ' + LOGGER_FACILITY + '.debug ' + message
d5176 1
a5176 1
        cmd = LOGGER + ' -t CATALINA' + ' -p ' + LOGGER_FACILITY + '.' + priority + ' ' + message
@


1.142
log
@removed get_resource_list and get_resource_name_list
@
text
@d615 1
d638 1
d651 1
d2607 1
d4240 1
d4514 1
@


1.141
log
@FORCETZ
@
text
@d56 2
a57 2
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resource_list
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_resource_name_list
@


1.140
log
@shared jobs request a node, but don't allocate it
@
text
@d189 9
d426 1
a426 1
    #print "window[0] (%s) window[1] (%s) window[2] (%s)" % (time.asctime(time.gmtime(window[0])), time.asctime(time.gmtime(window[1])), window[2])
d1008 1
a1008 1
                    print "end_limit is (%s)" % time.asctime(time.gmtime(end_limit))
d1256 1
a1256 1
                                        print "start (%s) end (%s)" % (time.asctime(time.gmtime(started_window[1]['start'])), time.asctime(time.gmtime(event[0])))
d1449 1
a1449 1
#                            print "appending open window (%s, %s, %s)" % (time.asctime(time.gmtime(potential_start_time_float)), time.asctime(time.gmtime(reservation[0])), node)
d1480 1
a1480 1
                  (time.asctime(time.gmtime(new_window[0])), time.asctime(time.gmtime(new_window[1])), new_window[2])
d1503 1
a1503 1
              (time.asctime(time.gmtime(new_window[0])), time.asctime(time.gmtime(new_window[1])), new_window[2])
d1506 1
a1506 1
    #      (time.asctime(time.gmtime(new_window[0])), time.asctime(time.gmtime(new_window[1])), new_window[2])
d1518 1
a1518 1
    #          (time.asctime(time.gmtime(window[0])), time.asctime(time.gmtime(window[1])), duration)
d1550 3
a1552 3
    if earliest_start_float != None :
        time_tuple = time.gmtime(math.floor(earliest_start_float))
        time_string = time.asctime(time_tuple)
d1558 1
a1558 1
    #print "current_start_time_float (%s)" % time.asctime(time.gmtime(current_start_time_float))
d1669 1
a1669 1
        #  (time.asctime(time.gmtime(new_window[1])), time.asctime(time.gmtime(current_start_time_float)), new_res['duration_float'])
d1683 1
a1683 1
            #  (time.asctime(time.gmtime(current_start_time_float)), new_res['duration_float'], time.asctime(time.gmtime(new_res['latest_end_float'])), time.asctime(time.gmtime(new_window[0] + new_res['duration_float'])))
d1691 1
a1691 1
                  (time.asctime(time.gmtime(new_window[1])), time.asctime(time.gmtime(current_start_time_float)), new_res['duration_float'])
d1693 1
a1693 1
            #  (time.asctime(time.gmtime(new_window[1])), time.asctime(time.gmtime(current_start_time_float)), new_res['duration_float'])
d1711 1
a1711 1
                  ( time.asctime(time.gmtime(current_start_time_float)), len(sized_windows_list), new_window[2] )
d1713 1
a1713 1
                  (time.asctime(time.gmtime(new_window[0])), time.asctime(time.gmtime(new_window[1])), new_window[2])
d2046 1
a2046 1
                #print "breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.gmtime(new_window[0])), time.asctime(time.gmtime(current_start_time_float)))
d2051 1
a2051 1
                #print "not breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.gmtime(new_window[0])), time.asctime(time.gmtime(current_start_time_float)))
d3297 1
a3297 1
def get_new_candidate_time(start_time_dict, candidate_epoch) :
d3304 7
a3310 1
    os.environ['TZ'] = 'GMT0'
d3317 1
a3317 1
    #new_candidate_tuple = time.gmtime(candidate_epoch)
d3319 2
a3320 2
        candidate_tuple = time.gmtime(candidate_epoch)
        #new_candidate_tuple = time.gmtime(candidate_epoch)
d3336 1
a3336 1
                new_candidate_tuple = time.gmtime(candidate_epoch)
d3369 1
a3369 1
                new_candidate_tuple = time.gmtime(candidate_epoch)
d3399 1
a3399 1
                new_candidate_tuple = time.gmtime(candidate_epoch)
d3432 1
a3432 1
                new_candidate_tuple = time.gmtime(candidate_epoch)
d3464 1
a3464 1
                new_candidate_tuple = time.gmtime(candidate_epoch)
d3645 4
d3705 1
a3705 1
              get_new_candidate_time( start_time_dict, candidate_epoch )
d3710 1
a3710 1
            #print "earliest_start_float (%s)" % time.asctime(time.gmtime(earliest_start_float))
d3877 1
a3877 1
                new_candidate_tuple = time.gmtime(candidate_epoch)
d4063 1
d4095 1
d4663 2
a4664 2
    #print "create_res: earliest_start_float (%s) (%s)" % (time.asctime(time.gmtime(earliest_start_float)), earliest_start_float)
    #print "create_res: latest_end_float (%s) (%s)" % (time.asctime(time.gmtime(latest_end_float)), latest_end_float)
d4895 1
a4895 1
                    print "blocking_reservation (%s, %s, %s)" % (time.asctime(time.gmtime(blocking_reservation['start_time_float'])), time.asctime(time.gmtime(blocking_reservation['end_time_float'])), blocking_reservation['node_list'])
d4965 1
a4965 1
                  (time.asctime(time.gmtime(new_window[0])), time.asctime(time.gmtime(new_window[1])), new_window[2])
@


1.139
log
@back out change to fix leaky system res
@
text
@d1807 1
d1968 1
a1968 1
                                    assigned_node = assigned_node + dict['requested_node']
d2299 1
a2299 1
            if requested_node != None and requested_node > 0 :
d2338 2
d2342 2
d2346 2
d2392 4
d2402 1
a2402 1
                          { 'node' : requested_node,
d2406 2
d2412 1
a2412 1
                       'node' : requested_node,
d5069 2
a5070 1
            allocated_dict['node'] = 1
@


1.138
log
@for node_shared reservations, use requested_node 1, but allocated_node 0
@
text
@a1806 1
                    requested_node = 1
d1967 1
a1967 1
                                    #assigned_node = assigned_node + dict['requested_node']
d2340 1
a2340 5
                    if counting_dict['node_usage'] == 'node_shared' :
                        allocated_node = 1 - consumed_cpu_memory_dict[nodename]['node']
                        #allocated_node = 0
                    else :
                        allocated_node = requested_node
@


1.137
log
@screen out open windows that are too small for the res
@
text
@d1807 1
d1968 1
a1968 1
                                    assigned_node = assigned_node + dict['requested_node']
d2341 5
a2345 1
                    allocated_node = requested_node
@


1.136
log
@use allocated_hosts instead of task_hosts, sincel LoadL may nt
report the taskinstancemachinemap for serial jobs...
@
text
@d1500 13
d4967 1
d4971 1
a4971 1
      new_res, open_windows_list, resources_db_handle, jobs_db_handle, resource_dict_list=resource_dict_list, requested_resource_list = requested_resource_list)
@


1.135
log
@fixed else : bug
@
text
@d2904 1
a2904 1
                            nonrisk_res_dict[nonrisknode] = { 'cpu' : resources_dict[nonrisknode]['ConsumableMemory'], 'memory' : resources_dict[nonrisknode]['ConsumableMemory'] }
d3936 6
@


1.134
log
@removed some prints
@
text
@d3818 1
a3818 1
            #else :
@


1.133
log
@set all allocated_dict['node'] to 1
@
text
@d3664 2
a3665 2
            print "standing_reservation (%s)" % standing_reservation['name']
            print "earliest_start_float (%s)" % time.asctime(time.gmtime(earliest_start_float))
d3694 1
a3694 1
                print "full size, full duration failed"
d3818 2
a3819 2
            else :
                print "full size, full duration succeeded"
@


1.132
log
@update all attributes of configured resources
@
text
@d1030 7
d1041 1
a1041 1
                              'amount' : event[2]['node'], 'state' : 'active'
d1099 2
a1100 2
                    if event[2].has_key('node') :
                        neednodes = event[2]['node']
d1102 4
a1105 1
                        neednodes = 1
d1247 1
d1812 2
d1816 2
d5036 1
@


1.131
log
@fixed bug in get_sized node count check
@
text
@d4444 2
@


1.130
log
@validate QOS
@
text
@d1653 1
d1661 8
a1668 4
            continue
        #print "adding this window (%s)" % (new_window,)
        # Add the new window to the list of sized_windows_list
        sized_windows_list.append(new_window)
@


1.129
log
@fixed bug in get_chosen, with missing else: continue
@
text
@d2551 7
a2557 4
    if string.atoi(QOS) < len(QOS_PRIORITY_dict.keys()) :
        return QOS_PRIORITY_dict[QOS]
    else :
        return 0
d2562 6
a2567 3
    if string.atoi(QOS) < len(QOS_TARGETXF_dict.keys()) :
        return QOS_TARGETXF_dict[QOS]
    else :
d2573 6
a2578 3
    if string.atoi(QOS) < len(QOS_TARGETQT_dict.keys()) :
        return QOS_TARGETQT_dict[QOS]
    else :
d2663 5
a2667 1
        QOS_priority = float(get_QOS_priority(temp_job['QOS']))
@


1.128
log
@allocated_dict['node'] = 0 for node_shared running job reservations
@
text
@d2343 1
a2343 1
                #else :
d2347 1
@


1.127
log
@check resource_dict in get_chosen
order node_list according to allocated_dict_list in create_res
@
text
@d3897 1
d3916 1
d3924 1
@


1.126
log
@for shared jobs, don't allocate a node twice, even if it will fit
@
text
@d2263 2
d4995 5
a4999 1
        new_res['node_list'] = chosen_nodes_list
@


1.125
log
@sort counting_dict_list in get_chosen, then resort back to initiatormap order
@
text
@d1863 1
d1880 1
d1905 1
d1924 5
a1928 1
                                if dict['requested_node'] <= node_bin_dict[window[1]]['node'] - assigned_node and dict['requested_cpu'] <= node_bin_dict[window[1]]['cpu'] - assigned_cpu and dict['requested_memory'] <= node_bin_dict[window[1]]['memory'] - assigned_memory :
d1932 1
d1935 1
d2320 1
a2320 1
                if node_cpu_memory_dict[nodename]['node'] - consumed_node >= allocated_node and \
d2346 1
a2346 1
                if node_cpu_memory_dict[nodename]['node'] - consumed_node >= requested_node and \
@


1.124
log
@reverse order of counting_dict_list
@
text
@d1561 1
d1588 1
d2184 2
d2211 8
d2327 1
d2353 1
d2374 5
@


1.123
log
@removed print statement
@
text
@d1552 2
d1583 2
d1603 2
a1604 2
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "sized_dict_list (%s)" % sized_dict_list
d1611 3
d1619 2
a1620 2
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "open_windows_list (%s)" % (open_windows_list,)
d1841 1
a1841 1
            #request_sorting_list.reverse()
d1855 6
d1871 2
a1872 2
                    if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        print "start of dict for loop for (%s)" % dict['amount_int']
d1874 2
a1875 2
                        if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                            print "dict['amount_int'] == None, continuing"
d1881 4
a1884 4
                        if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                            print "1. len(dict['saved_windows_list']) >= dict['amount_int'], continuing"
                            print "1. len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                            print "1. dict['amount_int'] (%s)" % dict['amount_int']
d1887 4
a1890 4
                        if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                            print "2. len(dict['saved_windows_list']) < dict['amount_int'], continuing"
                            print "2. len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                            print "2. dict['amount_int'] (%s)" % dict['amount_int']
d1943 5
d1998 2
d2072 1
d2089 2
d2993 2
d3004 2
d3010 7
a3016 4
                if len(job['resource_dict_list'][0]['resource_dict'].keys()) < job['resource_dict_list'][0]['amount_int'] :
                    job['ineligible_reason'] = 'BADRESOURCELIST'
                    ineligible_jobs.append(job)
                    continue
d4884 1
a4884 1
        print "len(sized_windows_list) (%s)" % len(sized_windows_list)
@


1.122
log
@fixed problem with finding last node
@
text
@d2331 1
a2331 1
                print "counting_dict['amount_int'] == None, setting need_more = 1"
@


1.121
log
@fixed bug for resource amount == None in get_chosen
@
text
@d1525 1
d1630 26
d1657 1
a1657 1
        if new_window[0] > current_start_time_float or new_window_index == last_window_index :
d1667 18
d1973 1
a1973 1
            if need_more == 0 and new_window[0] > current_start_time_float :
a1979 41
        # - update the start and end times, if necessary
        # - remove any windows that end before the new end time
        # If the new window start time is greater than the current_start_time_float
        # reset the current_start_time_float.  Delete any of the sized_windows_list
        # that have an end_time_float too early to accomodate the reservation
        # If resource_amount_int == None, save sized_windows_list with
        # current start time, so that the largest of the sets can be returned
        if new_window[0] > current_start_time_float :
            if getlongest == 1 :
                candidate_windows_list = [current_start_time_float, sized_windows_list, sized_dict_list]
                sized_windows_lists_list.append(candidate_windows_list)
            current_start_time_float = new_window[0]
            saved_windows_list = []
            for i in range(len(sized_windows_list)) :
                if sized_windows_list[i][1] - current_start_time_float >= \
                new_res['duration_float'] :
                    saved_windows_list.append(sized_windows_list[i])
            sized_windows_list = saved_windows_list
        # Break if latest_end_float exists, and there is not enough time
        if new_res['latest_end_float'] != None and \
          ( current_start_time_float + \
          new_res['duration_float'] > new_res['latest_end_float'] or \
          new_window[0] + new_res['duration_float'] > new_res['latest_end_float'] ) :
            if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
                print "breaking, not enough time left before end of window"
            #print "breaking, not enough time left before end of window"
            #print "current_start (%s) new_res['duration_float'] (%s) new_res['latest_end_float'] (%s) new_window[0]+new_res['duration_float'] (%s)" % \
            #  (time.asctime(time.gmtime(current_start_time_float)), new_res['duration_float'], time.asctime(time.gmtime(new_res['latest_end_float'])), time.asctime(time.gmtime(new_window[0] + new_res['duration_float'])))
            break
        # Skip if there is not enough time in this window
        if new_window[1] - current_start_time_float < \
          new_res['duration_float'] :
            if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
                print "skipping this window due to lack of time (%s) - (%s) < (%s)" % \
                  (time.asctime(time.gmtime(new_window[1])), time.asctime(time.gmtime(current_start_time_float)), new_res['duration_float'])
            #print "skipping this window due to lack of time (%s) - (%s) < (%s)" % \
            #  (time.asctime(time.gmtime(new_window[1])), time.asctime(time.gmtime(current_start_time_float)), new_res['duration_float'])
            continue
        #print "adding this window (%s)" % (new_window,)
        # Add the new window to the list of sized_windows_list
        sized_windows_list.append(new_window)
@


1.120
log
@only check for resource fulfillment when new window is later starting
or when on the last window
@
text
@d2324 1
a2324 1
            if counting_dict['amount_int'] == None and not (max_resource_int != None and len(chosen_nodes_list)) >= max_resource_int :
@


1.119
log
@filter accepted_nodes_list with resource_dict_list for job res
@
text
@d1540 1
d1606 1
d1609 1
d1614 1
d1616 6
a1621 1
    for new_window in open_windows_list :
d1629 306
a1934 4
        if need_more == 0 and new_window[0] > current_start_time_float :
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "need_more == 0"
            break
d1976 2
a1977 279
        # - check to see if the sized_dict is fully populated
        if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
            print "current_start_time_float (%s) len(sized_windows_list) (%s) new_window[2] (%s)" % \
              ( time.asctime(time.gmtime(current_start_time_float)), len(sized_windows_list), new_window[2] )
            print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
              (time.asctime(time.gmtime(new_window[0])), time.asctime(time.gmtime(new_window[1])), new_window[2])

        # For multiple-requirement reservation, need to see if each
        # resource_dict['amount'] has been reached with the current
        # set of open windows...
        #counting_dict_list = copy.deepcopy(sized_dict_list)
        counting_dict_list = sized_dict_list
        #print "counting_dict_list (%s)" % (counting_dict_list,)
        for sized_dict in counting_dict_list :
            sized_dict['saved_windows_list'] = []

        # aggregate all the cpu and memory windows into bins by node
        # I lose start and end info for the individual open_window
        # here...I need to keep a window_list for use by get_sorted
        # later on....
        node_bin_dict = {}
        for sized_window in sized_windows_list :
            #print "sized_window (%s)" % (sized_window,)
            if node_bin_dict.has_key(sized_window[2]['nodename']) :
                node_bin_dict[sized_window[2]['nodename']]['window_list'].append(sized_window)
                if sized_window[2].has_key('node') :
                    node_bin_dict[sized_window[2]['nodename']]['node'] = node_bin_dict[sized_window[2]['nodename']]['node'] + sized_window[2]['node']
                if sized_window[2].has_key('cpu') :
                    node_bin_dict[sized_window[2]['nodename']]['cpu'] = node_bin_dict[sized_window[2]['nodename']]['cpu'] + sized_window[2]['cpu']
                if sized_window[2].has_key('memory') and sized_window[2]['memory'] != None :
                    #print "adding (%s)" % (sized_window[2]['memory'],)
                    node_bin_dict[sized_window[2]['nodename']]['memory'] = node_bin_dict[sized_window[2]['nodename']]['memory'] + sized_window[2]['memory']
            else :
                node_bin_dict[sized_window[2]['nodename']] = {'window_list' : [sized_window,]}
                if sized_window[2].has_key('cpu') and sized_window[2]['cpu'] != None :
                    node_bin_dict[sized_window[2]['nodename']]['node'] = 0
                    node_bin_dict[sized_window[2]['nodename']]['cpu'] = sized_window[2]['cpu']
                    node_bin_dict[sized_window[2]['nodename']]['memory'] = 0
                elif sized_window[2].has_key('memory') :
                    node_bin_dict[sized_window[2]['nodename']]['node'] = 0
                    node_bin_dict[sized_window[2]['nodename']]['memory'] = sized_window[2]['memory']
                    #print "adding (%s)" % (sized_window[2]['memory'],)
                    node_bin_dict[sized_window[2]['nodename']]['cpu'] = 0
                elif sized_window[2].has_key('node') :
                    node_bin_dict[sized_window[2]['nodename']]['node'] = sized_window[2]['node']
                    node_bin_dict[sized_window[2]['nodename']]['cpu'] = 0
                    node_bin_dict[sized_window[2]['nodename']]['memory'] = 0
                else :
                    node_bin_dict[sized_window[2]['nodename']]['node'] = 0
                    node_bin_dict[sized_window[2]['nodename']]['cpu'] = 0
                    node_bin_dict[sized_window[2]['nodename']]['memory'] = 0
        for nodename in node_bin_dict.keys() :
            node_bin_dict[nodename]['viable_request_count'] = 0
        saved_sorting_list = []
        request_sorting_list = []
        # counting_dict has structure of requested_resource:
        # {'type' : <node_shared|node_exclusive>,
        #  'req_list' : [ {'cpu' : <cpus>, 'memory' : <memory>}...]}
        # req_list has one cpu-memory dictionary for each initiator
        # in the initiator map
        # indexa is which node in initiator map
        # indexb is which initiator in that node
        # For each initiator request, how many windows can satisfy
        # that request?
        #print "counting_dict_list (%s)" % (counting_dict_list,)
        for indexa in range(len(counting_dict_list)) :
            dict = counting_dict_list[indexa]
            #print "dict (%s)" % dict
            # for each term (representing a single node) in the
            # initiatormap, sum requested cpu and memory for all
            # initiators for that node.
            # each entry of the req_list is:
            # { 'cpu' : <cpus requested>,
            #   'memory' : <MB requested>}
            requested_node = 0
            requested_cpu = 0
            requested_memory = 0
            if new_res['node_usage'] == 'node_exclusive' :
                #print "'node_usage' == 'node_exclusive'"
                # what if amount_int == None?
                requested_node = 1
                requested_cpu = None
                requested_memory = None
            else :
                for indexb in range(len(dict['req_list'])) :
                    requested_resource = dict['req_list'][indexb]
                    if requested_resource.has_key('cpu') :
                        requested_cpu = requested_cpu + requested_resource['cpu']
                    if requested_resource.has_key('memory') :
                        requested_memory = requested_memory + requested_resource['memory']
            dict['node_usage'] = new_res['node_usage']
            dict['requested_node'] = requested_node
            dict['requested_cpu'] = requested_cpu
            dict['requested_memory'] = requested_memory
            #print "requested_node (%s) requested_cpu (%s) requested_memory (%s)" % (requested_node, requested_cpu, requested_memory)
            #print "dict (%s)" % dict
            # now, requested_cpu and requested_memory is the total for this
            # initiator term.  how many entries in node_bin_dict can
            # meet this request?
            viable_request_count = 0
            for nodename in node_bin_dict.keys() :
                if requested_node <= node_bin_dict[nodename]['node'] :
                    sufficient_node = 1
                else :
                    sufficient_node = 0
                if dict['node_usage'] == 'node_exclusive' :
                    if resources_db_handle[0][nodename].has_key('ConsumableCpus') :
                        acConsumableCpus = resources_db_handle[0][nodename]['ConsumableCpus']
                    else :
                        acConsumableCpus = 0
                    if resources_db_handle[0][nodename].has_key('ConsumableMemory') :
                        acConsumableMemory = resources_db_handle[0][nodename]['ConsumableMemory']
                    else :
                        acConsumableMemory = 0
                    if acConsumableCpus <= node_bin_dict[nodename]['cpu'] :
                        sufficient_cpu = 1
                    else :
                        sufficient_cpu = 0
                    if acConsumableMemory <= node_bin_dict[nodename]['memory'] :
                        sufficient_memory = 1
                    else :
                        sufficient_memory = 0
                else :
                    if requested_cpu <= node_bin_dict[nodename]['cpu'] :
                        sufficient_cpu = 1
                    else :
                        sufficient_cpu = 0
                    if requested_memory <= node_bin_dict[nodename]['memory'] :
                        sufficient_memory = 1
                    else :
                        sufficient_memory = 0
                if sufficient_node == 1 and sufficient_cpu == 1 and sufficient_memory == 1 :
                    viable_request_count = viable_request_count + 1
                    # how many of the requested_resources could fit in the node open
                    # window aggregate?  Need to fit all req_list into the aggregate
                    # node cpu and memory
                    # If this dict can fit, then increment the viable_request_count
                    # for each nodename.  node_bin_dict needs a 'viable_request_count'
                    # key
                    node_bin_dict[nodename]['viable_request_count'] = \
                      node_bin_dict[nodename]['viable_request_count'] + 1
            request_sorting_list.append( (viable_request_count, dict ) )

        # need to allow for the situation where two initiatormap entries
        # could fit on a single node....
        for nodename in node_bin_dict.keys() :
            saved_sorting_list.append( (node_bin_dict[nodename]['viable_request_count'], nodename) )
        request_sorting_list.sort()
        #request_sorting_list.reverse()
        #counting_dict_list = map(lambda x : x[1], request_sorting_list)
        counting_dict_list = map(getfirstindex, request_sorting_list)
        #print "counting_dict_list (%s)" % (counting_dict_list,)
        saved_sorting_list.sort()
        #saved_sorting_list.reverse()
        # this will break, next time through loop, sized_windows_list
        # elements will be wrong...
        #sized_windows_list = map(lambda x : x[1], saved_sorting_list)
        # This check should be whether there are sufficient
        # proc and memory windows to satisfy all
        # dict['requested_resource'] entries

        # populate saved_windows_list for each dict in counting_dict_list
        for window in saved_sorting_list :
            assigned_node = 0
            assigned_cpu = 0
            assigned_memory = 0
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "saved_sorting_list window (%s)" % (window,)
            #print "saved_sorting_list window (%s)" % (window,)
            if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                print "start of window for loop for (%s)" % window[1]
            for dict in counting_dict_list :
                if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                    print "start of dict for loop for (%s)" % dict['amount_int']
                if dict['amount_int'] == None :
                    if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        print "dict['amount_int'] == None, continuing"
                    if dict['resource_dict'].has_key(window[1]) :
                        dict['saved_windows_list'].append(window)
                        break
                    continue
                if len(dict['saved_windows_list']) >= dict['amount_int'] :
                    if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        print "1. len(dict['saved_windows_list']) >= dict['amount_int'], continuing"
                        print "1. len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                        print "1. dict['amount_int'] (%s)" % dict['amount_int']
                    continue
                else :
                    if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        print "2. len(dict['saved_windows_list']) < dict['amount_int'], continuing"
                        print "2. len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                        print "2. dict['amount_int'] (%s)" % dict['amount_int']
                    if dict['resource_dict'].has_key(window[1]) :
                        # if node_bin_dict[nodename]['cpu'] and memory more
                        # than total requested cpu and memory for the req dict,
                        # add the node
                        if dict['node_usage'] == 'node_exclusive' :
                            #if dict['requested_node'] <= node_bin_dict[window[1]]['node'] and dict['requested_cpu'] <= node_bin_dict[window[1]]['cpu'] and dict['requested_memory'] <= node_bin_dict[window[1]]['memory'] :
                            if dict['requested_node'] <= node_bin_dict[window[1]]['node'] - assigned_node and resources_db_handle[0][window[1]]['ConsumableCpus'] - assigned_cpu <= node_bin_dict[window[1]]['cpu'] and resources_db_handle[0][window[1]]['ConsumableMemory'] <= node_bin_dict[window[1]]['memory'] - assigned_memory :
                                if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                                    print "appending node (%s)" % window[1]
                                #print "appending node (%s)" % window[1]
                                dict['saved_windows_list'].append(window)
                                assigned_node = assigned_node + dict['requested_node']
                                assigned_cpu = assigned_cpu + node_bin_dict[window[1]]['cpu']
                                assigned_memory = assigned_memory + node_bin_dict[window[1]]['memory']
                                continue
                            else :
                                #print "not appending (%s)" % (window,)
                                #print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                #print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                #print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                #print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "not appending (%s)" % (window,)
                                    print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                    print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                    print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                    print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
                        else :
                            if dict['requested_node'] <= node_bin_dict[window[1]]['node'] - assigned_node and dict['requested_cpu'] <= node_bin_dict[window[1]]['cpu'] - assigned_cpu and dict['requested_memory'] <= node_bin_dict[window[1]]['memory'] - assigned_memory :
                                if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                                    print "appending node (%s)" % window[1]
                                #print "appending node (%s)" % window[1]
                                dict['saved_windows_list'].append(window)
                                assigned_node = assigned_node + dict['requested_node']
                                assigned_cpu = assigned_cpu + dict['requested_cpu']
                                assigned_memory = assigned_memory + dict['requested_memory']
                                continue
                            else :
                                #print "not appending (%s)" % (window,)
                                #print "not appending (%s)" % (window,)
                                #print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                #print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                #print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                #print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "not appending (%s)" % (window,)
                                    print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                    print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                    print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                    print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
                    continue
        # check for any under-populated counting_dict_list entries
        need_more = 0
        for dict in counting_dict_list :
            #print "dict (%s)" % dict
            if dict['amount_int'] == None :
                if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                    print "dict['amount_int'] == None, need_more = 1"
                #print "dict['amount_int'] == None, need_more = 1"
                need_more = 1
                break
            if len(dict['saved_windows_list']) < dict['amount_int'] :
                #print "len(dict['saved_windows_list']) < dict['amount_int'], need_more = 1"
                #print "len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                #print "dict['saved_windows_list'] (%s)" % (dict['saved_windows_list'],)
                #print "dict['amount_int'] (%s)" % dict['amount_int']
                #print "len(sized_windows_list) (%s)" % len(sized_windows_list)
                if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                    print "len(dict['saved_windows_list']) < dict['amount_int'], need_more = 1"
                    print "len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                    print "dict['amount_int'] (%s)" % dict['amount_int']
                    print "len(sized_windows_list) (%s)" % len(sized_windows_list)
                need_more = 1
                break
            #else :
            #    print "insufficient saved_windows_list (%s) dict['amount_int'] (%s)" % (len(dict['saved_windows_list']), dict['amount_int'])
        #if \
        #  ( max_resource_int != None \
        #  and len(sized_windows_list) >= max_resource_int \
        #  and new_window[0] > current_start_time_float ) \
        #  or ( resource_amount_int != None \
        #  and len(sized_windows_list) >= resource_amount_int \
        #  and new_window[0] > current_start_time_float ) \
        #  :
        #    if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
        #        print "breaking because we have reached target amount"
        #    break
d2035 1
d2060 1
a2060 1
        print "need_more == 1"
d2063 1
d2209 1
d2327 1
a2332 29

    #for window in sorted_windows_list :
    #    nodename = window[2]['nodename']
    #    for counting_dict in counting_dict_list :
    #        if counting_dict['amount_int'] == None :
    #            if counting_dict['resource_dict'].has_key(window[2]['nodename']) :
    #                counting_dict['saved_windows_list'].append(window)
    #                chosen_nodes_list.append(window[2]['nodename'])
    #                break
    #            continue
    #        if len(counting_dict['saved_windows_list']) >= counting_dict['amount_int'] :
    #            continue
    #        else :
    #            if counting_dict['resource_dict'].has_key(window[2]['nodename']) :
    #                counting_dict['saved_windows_list'].append(window)
    #                chosen_nodes_list.append(window[2]['nodename'])
    #                break
    #            continue
    #    if max_resource_int != None and len(chosen_nodes_list) >= max_resource_int :
    #        break
    #    if resource_amount_int != None and len(chosen_nodes_list) >= resource_amount_int :
    #        break
    #need_more = 0
    #for counting_dict in counting_dict_list :
    #    if counting_dict['amount_int'] == None :
    #        continue
    #    if len(counting_dict['saved_windows_list']) < counting_dict['amount_int'] :
    #        need_more = 1
    #        break
@


1.118
log
@removed print
@
text
@d1490 2
d1814 2
a1815 1
        counting_dict_list = map(lambda x : x[1], request_sorting_list)
d2969 5
d3821 1
a3821 1
                print "node_requested_list (%s)" % (node_requested_list,)
d4595 3
@


1.117
log
@*** empty log message ***
@
text
@d2171 1
a2171 1
        print "len(sorted_windows_list) (%s)" % len(sorted_windows_list)
@


1.116
log
@fix for missed backfill
@
text
@d896 7
d907 2
a908 2
                        'cpu' : resources_dict[node]['ConsumableCpus'],
                        'memory' : resources_dict[node]['ConsumableMemory'] }
d3776 8
d3813 1
a3813 1
                #print "node_requested_list (%s)" % (node_requested_list,)
@


1.115
log
@smp
@
text
@d1590 1
d1609 6
d1653 1
a1653 2
        #print "adding this window (%s)" % \
        #  (new_window,)
a1662 2
        # Break if we have reached our target resource_amount_int and the next window
        # starts later than the current crop
d1668 1
d1735 1
d1905 1
d1921 2
a1922 4
        if need_more == 0 :
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "need_more == 0"
            break
d2015 1
a2015 1
        #print "need_more == 1"
d2164 1
a2164 1
        #print "len(sorted_windows_list) (%s)" % len(sorted_windows_list)
d2191 5
d2245 4
d3542 5
d3572 1
d3594 1
a3594 1
                    print "trying a partial..."
d3621 1
d3623 1
d3672 1
d3675 1
a3675 1
                            low_duration = 1
d3682 1
d3697 1
d4562 1
d4866 1
@


1.114
log
@chmod and chown rolled logs
@
text
@d450 1
d464 1
d581 1
d720 1
d722 1
d793 1
d796 1
d851 1
a851 1
def get_open_windows_list(accepted_nodes_list, new_res, temp_reservations_list) :
d860 2
d872 2
d880 7
d889 21
d913 35
a947 1
            for node in node_list :
d949 1
d951 6
a956 1
                        node_reservations[node].append( (start_time_float, end_time_float) )
d958 4
a961 1
                        node_reservations[node] = [ (start_time_float, end_time_float) ]
d964 2
d973 17
a989 1
                reservation_windows.append( (potential_start_time_float, end_limit, accepted_node) )
d997 2
d1002 3
d1009 431
a1439 31
            for reservation in node_reservations[node] :
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "current reservation (%s, %s)" % (time.asctime(time.gmtime(reservation[0])), time.asctime(time.gmtime(reservation[1])))
                if reservation[0] < end_limit :
                    # reservation starts before end_limit
                    if reservation[0] - potential_start_time_float >= \
                    new_res['duration_float'] :
                        # There is enough space ( duration )
                        # between the potential start time and the start of this
                        # blocking node reservation
                        reservation_windows.append( 
                          (potential_start_time_float,
                          reservation[0], node) ) 
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "appending open window (%s, %s, %s)" % (potential_start_time_float, reservation[0], node)
                            print "appending open window (%s, %s, %s)" % (time.asctime(time.gmtime(potential_start_time_float)), time.asctime(time.gmtime(reservation[0])), node)
                else :
                    # reservations starts after end_limit
                    if end_limit - potential_start_time_float >= \
                    new_res['duration_float'] :
                        # There is enough space between the potential
                        # start time and the end_limit for duration
                        reservation_windows.append(
                        (potential_start_time_float,
                        end_limit, node) ) 
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "appending open window (%s, %s, %s)" % (potential_start_time_float, end_limit, node)
                if reservation[1] > potential_start_time_float :
                    # The node reservation ends after the potential start time
                    # advance the potential start time for this node
                    potential_start_time_float = reservation[1]
d1441 7
a1447 7
            last_window = None
            if end_limit >= potential_start_time_float + new_res['duration_float'] :
                last_window = ( potential_start_time_float, end_limit, node )
            if last_window != None :
                reservation_windows.append(last_window)
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "appending open window (%s, %s, %s)" % (potential_start_time_float, end_limit, node)
d1460 2
a1461 1
    input_tuple = (temp_nodes_list, new_res, temp_reservations_list, Now_float)
d1477 3
d1482 1
a1482 1
def get_sized_windows_list(new_res, open_windows_list, resources_db_handle, resource_dict_list = None ) :
d1484 1
d1516 6
d1525 2
a1526 1
            dict[resources_dict[open_window[2]]['name']] = resources_dict[open_window[2]]
d1533 12
a1544 1
    for dict in resource_dict_list :
d1547 48
a1594 6
        sized_dict = {
          'amount_int' : dict['amount_int'],
          'saved_windows_list' : [],
          'resource_dict' : dict['resource_dict']
          }
        sized_dict_list.append(sized_dict)
d1596 1
d1598 4
d1603 5
d1633 3
d1643 2
d1646 2
d1666 152
a1817 1
        for window in sized_windows_list :
d1819 1
a1819 1
                print "start of window for loop for (%s)" % window[2]
d1826 1
a1826 1
                    if dict.has_key(window[2]) :
d1841 50
a1890 5
                    if dict['resource_dict'].has_key(window[2]) :
                        if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                            print "appending node (%s)" % window[2]
                        dict['saved_windows_list'].append(window)
                        break
d1892 1
d1895 1
d1902 5
d1915 2
d1963 6
a1968 1
                            if sized_dict['resource_dict'].has_key(window[2]) :
d1975 1
a1975 1
                            if sized_dict['resource_dict'].has_key(window[2]) :
d1997 2
a1998 1
                print "len(sized_windows_list) (%s)" % len(sized_windows_list)
d2010 1
d2012 2
a2013 1
    return ( sized_windows_list, current_start_time_float )
d2033 1
a2033 1
            if resources_dict[window[2]].has_key('resource_usability_int') :
d2040 1
a2040 1
            result.append(window[-1])
d2064 2
a2065 2
            if resources_dict[window_tuple[0][2]].has_key('resource_usability_int') :
                resource_sort_int = resources_dict[window_tuple[0][2]]['resource_usability_int']
d2069 1
a2069 1
                if DEBUGJOB != None :
d2074 1
a2074 1
            if DEBUGJOB != None :
d2078 1
a2078 1
        if DEBUGJOB != None :
d2103 10
a2112 2
def get_chosen_nodes_list(new_res, sorted_windows_list) :
    #print "in get_chosen"
d2115 2
a2116 2
    resource_dict_list = new_res['resource_dict_list']
    counting_dict_list = resource_dict_list
d2120 147
a2266 10
    for window in sorted_windows_list :
        for counting_dict in counting_dict_list :
            if counting_dict['amount_int'] == None :
                if counting_dict['resource_dict'].has_key(window[2]) :
                    counting_dict['saved_windows_list'].append(window)
                    chosen_nodes_list.append(window[2])
                    break
                continue
            if len(counting_dict['saved_windows_list']) >= counting_dict['amount_int'] :
                continue
d2268 25
a2292 9
                if counting_dict['resource_dict'].has_key(window[2]) :
                    counting_dict['saved_windows_list'].append(window)
                    chosen_nodes_list.append(window[2])
                    break
                continue
        if max_resource_int != None and len(chosen_nodes_list) >= max_resource_int :
            break
        if resource_amount_int != None and len(chosen_nodes_list) >= resource_amount_int :
            break
d2300 1
a2300 1
    return chosen_nodes_list
d2627 52
a2678 50
def cancel_risk_jobs(reservations_db_handle,
                     jobs_db_handle,
                     events_db_handle) :
    jobs_dict = jobs_db_handle[0]
    reservations_dict = reservations_db_handle[0]
    job_steps_list = get_object_list(jobs_db_handle)
    reservations_list = get_object_list(reservations_db_handle)
    for job_step in job_steps_list :
        if ( job_step['state'] == 'Starting' or \
          job_step['state'] == 'Running' ) and \
          job_step.has_key('run_at_risk_int') and \
          job_step['run_at_risk_int'] >= 1 :
            # Check to see if job is run_at_risk, and if so,
            # whether its reservation conflicts with any job
            # reservations
            job_reservations_list = filter( lambda x,job_step=job_step : x['job_runID'] == job_step['name'], reservations_list)
            for job_reservation in job_reservations_list :
                job_node_list = job_reservation['node_list']
                job_end_time = job_reservation['end_time_float']
                job_start_time = job_reservation['start_time_float']
            try :
                for reservation in reservations_list :
                    if not ( reservation.has_key('job_runID') and \
                      jobs_dict.has_key(reservation['job_runID']) and \
                      jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                      jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1 ) and \
                      reservation['start_time_float'] <= \
                      Now_float + RUN_AT_RISK_CLEANUP_TIME and \
                      ( reservation['start_time_float'] < job_end_time <= reservation['end_time_float'] or \
                      reservation['start_time_float'] <= job_start_time < reservation['end_time_float'] ) :
                        for node in job_node_list :
                            if node in reservation['node_list'] :
                                input_tuple = ( job_step, )
                                result = apply_policy_code(
                                  reservation['job_restriction'],
                                  input_tuple)
                                if result != 0 :
                                    raise "FoundConflict"
            except "FoundConflict" :
                # Cancel the job
                if SERVERMODE == 'NORMAL' :
                    try :
                        cancel_job(job_step, events_db_handle)
                    except 'CancelJobFailure', failed_job_step :
                        print "cancel of %s failed" % \
                          failed_job_step['name']
                    except :
                        continue
                else :
                    print "%s should be canceled!" % job_step['name']
d2682 11
a2692 1
                             events_db_handle) :
d2694 16
d2713 96
a2808 36
    for this_res in reservations_list :
        if this_res.has_key('job_runID') and \
          jobs_dict.has_key(this_res['job_runID']) and \
          this_res['purpose_type_string'] != 'running' and \
          ( not jobs_dict[this_res['job_runID']].has_key('start_count_int') or \
          jobs_dict[this_res['job_runID']]['start_count_int'] < 1 ) and \
          jobs_dict[this_res['job_runID']].has_key('run_at_risk_int') and \
          jobs_dict[this_res['job_runID']]['run_at_risk_int'] >= 1 :
            # Check to see if reservation is run_at_risk, and if so,
            # whether it conflicts with any non-run_at_risk
            # reservations
            try :
                for reservation in reservations_list :
                    if not ( reservation.has_key('job_runID') and \
                      jobs_dict.has_key(reservation['job_runID']) and \
                      jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                      jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1 ) and \
                      reservation['start_time_float'] <= \
                      Now_float + RUN_AT_RISK_CLEANUP_TIME and \
                      ( reservation['start_time_float'] < this_res['end_time_float'] <= reservation['end_time_float'] or \
                      reservation['start_time_float'] <= this_res['start_time_float'] < reservation['end_time_float'] ) :
                        for node in this_res['node_list'] :
                            if node in reservation['node_list'] :
                                if DEBUGJOB != None :
                                    print "found conflict for (%s) (%s) (%s)" % (reservation['name'], reservation['purpose_type_string'], reservation['job_runID'])
                                input_tuple = ( jobs_dict[this_res['job_runID']], )
                                result = apply_policy_code(
                                  reservation['job_restriction'],
                                  input_tuple)
                                if result != 0 :
                                    raise "FoundConflict"
            except "FoundConflict" :
                # Cancel this_res
                if DEBUGJOB != None :
                    print "deleting (%s) (%s) (%s)" % (this_res['name'], this_res['purpose_type_string'], this_res['job_runID'])
                delete_object(this_res['name'], reservations_db_handle)
d2844 1
a2844 2
    job_step_list = jobs_dict.values()
    for job_step in job_step_list :
d2852 2
a2853 2
        job_step_resource_dict_list = get_resource_dict_list(job_step, resources_db_handle)
        update_object_attribute('resource_dict_list', job_step_resource_dict_list, job_step, jobs_db_handle)
d3347 2
d3363 2
d3385 6
d3471 20
d3510 1
a3510 1
            print "overlap_running_int found for (%s)" % standing_reservation['name']
d3520 2
a3521 1
        while number_found < depth :
d3538 2
d3574 1
d3584 2
d3615 5
a3619 1
                while full_found == 0 and duration_float > FUDGE_FACTOR :
d3621 1
d3623 1
d3633 2
d3649 9
a3657 1
                        duration_float = duration_float - FUDGE_FACTOR
d3659 7
a3665 1
                        full_found = 1
d3702 1
a3702 1
def update_running_reservations(runningstarting_jobs, reservations_db_handle) :
d3704 1
d3712 7
d3731 14
a3744 1
        node_list = job['allocated_hosts']
d3754 72
d3856 2
d3888 2
d3918 8
d3937 1
d3939 1
d4344 1
a4344 1
    update_running_reservations(runningstarting_jobs, reservations_db_handle)
d4358 4
a4361 3
    cancel_risk_jobs(reservations_db_handle=reservations_db_handle,
                     jobs_db_handle=jobs_db_handle,
                     events_db_handle=events_db_handle)
d4364 2
a4365 1
                     events_db_handle=events_db_handle)
d4407 1
d4410 1
d4450 2
d4483 2
d4505 1
d4514 1
d4522 1
d4532 3
a4534 2
                resource_dict_list[0]['resource_dict'][accepted_node] = resources_db_handle[0][accepted_node]
    new_res['resource_dict_list'] = resource_dict_list
d4543 3
d4549 5
d4557 1
d4568 29
d4600 7
d4615 4
a4618 1
            for node_name in bound_nodes_list :
d4622 53
a4674 23
                new_blocking_res_number = new_blocking_res_number + 1
                pre_blocking_res_name = 'tempblock' + "%s" % new_blocking_res_number
                pre_blocking_res = initialize_reservation(pre_blocking_res_name)
                pre_blocking_res['node_list'] = [node_name,]
                pre_blocking_res['start_time_float'] = 0.0
                pre_blocking_res['end_time_float'] = END_OF_SCHEDULING
                new_blocking_res_number = new_blocking_res_number + 1
                post_blocking_res_name = 'tempblock' + "%s" % new_blocking_res_number
                post_blocking_res = initialize_reservation(post_blocking_res_name)
                post_blocking_res['node_list'] = [node_name,]
                post_blocking_res['start_time_float'] = Now_float
                post_blocking_res['end_time_float'] = END_OF_SCHEDULING
                for reservation in bound_reservations_list :
                    # For each bound reservation, adjust the pre res
                    # end time and the post res start time to reflect
                    # the bound res start and end times, if they extend
                    # the times.
                    if reservation['start_time_float'] < pre_blocking_res['end_time_float'] :
                        pre_blocking_res['end_time_float'] = reservation['start_time_float']
                    if reservation['end_time_float'] > post_blocking_res['start_time_float'] :
                        post_blocking_res['start_time_float'] = reservation['end_time_float']
                temp_blocking_reservations_list.append(pre_blocking_res)
                temp_blocking_reservations_list.append(post_blocking_res)
a4687 1
        reservation_names = job_step['reservation_binding']
d4710 1
d4718 1
d4721 6
d4739 1
a4739 1
        open_windows_list = get_open_windows_list(accepted_nodes_list, new_res, blocking_reservations)
d4779 2
a4780 1
          accepted_nodes_list, new_res, existing_reservations)
d4784 4
a4787 2
    (sized_windows_list, current_start_time_float) = get_sized_windows_list(
      new_res, open_windows_list, resources_db_handle, resource_dict_list=resource_dict_list)
d4790 1
d4794 1
a4794 1
        relevant_nodes_list.append(window[2])
d4829 1
a4829 1
            if window[2] in reservation['node_list'] :
d4836 27
a4862 2
    chosen_nodes_list = get_chosen_nodes_list(new_res, sorted_windows_list )
    #print "after get_chosen"
d4864 3
a4866 2
        print "len(chosen_nodes_list) (%s)" % len(chosen_nodes_list)
        print "resource_amount_int (%s)" % resource_amount_int
d4870 1
@


1.113
log
@*** empty log message ***
@
text
@d3552 2
d3574 2
@


1.112
log
@removed deepcopys
@
text
@d63 1
d335 5
a339 4
if SERVERMODE == 'SIM' :
    Now_float = float(os.system(TIME_SIM))
else :
    Now_float = time.time()
d2432 1
d2487 5
d2527 1
d2530 1
@


1.111
log
@added multiple-requirement resource spec
@
text
@d972 1
d1073 4
a1076 1
        counting_dict_list = copy.deepcopy(sized_dict_list)
d1136 1
a1136 1
        print "getting longest"
d1162 4
a1165 1
                counting_dict_list = copy.deepcopy(sized_dict_list)
d1190 2
a1191 1
                    candidate_tuple_list.append((none_count, counting_dict_list))
d1214 1
d1302 1
d1306 1
a1306 1
    counting_dict_list = copy.deepcopy(resource_dict_list)
d3395 5
a3399 1
                            temp_res = copy.deepcopy(reservation)
d3486 6
a3491 7
                #print "job_step['name'] (%s)" % job_step['name']
                temp_res = copy.deepcopy(reservation)
                #temp_res['affinity_calculation'] = 'result = -1'
                temp_res['affinity_calculation'] = "result = %s" % (- jobs_dict[reservation['job_runID']]['priority'],)
                #print "temp_res['affinity_calculation'] (%s)" % temp_res['affinity_calculation']
                temp_res['job_restriction'] = 'result = 0'
                active_reservations.append(temp_res)
d3513 1
@


1.110
log
@moved in update_job_priorities,
@
text
@d55 1
d448 1
d828 13
a840 13
def get_screened_nodes(job_step_id, accepted_nodes_list, jobs_db_handle, resources_db_handle) :
    jobs_dict = jobs_db_handle[0]
    job_step = jobs_dict[job_step_id]
    if job_step.has_key('resource_list') :
        screened_resource_list = job_step['resource_list']
    else :
        raise 'NoResourceList', job_step
    screened_resource_name_list = get_resource_name_list(screened_resource_list)
    screened_nodes_list = []
    for node in accepted_nodes_list :
        if node in screened_resource_name_list :
            screened_nodes_list.append(node)
    return screened_nodes_list
d971 1
a971 1
def get_sized_windows_list(new_res, open_windows_list) :
d987 6
a992 2
    resource_amount_int = new_res['resource_amount_int']
    max_resource_int = new_res['max_resource_int']
d1003 21
d1025 2
a1026 34
        if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
            print "current_start_time_float (%s) len(sized_windows_list) (%s) new_window[2] (%s)" % \
              ( time.asctime(time.gmtime(current_start_time_float)), len(sized_windows_list), new_window[2] )
            print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
              (time.asctime(time.gmtime(new_window[0])), time.asctime(time.gmtime(new_window[1])), new_window[2])

        # Break if we have reached our target resource_amount_int and the next window
        # starts later than the current crop
        if \
          ( max_resource_int != None \
          and len(sized_windows_list) >= max_resource_int \
          and new_window[0] > current_start_time_float ) \
          or ( resource_amount_int != None \
          and len(sized_windows_list) >= resource_amount_int \
          and new_window[0] > current_start_time_float ) \
          :
            if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
                print "breaking because we have reached target amount"
            break
        # Skip if there is not enough time in this window
        if new_window[1] - current_start_time_float < \
          new_res['duration_float'] :
            if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
                print "skipping this window due to lack of time (%s) - (%s) < (%s)" % \
                  (time.asctime(time.gmtime(new_window[1])), time.asctime(time.gmtime(current_start_time_float)), new_res['duration_float'])
            continue
        # Break if latest_end_float exists, and there is not enough time
        if new_res['latest_end_float'] != None and \
          ( current_start_time_float + \
          new_res['duration_float'] > new_res['latest_end_float'] or \
          new_window[0] + new_res['duration_float'] > new_res['latest_end_float'] ) :
            if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
                print "breaking, not enough time left before end of window"
            break
d1033 2
a1034 2
            if resource_amount_int == None :
                candidate_windows_list = [current_start_time_float, sized_windows_list]
d1043 15
d1060 74
a1133 2
    # Append the last sized_windows_list
    if resource_amount_int == None :
a1135 1
    if resource_amount_int == None :
d1137 58
a1194 3
            longest_candidate = reduce(get_longer, sized_windows_lists_list)
            sized_windows_list = longest_candidate[1]
            current_start_time_float = longest_candidate[0]
d1196 2
d1199 4
d1295 4
d1300 27
a1326 13
    if max_resource_int != None :
        for window in sorted_windows_list :
            chosen_nodes_list.append(window[2])
            if len(chosen_nodes_list) >= max_resource_int :
                break
    elif resource_amount_int != None :
        for window in sorted_windows_list :
            chosen_nodes_list.append(window[2])
            if len(chosen_nodes_list) >= resource_amount_int :
                break
    else :
        for window in sorted_windows_list :
            chosen_nodes_list.append(window[2])
d1545 5
d1775 2
a1776 2
        job_step_resource_list = get_resource_list(job_step, resources_db_handle)
        update_object_attribute('resource_list', job_step_resource_list, job_step, jobs_db_handle)
d1785 2
a1786 2
        if job_step.has_key('resource_list') and len(job_step['resource_list']) > 0 :
            continue
d1792 2
a1793 2
        job_step_resource_list = get_resource_list(job_step, resources_db_handle)
        update_object_attribute('resource_list', job_step_resource_list, job_step, jobs_db_handle)
d1872 10
a1881 2
                if not job.has_key('resource_list') or \
                  len(job['resource_list']) == 0 :
a3011 9
    jr_dict = {}
    for job_name in jobs_dict.keys() :
        jr_dict[job_name] = []
        if jobs_dict[job_name].has_key('resource_list') :
            for resource in jobs_dict[job_name]['resource_list'] :
                jr_dict[job_name].append(resource['name'])
        else :
            if DEBUGJOB != None and job_name == DEBUGJOB :
                print "%s has not 'resource_list'" % job_name
a3012 1
        job_count = 0
d3015 4
a3018 3
            if jobs_dict[job_name].has_key('resource_list') and resource_name in jr_dict[job_name] :
                #job_count = job_count + 1
                node_seconds_int = node_seconds_int + long(jobs_dict[job_name]['resource_amount_int']) * long(jobs_dict[job_name]['wall_clock_limit'])
d3134 27
d3174 1
d3279 11
d3356 5
a3360 5
        screened_nodes = get_screened_nodes(
            job_step['name'], accepted_nodes_list, jobs_db_handle,
            resources_db_handle)
        if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
            print "len(screened_nodes) (%s)" % len(screened_nodes)
d3405 5
a3409 1
        open_windows_list = get_open_windows_list(screened_nodes, new_res, blocking_reservations)
d3446 1
d3450 1
a3450 1
      new_res, open_windows_list)
d3499 1
a3499 1
    chosen_nodes_list = get_chosen_nodes_list(new_res, sorted_windows_list)
d3501 2
@


1.109
log
@added overlap_running option, fixed migrate_shortpools
@
text
@a56 1
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import update_job_priorities
d227 29
a255 1
RESOURCE_WEIGHT = float(RESOURCE_WEIGHT)
d258 1
a258 1
        LOCAL_ADMIN_WEIGHT = float(LOCAL_ADMIN_WEIGHT)
d260 1
a260 1
        LOCAL_ADMIN_WEIGHT = 0.0
d262 1
a262 1
    LOCAL_ADMIN_WEIGHT = 0.0
d265 1
a265 1
        LOCAL_USER_WEIGHT = float(LOCAL_USER_WEIGHT)
d267 1
a267 1
        LOCAL_USER_WEIGHT = 0.0
d269 1
a269 1
    LOCAL_USER_WEIGHT = 0.0
d272 1
a272 1
        WALL_TIME_WEIGHT = float(WALL_TIME_WEIGHT)
d274 1
a274 1
        WALL_TIME_WEIGHT = 0.0
d276 8
a283 7
    WALL_TIME_WEIGHT = 0.0
EXPANSION_FACTOR_WEIGHT = float(EXPANSION_FACTOR_WEIGHT)
SYSTEM_QUEUE_TIME_WEIGHT = float(SYSTEM_QUEUE_TIME_WEIGHT)
SUBMIT_TIME_WEIGHT = float(SUBMIT_TIME_WEIGHT)
QOS_PRIORITY_WEIGHT = float(QOS_PRIORITY_WEIGHT)
QOS_TARGET_EXPANSION_FACTOR_WEIGHT = float(QOS_TARGET_EXPANSION_FACTOR_WEIGHT)
QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT = float(QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT)
d334 1
a334 1
    Now_float = os.system(TIME_SIM)
d1326 163
d3267 1
a3267 1
                temp_res['affinity_calculation'] = "result = %iL" % (- jobs_dict[reservation['job_runID']]['priority'],)
@


1.108
log
@treat running and starting run_at_risk jobs differently from
non-running run_at_risk jobs.  give the running jobs a cleanup
time.
@
text
@d305 4
a308 1
Now_float = time.time()
d427 1
d1904 8
d1927 1
d1950 2
d1992 6
a1997 1
          Now_float + FUDGE_FACTOR/2 >= reservation['start_time_float']) :
d2053 10
a2062 7
        for reservation in running_reservations_list :
            if not jobs_dict.has_key(reservation['job_runID']) :
                continue
            input_tuple = ( jobs_dict[reservation['job_runID']], )
            result = apply_policy_code(job_restriction, input_tuple)
            if result == 0 :
                ignore_reservations_list.append(reservation['name'])
d2281 2
a2282 1
  node_restriction='result = 1'
d2309 1
a2754 2
    print "migrating shortpools"
    migrate_shortpools(jobs_db_handle, resources_db_handle, reservations_db_handle)
d2764 2
d2993 2
a2994 1
                              reservation['purpose_type_string'] == 'job' and \
d2996 1
a2996 1
                              jobs_dict[reservation['job_runID']]['start_count_int'] >= 1 :
d3030 1
d3035 1
a3035 1
                      reservation['purpose_type_string'] == 'job' and \
d3037 1
a3037 1
                      jobs_dict[reservation['job_runID']]['start_count_int'] >= 1 :
d3067 1
@


1.107
log
@*** empty log message ***
@
text
@d1320 1
a1320 1
                      reservation['start_time_float'] < \
d1369 1
a1369 1
                      reservation['start_time_float'] < \
d2967 7
a2973 1
                            temp_res['end_time_float'] = Now_float
d3007 7
a3013 1
                    temp_res['end_time_float'] = Now_float
@


1.106
log
@don't cancel risk jobs or reservations if the relevant job passes
the reservation job_restriction
@
text
@d1313 1
d1320 1
a1320 1
                      reservation['start_time_float'] <= \
d1322 2
a1323 1
                      reservation['start_time_float'] < job_end_time :
d1355 3
d1369 1
a1369 1
                      reservation['start_time_float'] <= \
d1371 2
a1372 1
                      reservation['start_time_float'] < this_res['end_time_float'] :
d2967 1
a2967 1
                            temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
d3001 1
a3001 1
                    temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
@


1.105
log
@added cancel_risk_reservations
@
text
@d1319 1
a1319 1
                      reservation['start_time_float'] < \
d1324 6
a1329 15
                                raise "FoundConflict"
                    #else :
                    #    if not reservation.has_key('job_runID') :
                    #        print "no job_runID"
                    #    elif not jobs_dict.has_key(reservation['job_runID']) :
                    #        print "not in jobs_dict"
                    #    elif not jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') :
                    #        print "job_runID (%s)" % reservation['job_runID']
                    #        print "no run_at_risk"
                    #    elif not jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1 :
                    #        print "not >= 1"
                    #    if reservation['start_time_float'] >=  Now_float + RUN_AT_RISK_CLEANUP_TIME :
                    #        print "still too early"
                    #    if reservation['start_time_float'] >= job_end_time :
                    #        print "does not conflict"
d1364 1
a1364 1
                      reservation['start_time_float'] < \
d1369 8
a1376 1
                                raise "FoundConflict"
d1379 2
d2961 1
a2961 1
                            temp_res['end_time_float'] = Now_float
d2995 1
a2995 1
                    temp_res['end_time_float'] = Now_float
@


1.104
log
@removed CLEANUP_TIME from temp_res
@
text
@d1352 31
d2740 3
@


1.103
log
@added ]''s
@
text
@d1325 14
d2927 1
a2927 1
                            temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
d2961 1
a2961 1
                    temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
@


1.102
log
@added '
@
text
@d1317 2
a1318 2
                      jobs_dict[reservation['job_runID'].has_key('run_at_risk_int') and \
                      jobs_dict[reservation['job_runID']['run_at_risk_int'] >= 1 ) and \
@


1.101
log
@don't cancel run_at_risk job for a run_at_risk job
@
text
@d1315 1
a1315 1
                    if not ( reservation.has_key('job_runID) and \
@


1.100
log
@added run_at_risk
@
text
@d1315 5
a1319 1
                    if reservation['start_time_float'] < \
@


1.99
log
@added catsyslog function
@
text
@d192 14
d1293 41
a1333 1
    
a1596 27
def create_system_reservation(
  resources_db_handle,
  jobs_db_handle,
  reservations_db_handle,
  start_float=Now_float,
  duration_float=7776000) :
    earliest_start_float = start_float
    existing_reservations = get_object_list(reservations_db_handle)
    ignore_reservations_list = get_object_names_list(existing_reservations)
    job_restriction = 'result = 1'
    node_restriction = 'result = 0'
    create_reservation(
      resources_db_handle=resources_db_handle,
      reservations_db_handle=reservations_db_handle,
      jobs_db_handle=jobs_db_handle,
      earliest_start_float=earliest_start_float,
      duration_float=duration_float,
      job_restriction=job_restriction,
      node_restriction=node_restriction,
      node_sort_policy=sort_policy,
      conflict_policy=conflict_policy,
      purpose_type_string=purpose_type_string,
      ignore_reservations_list=ignore_reservations_list,
      comment_string=comment_string,
      mode='real'
    )
    
d2246 4
a2249 1
    duration_float = job_step['wall_clock_limit'] + FUDGE_FACTOR
d2689 4
d2815 5
d2897 20
a2916 1
                blocking_reservations.append(reservation)
d2937 11
a2947 1
            existing_reservations = non_job_reservations
d2958 26
a2983 6
    if purpose_type_string == 'job' :
        relevant_nodes_list = []
        for window in sized_windows_list :
            relevant_nodes_list.append(window[2])
        active_reservations = []
        for reservation in existing_reservations :
d2985 3
a2987 2
              and reservation['start_time_float'] <= current_start_time_float \
              <= reservation['end_time_float'] :
d2989 9
a2997 10
        windows_and_reservations_list = []
        for window in sized_windows_list :
            relevant_reservations_list = []
            for reservation in active_reservations :
                if window[2] in reservation['node_list'] :
                    relevant_reservations_list.append(reservation)
            windows_and_reservations_list.append((window, relevant_reservations_list, job_step))
    else :
        windows_and_reservations_list = None
    # job_step gets passed in, or defaults to None
@


1.98
log
@added DB filesize and valid pickle tests
@
text
@d188 4
d2980 10
@


1.97
log
@removed prints
@
text
@d508 28
a535 3
        FO = open(REMOTEFILE, 'r')
        dict = cPickle.load(FO)
        FO.close()
@


1.96
log
@added more per_user code
@
text
@a269 1
    print "No QOS_MAXNODESECRUNNINGPERUSERPOLICY_STRING"
a277 1
    print "No QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_STRING"
@


1.95
log
@initialized active_reserved_node_sec_per_user_per_qos
@
text
@d263 9
d1327 2
d1430 23
@


1.94
log
@initialize running_node_sec_per_user_per_qos
@
text
@d2230 1
@


1.93
log
@fixed bug with running jobs counted twice in create_job_reservations
@
text
@d2189 1
@


1.92
log
@added the section for NODESEC in create_job_reservations
@
text
@d2206 5
a2210 2
            if running_jobs_per_user.has_key(user) :
                running_jobs_per_user[user] = running_jobs_per_user[user] + 1
d2212 1
a2212 1
                running_jobs_per_user[user] = 1
d2275 7
d2347 23
@


1.91
log
@fixed vars() problem
@
text
@d1418 1
a1418 1
                                job['ineligible_reason'] = 'MAXJOBQUEUEDPERACCOUNTPOLICY'
d1429 1
a1429 1
                            job['ineligible_reason'] = 'MAXNODESECRUNNINGPERACCOUNTPOLICY'
d1441 1
a1441 1
                                job['ineligible_reason'] = 'MAXNODESECQUEUEDPERACCOUNTPOLICY'
d2188 1
d2206 11
d2225 1
d2272 7
d2347 23
d2399 7
@


1.90
log
@added MAXNODESECQUEUED and MAXNODESECRUNNING policies
@
text
@d263 9
d1421 1
a1421 1
            if vars(Catalina).has_key('QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict') and \
d1432 1
a1432 1
            if vars(Catalina).has_key('QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict') and \
@


1.89
log
@added blocking_res_list parameter to create_reservation
@
text
@d1309 2
d1339 8
d1412 23
d1450 8
@


1.88
log
@changed from flock to lockf
@
text
@d2467 2
a2468 2
  resources_db_handle,
  reservations_db_handle,
d2488 1
d2665 1
a2665 1
            blocking_reservations = blocking_reservations + temp_blocking_reservations_list
d2684 1
@


1.87
log
@for DB warn mail, add exception info
@
text
@a368 1
    new_reservation['privacy_type_string'] = None
a378 1
    new_reservation['privacy_type_string'] = None
d416 1
a416 1
        fcntl.flock(LOCKFO.fileno(), fcntl.LOCK_EX)
d420 1
a420 1
        fcntl.flock(LOCKFO.fileno(), fcntl.LOCK_SH)
d448 1
a448 1
    fcntl.flock(LOCKFO.fileno(), fcntl.LOCK_UN)
a2057 1
  privacy_type_string,
a2482 1
  privacy_type_string=None,
a2559 1
    new_res['privacy_type_string'] = 'private'
a2571 1
    new_res['privacy_type_string'] = privacy_type_string
@


1.86
log
@don't warn about lost jobs, if job was found in Completed, Canceled or Removed
@
text
@d499 5
d507 2
a508 1
db: (%s)""" % REMOTEFILE
@


1.85
log
@if available inodes == -1, then don't raise objection
@
text
@d2172 2
@


1.84
log
@backout install and db dir
DBDIR
@
text
@d2737 1
a2737 1
    if archive_stat[statvfs.F_FAVAIL] < 100000 :
@


1.83
log
@removed SG_CLASS_TO_QOS_dict
@
text
@d148 1
d152 2
a153 1
  'HOMEDIR' : '___HOMEDIR_PLACEHOLDER___'
d413 1
a413 1
    LOCK = HOMEDIR + '/' + db_name + LOCK_SUFFIX
d429 1
a429 1
    LOCK = HOMEDIR + '/' + db_name + LOCK_SUFFIX
d455 1
a455 1
    LOCK = HOMEDIR + '/' + db_name + LOCK_SUFFIX
d463 1
a463 1
    FILE = HOMEDIR + '/' + db_name
d473 1
a473 1
    ROLOCK = HOMEDIR + '/' + RO_DB_NAME + LOCK_SUFFIX
d479 1
a479 1
    ROFILE = HOMEDIR + '/' + RO_DB_NAME
d491 1
a491 1
    REMOTEFILE = HOMEDIR + '/' + db_name
d507 1
a507 1
            ROFILE = HOMEDIR + '/' + RO_DB_NAME
d545 1
a545 1
    ROFILE = HOMEDIR + '/' + RO_DB_NAME
d580 1
a580 1
        REMOTEFILE = HOMEDIR + '/' + db_name
d588 1
a588 1
        ROFILE = HOMEDIR + '/' + db_name + '_readonly'
d2743 1
a2743 1
        file_stat = os.stat(HOMEDIR + '/' + db_name)
@


1.82
log
@fixed many bugs with policies
@
text
@a245 1
exec('SG_CLASS_TO_QOS_dict = ' + SG_CLASS_TO_QOS_DICT_STRING)
@


1.81
log
@added global MAXJOBPERUSER
@
text
@a1353 2
                else :
                    running_jobs_per_user[user] = 1
a1361 2
                else :
                    running_jobs_per_user[user] = 1
a1370 4
                    else :
                        running_jobs_per_account_per_qos[account][qos] = 1
                else :
                    running_jobs_per_account_per_qos[account] = { qos : 1, }
a1378 5
                    else :
                        queued_jobs_per_user[user] = \
                          queued_jobs_per_user[user] + 1
                else :
                    queued_jobs_per_user[user] = 1
a1386 5
                    else :
                        queued_jobs_per_user[user] = \
                          queued_jobs_per_user[user] + 1
                else :
                    queued_jobs_per_user[user] = 1
a1396 7
                        else :
                            queued_jobs_per_account_per_qos[account][qos] = \
                              queued_jobs_per_account_per_qos[account][qos] + 1
                    else :
                        queued_jobs_per_account_per_qos[account][qos] = 1
                else :
                    queued_jobs_per_account_per_qos[account] = { qos : 1, }
d1400 12
@


1.80
log
@added attempt to recover db files from _readonly files
@
text
@d246 1
d1346 10
d1361 1
a1361 1
                        job['ineligible_reason'] = 'MAXJOBPERUSERPOLICY'
d1379 13
d1397 1
a1397 1
                        job['ineligible_reason'] = 'MAXJOBQUEUEDPERUSERPOLICY'
d2219 21
d2250 1
a2250 1
              'MAXJOBPERUSERPOLICY',
@


1.79
log
@fixes to MAXJOBPERACCOUNT code
@
text
@d190 5
d490 47
a536 3
    FO = open(REMOTEFILE, 'r')
    dict = cPickle.load(FO)
    FO.close()
@


1.78
log
@added checks for QOS_MAXJOBPERACCOUNTPOLICY_dict.has_key(job_step['QOS'])
and QOS_MAXJOBPERUSERPOLICY_dict.has_key(job_step['QOS'])
@
text
@d1252 2
a1253 2
        running_jobs_per_account = {}
        queued_jobs_per_account = {}
d1264 1
d1274 6
a1279 3
                if running_jobs_per_account.has_key(account) :
                    running_jobs_per_account[account] = \
                      running_jobs_per_account[account] + 1
d1281 1
a1281 1
                    running_jobs_per_account[account] = 1
d1288 2
d1307 10
a1316 7
                if running_jobs_per_account.has_key(account) :
                    if running_jobs_per_account[account] >= \
                      QOS_MAXJOBPERACCOUNTPOLICY_dict[job['QOS']] :
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'MAXJOBPERACCOUNTPOLICY'
                        ineligible_jobs.append(job)
                        continue
d1318 1
a1318 1
                    running_jobs_per_account[account] = 1
d1333 12
a1344 7
                if queued_jobs_per_account.has_key(account) :
                    if queued_jobs_per_account[account] >= \
                      QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict[job['QOS']] :
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'MAXJOBQUEUEDPERACCOUNTPOLICY'
                        ineligible_jobs.append(job)
                        continue
d1346 1
a1346 2
                        queued_jobs_per_account[account] = \
                          queued_jobs_per_account[account] + 1
d1348 1
a1348 1
                    queued_jobs_per_account[account] = 1
d2073 1
a2073 1
    running_jobs_per_account = {}
d2078 1
d2084 5
a2088 2
            if running_jobs_per_account.has_key(account) :
                running_jobs_per_account[account] = running_jobs_per_account[account] + 1
d2090 1
a2090 1
                running_jobs_per_account[account] = 1
d2098 1
a2098 1
    active_reservations_per_account = {}
d2136 5
a2140 2
                if active_reservations_per_account.has_key(active_res_account) :
                    active_reservations_per_account[active_res_account].append(reservation['job_runID'])
d2142 1
a2142 1
                    active_reservations_per_account[active_res_account] = [reservation['job_runID'],]
d2169 3
a2171 2
          ( running_jobs_per_account.has_key(job_step['account']) and \
          running_jobs_per_account[job_step['account']] >= \
d2173 3
a2175 2
          active_reservations_per_account.has_key(job_step['account']) and \
          len(active_reservations_per_account[job_step['account']]) >= \
d2212 5
a2216 2
                if running_jobs_per_account.has_key(job_step['account']) :
                    running_jobs_per_account[job_step['account']] = running_jobs_per_account[job_step['account']] + 1
d2218 1
a2218 1
                    running_jobs_per_account[job_step['account']] = 1
d2224 6
a2229 3
                if active_reservations_per_account.has_key(job_step['account']) :
                    if not job_step['name'] in active_reservations_per_account[job_step['account']] :
                        active_reservations_per_account[job_step['account']].append(job_step['name'])
d2231 1
a2231 1
                    active_reservations_per_account[job_step['account']] = [job_step['name'],]
@


1.77
log
@fixed more places, where MAXJOBPERACCOUNT code needs to go
@
text
@d2126 2
a2127 1
        if QOS_MAXJOBPERUSERPOLICY_dict[job_step['QOS']] != None and \
d2147 2
a2148 1
        if QOS_MAXJOBPERACCOUNTPOLICY_dict[job_step['QOS']] != None and \
@


1.76
log
@added JOB_START_WARN_LIMIT
@
text
@d1303 1
a1303 1
                      QOS_MAXJOBPERUSERPOLICY_dict[job['QOS']] :
d2060 1
d2064 1
d2070 4
d2081 1
d2114 1
d2119 4
d2146 20
d2188 4
d2197 5
@


1.75
log
@default affinity is 1 (positive affinity towards reserved nodes)
some SIM mode code
@
text
@d186 4
d2086 5
a2090 1
              or LOST_JOB_WARN == 'TRUE' ) and SERVERMODE == 'NORMAL' and Now_float > reservation['start_time_float'] + JOB_START_TIME_LIMIT :
d2094 4
d2099 1
@


1.74
log
@check for non-existence of resource_usability_int
@
text
@d173 4
d244 8
d302 2
d305 2
d309 2
d959 1
a959 1
                    result = -1
d1248 2
d1259 1
d1269 5
d1296 10
d1319 13
@


1.73
log
@MINRESOURCE
@
text
@d912 4
a915 1
            temp_windows.append((- (window[0]), long(resources_dict[window[2]]['resource_usability_int']), window))
d943 4
d949 2
a950 2
                    print "sort components (%s) (%s)" % (window[0], resources_dict[window_tuple[0][2]]['resource_usability_int'])
                sort_tuple = ( 0, -(window[0]), long(resources_dict[window_tuple[0][2]]['resource_usability_int']), window)
d952 1
a952 1
                sort_tuple = (-(affinity), -(window[0]), long(resources_dict[window_tuple[0][2]]['resource_usability_int']), window)
@


1.72
log
@check for LOST_JOB_WARN or job state not Unknown before warning
about exceeding JOB_START_TIME_LIMIT
@
text
@d896 6
d903 3
d909 8
a916 3
        windows.sort()
        windows.reverse()
        result = windows
d920 12
a931 17
        def sortfunc (first, second) :
            first_relevant_reservations_list = first[1]
            second_relevant_reservations_list = second[1]
            job_step = first[2]
            first_affinity = 0
            second_affinity = 0
            for reservation in first_relevant_reservations_list :
                # affinity calculation should be a block of python
                # code that uses input_tuple[0] as job_step, and
                # sets result to a float value, for example:
                # new_job_step = input_tuple[0]
                # if new_job_step['user'] == 'kenneth' :
                #     result = 100.0
                # elif string.atof(new_job_step['QOS']) < 4 :
                #     result = -50.0
                # else :
                #     result = 1.0
a937 2
                    # The default of -1 when no affinity is specified means
                    # that jobs will _avoid_ nodes with generic reservations.
d939 14
a952 28
                first_affinity = first_affinity + result
            for reservation in second_relevant_reservations_list :
                input_tuple = (job_step,)
                if reservation['affinity_calculation'] != None :
                    result = 0
                    reservation_affinity_code = reservation['affinity_calculation']
                    exec reservation_affinity_code in globals(), locals()
                else :
                    result = -1
                second_affinity = second_affinity + result
            if first_affinity > second_affinity :
                return -1
            if first_affinity < second_affinity :
                return 1
            # affinities are equal so order by last_available
            if first[0][0] > second[0][0] :
                return -1
            if first[0][0] == second[0][0] :
                return 0
            if first[0][0] < second[0][0] :
                return 1
        windows = input_tuple[0]
        nodes = input_tuple[1]
        new_res = input_tuple[2]
        windows_and_reservations_list = input_tuple[4]
        # windows_and_reservations_list is a list of tuples, containing
        # (window, list of relevant reservations, job_step)
        windows_and_reservations_list.sort(sortfunc)
d954 2
a955 2
        for window_tuple in windows_and_reservations_list :
            windows.append(window_tuple[0])
d968 5
d2112 23
d2213 2
@


1.71
log
@changed many *.close to *.close()
@
text
@d2026 3
a2028 1
            if SERVERMODE == 'NORMAL' and Now_float > reservation['start_time_float'] + JOB_START_TIME_LIMIT :
@


1.70
log
@added max_resource_int
@
text
@d439 1
a439 1
    FO.close
d455 1
a455 1
    ROFO.close
d465 1
a465 1
    FO.close
d479 1
a479 1
            ROFO.close
d514 1
a514 1
        FO.close
d2513 1
a2513 1
        FO.close
@


1.69
log
@only print out chown errors if DEBUG == 'locks'
@
text
@d818 1
d838 8
a845 3
        if resource_amount_int != None \
        and len(sized_windows_list) >= resource_amount_int \
        and new_window[0] > current_start_time_float :
d980 7
a986 1
    if resource_amount_int != None :
d2240 2
a2241 1
  mode='lookahead'
d2257 3
d2317 1
@


1.68
log
@fixed some debug print statements
@
text
@d409 9
a417 8
            info_tuple = sys.exc_info()
            print "(%s) (%s) (%s)" % info_tuple
            info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
            traceback.print_tb(info_tuple[2])
            tb_list = traceback.format_tb(info_tuple[2])
            info_list = info_list + tb_list
            tb_text = string.join(info_list)
            print tb_text
d526 9
a534 8
                info_tuple = sys.exc_info()
                print "(%s) (%s) (%s)" % info_tuple
                info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                traceback.print_tb(info_tuple[2])
                tb_list = traceback.format_tb(info_tuple[2])
                info_list = info_list + tb_list
                tb_text = string.join(info_list)
                print tb_text
@


1.67
log
@modified get_broken_reservations_tuple to check if running jobs
are allowed by reservation
@
text
@d745 1
a745 1
                            print "appending open window (%s, %s, %s)" % (time.asctime(time.gmtime(potential_start_time_float)), time.asctime(time.gmtime(reservation[0])), accepted_node)
d846 1
a846 1
                  (time.asctime(time.gmtime(new_window[1])), time.asctime(time.gmtime(current_start_time_float)), time.asctime(time.gmtime(new_res['duration_float'])))
@


1.66
log
@put in check for ROFO.close, changed to ROFO.close()
chown and chmod regular, readonly and lock files
@
text
@d549 1
d561 1
d585 8
@


1.65
log
@removed LOCALDIR in required config dictionary
@
text
@d399 2
d402 17
d472 1
d480 2
a481 1
            ROFO.close
d518 1
a518 2
        current_uid = os.getuid()
        if current_uid == uid_int :
d523 12
@


1.64
log
@do not chmod unless owner of files
@
text
@a86 1
  'LOCALDIR' : None,
@


1.63
log
@do os.getuid() check before chowning db files in close_db
@
text
@a497 2
        os.chmod(REMOTEFILE,0664)
        os.chmod(ROFILE,0664)
d500 2
@


1.62
log
@don't use local disk cache
@
text
@d500 4
a503 2
        os.chown(REMOTEFILE,uid_int,gid_int)
        os.chown(ROFILE,uid_int,gid_int)
@


1.61
log
@fixed unbind_reservation
@
text
@d444 1
a444 7
    LOCALFILE = LOCALDIR + '/' + db_name
    if os.access(LOCALDIR, os.W_OK) :
        shutil.copyfile(REMOTEFILE, LOCALFILE)
        FILE = LOCALFILE
    else :
        FILE = REMOTEFILE
    FO = open(FILE, 'r')
d487 1
a487 9
        LOCALFILE = LOCALDIR + '/' + db_name
        if os.access(LOCALDIR, os.W_OK) :
            shutil.copyfile(REMOTEFILE, LOCALFILE)
            FILE = LOCALFILE
            cache = 'local'
        else :
            FILE = REMOTEFILE
            cache = 'remote'
        FO = open(FILE, 'w')
a493 2
        if cache == 'local' :
            shutil.copyfile(FILE, REMOTEFILE)
d496 6
@


1.60
log
@added CANCELCMD
@
text
@d1020 6
d1027 1
@


1.59
log
@added bind_reservation_to_job, unbind_reservation_from_job
@
text
@a63 1
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import CANCELCMD
d141 1
@


1.58
log
@many changes to support test info in catalina.config
@
text
@d340 1
a979 9
def bind_res_to_job(res_id_string, db_handle, job_restriction_code) :
    res_shelf = db_handle[0]
    if res_shelf.has_key(res_id_string) :
        temp_reservation = copy.copy(res_shelf[res_id_string])
        temp_reservation['job_restriction'] = job_restriction_code
        insert_new_object(temp_reservation, db_handle)
    else :
        print "res_id_string (%s) does not exist!" % res_id_string

d997 24
@


1.57
log
@added SCHEDULING_WINDOW
@
text
@a63 2
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import TESTJOB
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import SUBMITCMD
a113 1
  'SCHEDULING_WINDOW' : None,
d130 12
d225 10
a246 7
if vars().has_key('SCHEDULING_WINDOW') :
    try :
        SCHEDULING_WINDOW = float(SCHEDULING_WINDOW)
    except :
        SCHEDULING_WINDOW = 7776000.0
else :
    SCHEDULING_WINDOW = 7776000.0
d250 1
a250 1
END_OF_SCHEDULING = Now_float + SCHEDULING_WINDOW
d697 2
d711 1
a711 1
                            print "appending open window (%s, %s, %s)" % (time.asctime(time.gmtime(potential_start_time_float)), time.asctime(time.gmtime(end_limit)), accepted_node)
d816 1
a816 1
          current_start_time_float + \
d818 1
a818 1
          new_window[0] + new_res['duration_float'] > new_res['latest_end_float'] :
d2296 1
a2296 1
                pre_blocking_res['start_time_float'] = Now_float
@


1.56
log
@added notify_count_int to initialize_res
@
text
@d116 1
d228 7
d238 1
a238 1
END_OF_SCHEDULING = Now_float + 7776000
@


1.55
log
@add notification and get_broken_reservations_tuple
@
text
@d344 1
@


1.54
log
@Do not filter out job reservations for new user_set reservation
@
text
@d344 1
d501 51
d2146 1
d2231 1
@


1.53
log
@fixed end_float bug
@
text
@d2277 5
a2281 4
        non_job_reservations = filter(
          lambda reservation : reservation['purpose_type_string'] \
          != 'job' or reservation['start_count_int'] > 0, existing_reservations)
        existing_reservations = non_job_reservations
@


1.52
log
@fixed vars.has_key
@
text
@d741 3
a743 2
          new_window[0] + \
          new_res['duration_float'] > new_res['latest_end_float'] :
@


1.51
log
@added stuff for user_set_res
don't delete job reservations
@
text
@d223 1
a223 1
if vars.has_key('USER_SET_LIMITS_DICT_STRING') :
d225 1
a225 1
if vars.has_key('CLASS_PRIORITY_DICT_STRING') :
@


1.50
log
@changed LOST_JOB_WARN to caps
@
text
@d214 1
d223 4
a2115 12
    #if DEBUG == 'usr' :
    #    print "in create_reservation after get_object_list, existing running reservations:"
    #    for reservation in existing_reservations :
    #        if reservation['purpose_type_string'] == 'running' :
    #            print "%s: (%s) (%s) (%s) (%s) (%s) (%s)" % (
    #              reservation['name'],
    #              time.asctime(time.gmtime(reservation['start_time_float'])),
    #              time.asctime(time.gmtime(reservation['end_time_float'])),
    #              reservation['job_restriction'],
    #              reservation['purpose_type_string'],
    #              reservation['comment_string'],
    #              reservation['node_list'] )
d2123 5
a2127 15
    if purpose_type_string != 'job' :
        jobs_dict = jobs_db_handle[0]
        for reservation in existing_reservations :
            if reservation['purpose_type_string'] == 'job' and \
              ( not reservation.has_key('start_count_int') or \
              not reservation['start_count_int'] >= 1 or \
              Now_float > reservation['start_time_float'] + JOB_START_TIME_LIMIT or \
              ( jobs_dict.has_key(reservation['job_runID']) and \
              jobs_dict[reservation['job_runID']]['state'] in ['Running', 'Removed', 'Canceled', 'Completed']) ):
                delete_object(reservation['name'], reservations_db_handle)
                if SERVERMODE == 'NORMAL' and Now_float > reservation['start_time_float'] + JOB_START_TIME_LIMIT :
                    message = "job (%s) has exceeded JOB_START_TIME_LIMIT, reservation deleted" % reservation['job_runID']
                    recipient = MAIL_RECIPIENT
                    subject = "Catalina job reservation timed out"
                    warn(message, subject, recipient)
d2275 1
d2278 1
a2278 1
          != 'job', existing_reservations)
@


1.49
log
@LOST_JOB_WARN, move_old, DEBUG modified
@
text
@d996 1
a996 1
            if LOST_JOB_WARN == 'True' and old_job_step['state'] == 'Running' :
@


1.48
log
@initialize start_count_int to 0
@
text
@d174 2
d606 1
d631 1
a631 1
                        #print "appending open window (%s, %s, %s)" % (time.asctime(time.gmtime(potential_start_time_float)), time.asctime(time.gmtime(end_limit)), accepted_node)
d656 5
d678 5
d716 2
d724 2
d730 3
d738 2
d927 1
d932 2
a933 2
        if job_step['name'] not in new_job_steps_names_list \
          or new_job_steps_dict[job_step['name']]['state'] in ['Canceled', 'Completed', 'Removed'] :
d996 1
a996 1
            if old_job_step['state'] == 'Running' :
d2221 1
d2254 4
d2282 5
@


1.47
log
@use start_count_int instead of start_returncode_int
@
text
@d336 1
a336 1
    new_reservation['start_count_int'] = None
d1847 1
a1847 1
          reservation['start_count_int'] != 1 or \
d2112 1
a2112 1
              reservation['start_count_int'] != 1 or \
d2171 1
a2171 1
    new_res['start_count_int'] = None
@


1.46
log
@changed order of active reservation count in create_job_reservations
@
text
@d336 1
d1846 2
a1847 2
          ( not reservation.has_key('start_returncode_int') or \
          reservation['start_returncode_int'] != 0 or \
d2111 2
a2112 2
              ( not reservation.has_key('start_returncode_int') or \
              reservation['start_returncode_int'] != 0 or \
d2171 1
a2171 1
    new_res['start_returncode_int'] = None
@


1.45
log
@remember started job
@
text
@a1843 9
        if ( reservation['purpose_type_string'] == 'job' \
          or reservation['purpose_type_string'] == 'running' ) \
          and jobs_dict.has_key(reservation['job_runID']) :
            if (Now_float + 1) >= reservation['start_time_float'] :
                active_res_owner = jobs_dict[reservation['job_runID']]['user']
                if active_reservations_per_user.has_key(active_res_owner) :
                    active_reservations_per_user[active_res_owner].append(reservation['job_runID'])
                else :
                    active_reservations_per_user[active_res_owner] = [reservation['job_runID'],]
d1857 10
@


1.44
log
@moved update_standing_reservations to just before create_job_reservations
this way, both update_job_info and update_running_reservations happens
before update_standing_reservations.
@
text
@d1853 6
a1858 1
        if reservation['purpose_type_string'] == 'job' :
d1860 6
d1890 8
d2106 1
d2109 5
a2113 1
              reservation['start_time_float'] > (Now_float + FUDGE_FACTOR/2) :
d2115 5
a2119 1

d2169 1
@


1.43
log
@modified update_standing_reservations to consider jobs
starting up to FUDGE_FACTOR/2 in the future as already running.
This matches create_reservation.
@
text
@a1986 7
    print "updating standing reservations"
    update_standing_reservations(
      events_db_handle=events_db_handle,
      jobs_db_handle=jobs_db_handle,
      resources_db_handle=resources_db_handle,
      reservations_db_handle=reservations_db_handle,
      standing_reservations_db_handle=standing_reservations_db_handle)
d2006 7
@


1.42
log
@added warning for decrease in available nodes
@
text
@d1473 1
a1473 1
          Now_float >= reservation['start_time_float']) :
@


1.41
log
@fixed problem with update_standing_res not taking last available
@
text
@d1925 1
d1928 1
d1941 18
@


1.40
log
@update standing reservations every iteration
@
text
@d275 4
a278 4
    if first[0][2] in first[2] and not second[0][2] in second[2] :
        return -1
    if second[0][2] in second[2] and not first[0][2] in first[2] :
        return 1
d290 1
a290 1
original_nodes = ___ORIGINAL_NODE_PLACEHOLDER___
d293 2
a294 1
    windows_state_list.append((window, resources_dict[window[2]]['State'], original_nodes))
d298 1
a298 1
    print "using sortfunc_immediate"
d301 1
a301 1
    print "using softfunc_future"
d593 4
a596 3
        potential_start_time_float = new_res['earliest_start_float']
        if potential_start_time_float == None :
            potential_start_time_float = Now_float
d610 4
a613 3
            potential_start_time_float = new_res['earliest_start_float']
            if potential_start_time_float == None :
                potential_start_time_float = Now_float
d627 1
d697 1
a697 1
        current_start_time_float = 0.0
d1519 2
a1520 1
            sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', '[]', LAST_AVAILABLE_IGNORED_FIRST)
d1536 2
a1537 2
        original_nodes = []
        found_original_nodes = 0
d1610 4
a1613 4
                    if found_original_nodes == 0 :
                        found_original_nodes = 1
                        original_nodes = new_res['node_list']
                        sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', `original_nodes`, LAST_AVAILABLE_IGNORED_FIRST)
d1649 4
a1652 4
                        if found_original_nodes < 2 :
                            found_original_nodes = found_original_nodes + 1
                            original_nodes = original_nodes + new_res['node_list']
                            sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', `original_nodes`, LAST_AVAILABLE_IGNORED_FIRST)
d1658 4
a1661 4
                if found_original_nodes == 0 :
                    found_original_nodes = 2
                    original_nodes = new_res['node_list']
                    sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', `original_nodes`, LAST_AVAILABLE_IGNORED_FIRST)
@


1.39
log
@use original_nodes in subsequent instances of standing reservations.
@
text
@d1949 1
a1949 1
def schedule_jobs(events_db_handle, jobs_db_handle, resources_db_handle, reservations_db_handle, cfg_resources_db_handle) :
d1962 7
@


1.38
log
@changed priority calculation, fixed string.strip bug
@
text
@d254 7
d263 1
a263 1
def sortfunc (first, second) :
d274 11
d290 2
d293 9
a301 2
    windows_state_list.append((window, resources_dict[window[2]]['State']))
windows_state_list.sort(sortfunc)
d1515 1
a1515 1
            sort_policy = LAST_AVAILABLE_IGNORED_FIRST
d1531 2
d1605 4
d1644 4
d1653 4
@


1.37
log
@removed '
@
text
@d51 1
d1746 2
a1747 1
  job_step
d1759 1
a1759 1
    new_res =create_reservation(
d1773 1
a1773 1
    )
d1777 1
a1777 1
def create_job_reservations(eligible_jobs, resources_db_handle, reservations_db_handle, jobs_db_handle) :
d1837 2
a1838 1
              job_step=job_step
d1855 3
a1857 1
                print "(%s) (%s) (%s)" % sys.exc_info()
d1942 1
a1942 1
        reservations_db_handle, jobs_db_handle)
d2216 1
a2216 1
        raise 'InsufficientNodes'
@


1.36
log
@put in check for existence of active_res job_runID in jobs_dict
@
text
@d1798 1
a1798 1
          and jobs_dict.has_key('reservation['job_runID']) :
@


1.35
log
@fixed MAXJOB problem again
@
text
@d1796 3
a1798 2
        if reservation['purpose_type_string'] == 'job' \
          or reservation['purpose_type_string'] == 'running' :
@


1.34
log
@fixed bug with warn message for LOST_JOB_LIMIT
@
text
@a65 1
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import NODERESTCODE
d86 2
d219 1
d1787 7
d1796 8
d1810 1
a1810 1
          running_jobs_per_user.has_key(job_step['user']) and \
d1812 4
a1815 1
          QOS_MAXJOBPERUSERPOLICY_dict[job_step['QOS']] :
d1839 1
a1839 1
                    running_jobs_per_user[job_step['user']] + 1
d1842 5
d1867 2
d1872 1
d1897 4
@


1.33
log
@fixed bug in running_jobs_per_user[user] + 1
@
text
@d941 1
a941 4
                message = """
Running Job (%s) is no longer detected.
It's state has been changed to 'Unknown'.
""" % old_job_step['name'] 
@


1.32
log
@added local_admin, local_user, wall_time priority elements
@
text
@d1785 1
a1785 1
                running_jobs_per_user[user] + 1
@


1.31
log
@removed NODE_RESTRICTION_FILE, CONFLICT_POLICY_CODE_FILE,
USERNAMESUFFIX
@
text
@d172 7
d183 21
d925 1
d929 1
d932 4
a935 1
        if old_job_step['name'] not in new_job_ids :
d938 8
@


1.30
log
@removed RESLIST_CMD and LOADL_CONFIG
@
text
@a103 2
  'NODE_RESTRICTION_FILE' : None,
  'CONFLICT_POLICY_CODE_FILE' : None,
a129 1
  'USERNAMESUFFIX' : None,
@


1.29
log
@changed open_ro_db to catch all exceptions
@
text
@a88 1
  'LOADL_ADMIN_FILE' : None,
a107 1
  'RESLIST_CMD' : None,
@


1.28
log
@added cPickle qualifications to pickle errors
@
text
@a372 1
    #db_tuple = lock_db_files(RO_DB_NAME, db_mode)
d381 3
a383 14
        except EOFError:
            print "EOFError retrying"
            time.sleep(1)
            tries = tries + 1
        except IOError:
            print "IOError retrying"
            time.sleep(1)
            tries = tries + 1
        except cPickle.PickleError:
            print "PickleError retrying"
            time.sleep(1)
            tries = tries + 1
        except cPickle.UnpicklingError:
            print "UnpicklingError retrying"
d385 1
a398 1
    #unlock_db_files(db_tuple)
@


1.27
log
@removed PickleError exception catch
@
text
@d390 5
a394 1
        except UnpicklingError:
@


1.26
log
@added COPYRIGHT here also
@
text
@a389 4
        except PickleError:
            print "PickleError retrying"
            time.sleep(1)
            tries = tries + 1
@


1.25
log
@cleaned up a bit...
@
text
@d1 35
@


1.24
log
@removed import of get_accepted_nodes_list
@
text
@a570 1
        #reservation_windows.sort(sort_by_start)
a670 8
        # default sort policy, last_available
        #def sortfunc (first, second) :
        #    if first[0] > second[0] :
        #        return -1
        #    if first[0] == second[0] :
        #        return 0
        #    if first[0] < second[0] :
        #        return 1
a673 1
        #windows.sort(sortfunc)
a949 1
            #keylist = map(lambda x, key=key : (x[key], x) , list)
a953 6
            #if first['priority'] > second['priority'] :
            #    return -1
            #if first['priority'] == second['priority'] :
            #    return 0
            #if first['priority'] < second['priority'] :
            #    return 1
a980 1
        #idle_jobs.sort(sort_by_priority)
a1416 1
            #raise 'InsufficientAcceptedNodes', standing_reservation
d2155 1
a2155 1
    if SERVERMODE != 'NORMAL' :
@


1.23
log
@removed get_unique_number_key
@
text
@a26 1
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import get_accepted_nodes_list
d341 26
a366 3
    ROFO = open(ROFILE, 'r')
    dict = cPickle.load(ROFO)
    ROFO.close
d901 3
a903 3
        Catalina_job_step_state = get_job_step_state(job_step)
        if Catalina_job_step_state == 'STARTING' or \
          Catalina_job_step_state == 'RUNNING' :
@


1.22
log
@changed second exec statement in affinity sort to explicit globals() and
locals()
@
text
@a751 9
def get_unique_number_key(db_handle) :
    shelf = db_handle[0]
    new_id_number_float = math.floor(Now_float)
    new_id_string = "%i" % new_id_number_float
    while shelf.has_key(new_id_string) :
        new_id_number_float = new_id_number_float + 1
        new_id_string = "%i" % new_id_number_float
    return new_id_string

d818 1
a818 1
            key = get_unique_number_key(old_jobs_db_handle)
d835 1
a835 1
            key = get_unique_number_key(old_reservations_db_handle)
@


1.21
log
@changed exec affinity_code to avoid unqualified exec warning
added check that earliest_start_float is not None
@
text
@d689 1
a689 1
                    exec reservation_affinity_code in Catalina.__dict__, Catalina.get_sorted_with_affinity.sortfunc.__dict__
d700 1
a700 1
                    exec(reservation_affinity_code)
@


1.20
log
@added job reservations to running_reservations_list
in update_standing_res, if the job res has already started
@
text
@d689 1
a689 1
                    exec(reservation_affinity_code)
d1891 1
a1891 1
    if earliest_start_float < 0 :
@


1.19
log
@added node_restriction to attributes saved for new_res
@
text
@d1386 3
a1388 1
        if reservation['purpose_type_string'] == 'running' :
@


1.18
log
@do not delete job reservations, unless they are farther in the
future than FUDGE_FACTOR/2
@
text
@d1973 1
@


1.17
log
@changed comparison in LAST_AVAILABLE_IGNORE_FIRST
@
text
@d1925 2
a1926 1
            if reservation['purpose_type_string'] == 'job' :
@


1.16
log
@renew running reservations each iteration
in get_eligible_and_running_jobs, check for priority key in job
@
text
@d205 1
a205 1
    if first[0][0] > second[0][0] :
@


1.15
log
@added persistence to res_ids for standing and job reservations
@
text
@d942 7
a948 1
            keylist = map(lambda x, key=key: (x[key], x), list)
d1592 1
a1592 7
            #delete_object(reservation['name'], reservations_db_handle)
            #if 0 < Now_float + FUDGE_FACTOR/2 - reservation['end_time_float'] \
            #  or reservation['job_runID'] not in runningstarting_names_list :
            if reservation['job_runID'] not in runningstarting_names_list :
                delete_object(reservation['name'], reservations_db_handle)
            else :
                save_jobs_list.append(reservation['job_runID'])
a1593 2
        if job['name'] in save_jobs_list and job['state'] == 'Running' :
            continue
@


1.14
log
@fix for MAXJOBPERUSER bug
@
text
@d1587 3
a1589 3
            if 0 < Now_float + FUDGE_FACTOR/2 - reservation['end_time_float'] \
              or reservation['job_runID'] not in runningstarting_names_list :
                # job is close to end time
d1596 5
a1600 1
        new_res_name = get_new_db_key(reservations_db_handle)
d1760 1
a1760 1
            new_res =create_job_reservation(
d1766 6
d1944 14
a1957 1
    reservation_name = get_new_db_key(reservations_db_handle)
@


1.13
log
@put reverse in get_eligible sort
@
text
@a996 3
                    else :
                        running_jobs_per_user[user] = \
                          running_jobs_per_user[user] + 1
d1437 2
d1721 10
d1738 17
@


1.12
log
@changed (fixed?) bug with MAXRUNNINGJOBS
@
text
@d944 1
@


1.11
log
@patched to fix binding
@
text
@d996 5
@


1.10
log
@added DEBUGJOB
@
text
@d478 2
d492 2
d501 2
d524 2
d535 2
d547 2
d559 2
d1925 2
a1926 1
    print "len(accepted_nodes_list) (%s)" % len(accepted_nodes_list)
d1948 2
a1949 1
            print "after filter, len(accepted_nodes_list) (%s)" % len(accepted_nodes_list)
d1984 2
d1997 2
d2016 2
a2017 1
    print "len(open_windows_list) (%s)" % len(open_windows_list)
d2020 2
a2021 1
    print "len(sized_windows_list) (%s)" % len(sized_windows_list)
d2045 2
a2046 1
    print "len(sorted_windows_list) (%s)" % len(sorted_windows_list)
@


1.9
log
@*** empty log message ***
@
text
@d133 1
@


1.8
log
@*** empty log message ***
@
text
@d115 2
a116 5
try :
    config.read(CONFIGFILE)
except :
    print "problem reading config file (%s)!" % CONFIGFILE
    sys.exit(1)
@


1.7
log
@*** empty log message ***
@
text
@a54 1
  'QUERYMACHINES' : None,
@


1.6
log
@removed USERNAMESUFFIX import
@
text
@a402 1
            print "node_restriction_code (%s)" % node_restriction_code
@


1.5
log
@removed BATCH_POLICY
@
text
@a33 1
from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import USERNAMESUFFIX
@


1.4
log
@added DEBUGJOB
@
text
@a71 1
  'BATCH_POLICY_CODE_FILE' : None,
a1009 3
    # This should be done once, at config parse...
    #batch_policy_code = get_file_code(BATCH_POLICY_CODE_FILE)
    #(eligible_jobs, ineligible_jobs, runningstarting_jobs, message_string) = apply_policy_code(batch_policy_code, input_tuple)
@


1.3
log
@*** empty log message ***
@
text
@d590 4
@


1.2
log
@parameterized RM name
@
text
@d27 2
d103 1
a166 1
os.environ['TZ'] = 'GMT0'
d851 1
a851 1
    attribute_list = ['state', 'allocated_hosts']
d1094 5
d1278 2
@


1.1
log
@Initial revision
@
text
@d16 18
a33 11
from Catalina_PBS import get_job_steps_dict
from Catalina_PBS import get_resources_list
from Catalina_PBS import get_configured_resources_list
from Catalina_PBS import get_resource_list
from Catalina_PBS import get_resource_name_list
from Catalina_PBS import update_job_priorities
from Catalina_PBS import run_jobs
from Catalina_PBS import get_job_step_state
from Catalina_PBS import cancel_job
from Catalina_PBS import cancel_bad_jobs
from Catalina_PBS import initialize_job_step
@
