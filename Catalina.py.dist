# Copyright 2001 The Regents of the University of California
# All Rights Reserved
# 
# Permission to use, copy, modify and distribute any part of this
# Catalina Scheduler program for educational, research and non-profit purposes,
# without fee, and without a written agreement is hereby granted, provided that
# the above copyright notice, this paragraph and the following three paragraphs
# appear in all copies.
# 
# Those desiring to incorporate this Catalina Scheduler program into commercial
# products or use for commercial purposes should contact:
# William J. Decker, Ph.D.
# Licensing Officer
# Technology Transfer and Intellectual Property Services (TTIPS)
# University of California, San Diego
# 9500 Gilman Drive
# La Jolla, CA 92093-0910
# phone:858-822-5128, fax: 858-534-7345
# e-mail:wjdecker@ucsd.edu
# 
# IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR
# DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING
# LOST PROFITS, ARISING OUT OF THE USE OF THIS CATALINA SCHEDULER PROGRAM,
# EVEN IF THE UNIVERSITY OF CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY
# OF SUCH DAMAGE.
# 
# THE CATALINA SCHEDULER PROGRAM PROVIDED HEREIN IS ON AN "AS IS" BASIS, AND
# THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT,
# UPDATES, ENHANCEMENTS, OR MODIFICATIONS.  THE UNIVERSITY OF CALIFORNIA MAKES
# NO REPRESENTATIONS AND EXTENDS NO WARRANTIES OF ANY KIND, EITHER IMPLIED OR
# EXPRESS, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
# MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, OR THAT THE USE OF
# THE CATALINA SCHEDULER PROGRAM WILL NOT INFRINGE ANY PATENT, TRADEMARK OR
# OTHER RIGHTS.

# Python module for Catalina Reservation System
# Hooks:
# - job_restriction,  screens jobs from running in the reservation
# - node_restriction, screens nodes from being used in the reservation
# - conflict_policy, finds and sorts open windows of all sizes
# - sort_policy, sorts nodes for allocation
# Users should be allowed access to job_restriction and node_restriction
# Rules for binding a job to a reservation:
#  - This can happen either at job submission with a comment line or
#  - at a later time, by calling the bind_job_to_reservation function
#  The comment line should be delimited by semicolons.  The reservation
#  binding should be in the form ...;\w*Catalina_res_bind=<res_id>:<res_id>:<res_id>...;


import fcntl
import traceback
import subprocess
import shutil
import exceptions
import Catalina____RESOURCEMANAGER_PLACEHOLDER___
#from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import Catalina____RESOURCEMANAGER_PLACEHOLDER___.get_job_step_state
#from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import Catalina____RESOURCEMANAGER_PLACEHOLDER___.JOBSUFFIX
#from Catalina____RESOURCEMANAGER_PLACEHOLDER___ import Catalina____RESOURCEMANAGER_PLACEHOLDER___.SUBMIT_OUTPUT_PATTERN

from operator import itemgetter
import copy
import string
import time
import sys
import math
import re
import cPickle
import stat
import statvfs
import os
import shutil
import pwd
import grp
import ConfigParser
import gc
import socket

gc.enable()

mandatory_dict = {
  'SERVERMODE' : None,
  'DEBUG' : None,
  'DEFAULT_JOB_CLASS' : None,
  'NODERESTCODE_STRING' : None,
  'HOMEDIR' : None,
  'ARCHIVE_DIR' : None,
  'CONFIGFILE' : None,
  'CONFIGURATION_DB' : None,
  'CONFIGURED_RESOURCES_DB' : None,
  'EVENTS_DB' : None,
  'JOBS_DB' : None,
  'OLD_JOBS_DB' : None,
  'OLD_RESERVATIONS_DB' : None,
  'RESERVATIONS_DB' : None,
  'RESOURCE_DB' : None,
  'STANDING_RESERVATIONS_DB' : None,
  'FUDGE_FACTOR' : None,
  'MAXJOBOVERRUN' : None,
  'MAXPRIORITY' : None,
  'NODE_SORT_POLICY_CODE_FILE' : None,
  'RESERVATION_DEPTH' : None,
  'DBSIZE_LIMIT' : None,
  'MAIL_RECIPIENT' : None,
  'MAILX' : None,
  'ECHO' : None,
  'LOCK_SUFFIX' : None,
  'CAT_LOCK_OWNER' : None,
  'CAT_LOCK_GROUP' : None,
  'JOB_START_TIME_LIMIT' : None,
  'RESOURCE_DOWN_TIME_LIMIT' : None,
  'MAXJOBPERUSERPOLICY' : None,
  'MAXJOBPERUSERCOUNT' : None,
  'MAXJOBQUEUEDPERUSERPOLICY' : None,
  'MAXJOBQUEUEDPERUSERCOUNT' : None,
  'BADRESOURCELIST' : None,
  'RESOURCE_WEIGHT' : None,
  'EXPANSION_FACTOR_WEIGHT' : None,
  'SYSTEM_QUEUE_TIME_WEIGHT' : None,
  'SUBMIT_TIME_WEIGHT' : None,
  'QOS_PRIORITY_WEIGHT' : None,
  'QOS_TARGET_EXPANSION_FACTOR_WEIGHT' : None,
  'QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT' : None,
  'QOS_PRIORITY_STRING' : None,
  'QOS_MAX_PRIORITY_STRING' : None,
  'QOS_TARGETXF_STRING' : None,
  'QOS_TARGETQT_STRING' : None,
  'TEST_SHORTPOOL_AMOUNT' : None,
  'TEST_SHORTPOOL_SPEC' : None,
  'TEST_SHORTPOOL_DURATION' : None,
  'TEST_STANDING_AMOUNT' : None,
  'TEST_STANDING_SPEC' : None,
  'TEST_STANDING_DURATION' : None,
  'TEST_USERRES_AMOUNT' : None,
  'TEST_USERRES_MOD_AMOUNT' : None,
  'TEST_USERRES_END' : None,
  'TEST_NOEARLIEST_AMOUNT' : None,
  'TEST_JOB' : None,
  'SUBMITCMD' : None,
  'CANCELCMD' : None,
  'RM_TO_CAT_RESOURCE_DICT_STRING' : None,
  'RM_TO_CAT_JOB_DICT_STRING' : None
  }

# In the future, could put this in a function, so
# that clients can specify alternate config files.
HOMEDIR = '___HOMEDIR_PLACEHOLDER___'
DBDIR = '___DBDIR_PLACEHOLDER___'
CONFIGFILE = HOMEDIR + '/' + 'catalina.config'
defaults_dict = {
  'SERVERMODE' : 'TEST',
  'HOMEDIR' : '___HOMEDIR_PLACEHOLDER___',
  'DBDIR' : '___DBDIR_PLACEHOLDER___'
  }

config = ConfigParser.ConfigParser(defaults_dict)
config.read(CONFIGFILE)

options_list = config.options('main')
found_list = []
# string.upper is used here, because config.optionxform = str
# didn't work...maybe needs to be done differently...
for option in options_list :
    upper_option = string.upper(option)
    sys.__dict__['modules']['Catalina'].__dict__[upper_option] = config.get('main', option)
    found_list.append(upper_option)
    if mandatory_dict.has_key(upper_option) :
        del mandatory_dict[upper_option]
mandatory_keys_left = mandatory_dict.keys()
if len(mandatory_keys_left) > 0 :
    print "Mandatory option(s) not found: (%s)!" % mandatory_keys_left
    sys.exit(1)

print "SERVERMODE (%s)" % SERVERMODE
if not vars().has_key('SCHEDULING_WINDOW') :
    SCHEDULING_WINDOW = 7776000.0
else:
    SCHEDULING_WINDOW = float(SCHEDULING_WINDOW)
if not vars().has_key('LOCKTIMEOUT'):
    LOCKTIMEOUT = 60.0
if not vars().has_key('LICENSENODE'):
    LICENSENODE = None
elif LICENSENODE == 'None':
    LICENSENODE = None
else:
    exec('LICENSEDICT = ' + LICENSEDICT_STRING)
if not vars().has_key('TOPOLOGY_MODULE'):
    TOPOLOGY_MODULE = None
elif TOPOLOGY_MODULE == 'None':
    TOPOLOGY_MODULE = None
else:
    #try:
    #    exec("import %s" % TOPOLOGY_MODULE)
    #except:
    #    import Topology
    #import Topology
    TOPOFILE='___HOMEDIR_PLACEHOLDER___/topo.local'
    if os.path.exists(TOPOFILE):
        TOPOFO = open(TOPOFILE)
        node_switch_dict_text = TOPOFO.read()
        TOPOFO.close()
        node_switch_dict = eval(node_switch_dict_text)
        #print node_switch_dict
    else:
        print "failed to find TOPOFILE (%s)!" % (TOPOFILE,)
        node_switch_dict = None
        # run without TOPOLOGY
        TOPOLOGY_MODULE = None
    
    
    SWITCHFILE='___HOMEDIR_PLACEHOLDER___/switch.local'
    if os.path.exists(SWITCHFILE):
        SWITCHFO = open(SWITCHFILE)
        switch_distance_dict_text = SWITCHFO.read()
        SWITCHFO.close()
        switch_distance_dict = eval(switch_distance_dict_text)
        #print node_switch_dict
    else:
        #generate switch_distance_dict from XDIM,YDIM,ZDIM
        if not vars().has_key('XDIM') or \
          not vars().has_key('YDIM') or not vars().has_key('ZDIM'):
            print "failed to find XDIM, YDIM or ZDIM!"
            # run without TOPOLOGY
            TOPOLOGY_MODULE = None
        else:
            try:
                XDIM = int(XDIM)
                YDIM = int(YDIM)
                ZDIM = int(ZDIM)
            except:
                print "failed to int XDIM (%s), YDIM  (%s)or ZDIM (%s)!" % (XDIM, YDIM, ZDIM)
                # run without TOPOLOGY
                TOPOLOGY_MODULE = None
        if TOPOLOGY_MODULE != None:
            switch_distance_dict = {}
            for xcoorda in range(XDIM):
                for ycoorda in range(YDIM):
                    for zcoorda in range(ZDIM):
                        for xcoordb in range(XDIM):
                            for ycoordb in range(YDIM):
                                for zcoordb in range(ZDIM):
                                    xhops = min((xcoorda - xcoordb) % XDIM,
                                                (xcoordb - xcoorda) % XDIM)
                                    yhops = min((ycoorda - ycoordb) % YDIM,
                                                (ycoordb - ycoorda) % YDIM)
                                    zhops = min((zcoorda - zcoordb) % ZDIM,
                                                (zcoordb - zcoorda) % ZDIM)
                                    hopcount = xhops + yhops + zhops
                                    switch_distance_dict[((xcoorda,ycoorda,zcoorda),(xcoordb,ycoordb,zcoordb))] = hopcount

    #else:
    #    XDIM = 4
    #    YDIM = 4
    #    ZDIM = 4
    
    #maxhops = 2
    #candidate_switch_list = [
    #  (0,0,0),
    #  (0,1,0),
    #  (0,2,0),
    #  (0,3,0),
    #  (0,0,1),
    #  (0,0,2),
    #  (0,0,3)
    #  ]
    #candidate_switch_list = switch_list
    
    #switch_list = []
    #for i in range(XDIM):
    #    for j in range(YDIM):
    #        for k in range(ZDIM):
    #            switch_list.append((i,j,k))
    
    #def gethops(aswitch, bswitch):
    #    xhops = min((aswitch[0] - bswitch[0]) % XDIM,
    #                (bswitch[0] - aswitch[0]) % XDIM)
    #    yhops = min((aswitch[1] - bswitch[1]) % YDIM,
    #                (bswitch[1] - aswitch[1]) % YDIM)
    #    zhops = min((aswitch[2] - bswitch[2]) % ZDIM,
    #                (bswitch[2] - aswitch[2]) % ZDIM)
    #    #xab = (aswitch[0] - bswitch[0]) % XDIM
    #    #xba = (bswitch[0] - aswitch[0]) % XDIM
    #    #if xab <= xba:
    #    #    xhops = xab
    #    #else:
    #    #    xhops = xba
    #    #yab = (aswitch[1] - bswitch[1]) % YDIM
    #    #yba = (bswitch[1] - aswitch[1]) % YDIM
    #    #if yab <= yba:
    #    #    yhops = yab
    #    #else:
    #    #    yhops = yba
    #    #zab = (aswitch[2] - bswitch[2]) % ZDIM
    #    #zba = (bswitch[2] - aswitch[2]) % ZDIM
    #    #if zab <= zba:
    #    #    zhops = zab
    #    #else:
    #    #    zhops = zba
    #    hops = xhops + yhops + zhops
    #    return hops
    
    def test_maxhops(test_set, maxhops):
        if maxhops == None:
            return 1
        overmax = 0
        for a_index in range(len(test_set)):
            #switcha0 = test_set[a_index][0]
            #switcha1 = test_set[a_index][1]
            #switcha2 = test_set[a_index][2]
            #switcha = (switcha0,switcha1,switcha2)
            #switcha = (test_set[a_index][0],test_set[a_index][1],test_set[a_index][2])
            for b_index in range(len(test_set)):
                if b_index <= a_index:
                    continue
                #hopcount = gethops(test_set[a_index], test_set[b_index])
                #xhops = min((test_set[a_index][0] - test_set[b_index][0]) % XDIM,
                #            (test_set[b_index][0] - test_set[a_index][0]) % XDIM)
                #yhops = min((test_set[a_index][1] - test_set[b_index][1]) % YDIM,
                #            (test_set[b_index][1] - test_set[a_index][1]) % YDIM)
                #zhops = min((test_set[a_index][2] - test_set[b_index][2]) % ZDIM,
                #            (test_set[b_index][2] - test_set[a_index][2]) % ZDIM)
                #hopcount = xhops + yhops + zhops
                #if maxhops != None and hopcount > maxhops:
                hopcount = switch_distance_dict[((test_set[a_index][0],test_set[a_index][1],test_set[a_index][2]),(test_set[b_index][0],test_set[b_index][1],test_set[b_index][2]))]
                #switchb = (test_set[b_index][0],test_set[b_index][1],test_set[b_index][2])
                #switchb0 = test_set[b_index][0]
                #switchb1 = test_set[b_index][1]
                #switchb2 = test_set[b_index][2]
                #switchb = (switchb0,switchb1,switchb2)
                #switchb = (test_set[b_index][0],test_set[b_index][1],test_set[b_index][2])
                #hopcount = switch_distance_dict[(switcha,switchb)]
                if maxhops != None and hopcount > maxhops:
                    overmax = 1
                    break
            if overmax == 1:
                break
        if overmax == 1:
            return 0
        else:
            return 1
    
    def get_maxset(candidate_switch, test_set, maxhops):
        maxset = [candidate_switch,]
        for test_switch in test_set:
            if test_switch == candidate_switch:
                continue
            maxset.append(test_switch)
            if test_maxhops(maxset, maxhops):
                pass
            else:
                del(maxset[-1])
        return maxset
    
    def get_node_switch_dict(nodenames_list):
        #return {'debian' : (2,2,2),
        #        'switch0_node0': (0,0,0),
        #        'switch0_node1': (0,0,0),
        #        'switch1_node0': (0,0,1),
        #        'switch1_node1': (0,0,1),
        #        'switch2_node0': (0,0,2),
        #        'switch2_node1': (0,0,2),
        #        'switch3_node0': (0,0,3),
        #        'switch3_node1': (0,0,3),
        #        'switch4_node0': (0,1,0),
        #        'switch4_node1': (0,1,0),
        #        'switch5_node0': (0,1,1),
        #        'switch5_node1': (0,1,1),
        #        'switch6_node0': (0,1,2),
        #        'switch6_node1': (0,1,2),
        #       }
        global node_switch_dict
        if node_switch_dict == None:
            print "node_switch_dict is None!"
            node_switch_dict = {}
            for nodename in nodenames_list:
                node_switch_dict[nodename] = (0,0,0)
        return node_switch_dict
     
    #def get_nodepriority(nodename, resources_dict, jobs_dict, reservations_dict):
    
    def get_switch_sets(maxhops, switch_node_list, resources_dict, jobs_dict):
    
        #print "switch_node_list (%s)" % (switch_node_list,)
        #print "start of get_switch_sets"
        nodenames_list = resources_dict.keys()
        node_switch_dict = get_node_switch_dict(nodenames_list)
        candidate_switch_list = []
        for switch_node in switch_node_list:
            #if not switch_node in candidate_switch_list:
            if not node_switch_dict.has_key(switch_node):
                # switch_node is not in switch.local, so assume
                # it can participate in all node sets, but it does
                # not have an associated switch
                continue
            if not node_switch_dict[switch_node] in candidate_switch_list:
                candidate_switch_list.append(node_switch_dict[switch_node])
        #print "candidate_switch_list (%s)" % (candidate_switch_list,)
        switch_sets_list = [[],]
        #print "len(candidate_switch_list) (%s)" % (len(candidate_switch_list),)
        for candidate_switch in candidate_switch_list:
            #print "candidate_switch (%s)" % (candidate_switch,)
            newsets_list = []
            del_list = []
            #print "before constructing newset"
            for switch_set_index in range(len(switch_sets_list)):
                #test_set = copy.deepcopy(switch_sets_list[switch_set_index])
                #test_set.append(candidate_switch)
                test_set = switch_sets_list[switch_set_index] + [candidate_switch,]
                #print "test_set (%s)" % (test_set,)
                replace_old = 0
                if test_maxhops(test_set, maxhops):
                    replace_old = 1
                    #print "passed test_maxhops (%s)" % (test_set,)
                    #switch_sets_list[switch_set_index] = test_set
                    # need to replace old switch_set
                    newset = test_set
                else:
                    replace_old = 0
                    #print "failed test_maxhops (%s)" % (test_set,)
                    #print "before get_maxset"
                    newset = get_maxset(candidate_switch, test_set, maxhops)
                    #print "after get_maxset"
                    #print "newsets_list.append (%s)" % (newset,)
                    #print "test_set (%s) newset (%s)" % (test_set, newset)
                redundant = 0
                #for node_set in newsets_list:
                #print "newsets_list + switch_sets_list (%s)" % (newsets_list + switch_sets_list,)
                #print "newsets_list (%s) switch_sets_list (%s)" % (newsets_list, switch_sets_list)
                # delete old subsets in newsets_list
                nonred_newsets_list = []
                for node_set_index, node_set in enumerate(newsets_list):
                    if set(node_set).issubset(newset):
                        continue
                    else:
                        nonred_newsets_list.append(node_set)
                newsets_list = nonred_newsets_list
                    
                for node_set in newsets_list + switch_sets_list:
                    #print "checking redundance for node_set (%s)" % (node_set,)
                    if set(newset).issubset(node_set):
                        redundant = 1
                        break
                    #else:
                        #print "newset (%s) is not subset of (%s)" % (newset, node_set)
                    #if set(node_set).issubset(newset):
                    #    print "old node_set (%s) is subset of (%s)" % (node_set, newset)
                    #    redundant = 1
                    #    break
                    #else:
                    #    print "node_set (%s) is not subset of (%s)" % (node_set, newset)
                    
                if redundant == 0:
                    if replace_old == 1:
                        #print "replacing old switch_sets_list (%s)" % (newset,)
                        switch_sets_list[switch_set_index] = newset
                        # delete any old switch_sets that are subsets of
                        # the newset, skipping self
                        for switch1index, switch1 in enumerate(switch_sets_list):
                            if switch1index == switch_set_index:
                                continue
                            if set(switch1).issubset(newset):
                                del_list.append(switch1index)
                            #else:
                            #    print "1. not_subset (%s) (%s)" % (switch1,newset)
                    else:
                        #print "appending to newsets_list (%s)" % (newset,)
                        newsets_list.append(newset)
                        # delete any old switch_sets that are subsets of
                        # the newset
                        for switch1index, switch1 in enumerate(switch_sets_list):
                            if set(switch1).issubset(newset):
                                del_list.append(switch1index)
                            #else:
                            #    print "2. not_subset (%s) (%s)" % (switch1,newset)
                #else:
                #    print "not appending to newsets_list (%s)" % (newset,)
            # is there redundancy amongst newsets_list members?
            #print "before filtering out redundant newsets_list"

            #for switch1index, switch1 in enumerate(newsets_list):
            #    for switch2index, switch2 in enumerate(newsets_list):
            #        if switch1index == switch2index:
            #            continue
            #        if set(switch1).issubset(switch2):
            #            print "found redundant newset (%s) in (%s)" % (switch1,switch2)
            #print "after constructing newset"
            #print "del_list (%s)" % (del_list,)
            nonred_switch_sets_list = []
            for oldset_index, oldset in enumerate(switch_sets_list):
                if not oldset_index in del_list:
                    nonred_switch_sets_list.append(oldset)
            switch_sets_list = nonred_switch_sets_list
            switch_sets_list = switch_sets_list + newsets_list
            #print "switch_sets_list (%s)" % (switch_sets_list,)
            #print "newsets_list (%s)" % (newsets_list,)
            #new_switch_sets_list = []
            #print "before filtering out redundant switch_sets"
            # should have no redundant sets at this point
            #for switch1index, switch1 in enumerate(switch_sets_list):
            #    subsetstatus = 0
            #    for switch2index, switch2 in enumerate(switch_sets_list):
            #        if switch1index == switch2index:
            #            continue
            #        #if set(switch1).issubset(switch2) or set(switch2).issubset(switch1):
            #        if set(switch1).issubset(switch2):
            #            print "found redundant set (%s) in (%s)" % (switch1,switch2)
            #            subsetstatus = 1
            #            break
            #    if subsetstatus == 0:
            #        #print "not subset (%s) (%s)" % (switch1, new_switch_sets_list)
            #        new_switch_sets_list.append(switch1)
            #    #else:
            #    #    print "not appending redundant set (%s)" % (switch1,)
           # print "after filtering out redundant switch_sets"
            #switch_sets_list = new_switch_sets_list
            #print "len(switch_sets_list) (%s)" % (len(switch_sets_list),)

#            keepsets_list = []
#            print "before checking newsets_list"
#            for newset_index in range(len(newsets_list)):
#                newsets_tuple = tuple(newsets_list[newset_index])
#                #print "newsets_tuple (%s)" % (newsets_tuple,)
#                redundant = 0
#                for switch_set_index in range(len(switch_sets_list)):
#                    switch_sets_tuple = tuple(switch_sets_list[switch_set_index])
#                    #print "switch_sets check: newsets_tuple (%s) switch_sets_tuple (%s)" % (newsets_tuple,switch_sets_tuple)
#                    localred = 0
#                    if set(newsets_tuple).issubset(switch_sets_tuple):
#                        #print "redundant"
#                        redundant = 1
#                        localred = 1
#                    nonredundant = 0
#                    for newset in newsets_tuple:
#                        if not newset in switch_sets_tuple:
#                            nonredundant = 1
#                    if (localred == 0 and nonredundant == 0) or (localred == 1 and nonredundant == 1):
#                        print "switch_sets_tuple redundancy conflict!"
#                        print "localred (%s) nonredundant (%s)" % (localred, nonredundant)
#                        print "newsets_tuple (%s) switch_sets_tuple (%s)" % (newsets_tuple, switch_sets_tuple)
#                for switch_set_index in range(len(keepsets_list)):
#                    keepsets_tuple = tuple(keepsets_list[switch_set_index])
#                    #print "keep_sets check: newsets_tuple (%s) keepsets_tuple (%s)" % (newsets_tuple,keepsets_tuple)
#                    localred = 0
#                    if set(newsets_tuple).issubset(keepsets_tuple):
#                        #print "redundant"
#                        redundant = 1
#                        localred = 1
#                    nonredundant = 0
#                    for newset in newsets_tuple:
#                        if not newset in keepsets_tuple:
#                            nonredundant = 1
#                    if (localred == 0 and nonredundant == 0) or (localred == 1 and nonredundant == 1):
#                        print "keepsets_tuple redundancy conflict!"
#                        print "localred (%s) nonredundant (%s)" % (localred, nonredundant)
#                        print "newsets_tuple (%s) keepsets_tuple (%s)" % (newsets_tuple, keepsets_tuple)
#                if redundant == 0:
#                #if nonredundant == 1:
#                    #keepsets_list.append(copy.deepcopy(newsets_list[newset_index]))
#                    #print "appending (%s)" % (newsets_list[newset_index],)
#                    keepsets_list.append(newsets_list[newset_index])
#            for keepset in keepsets_list:
#                for switch_set in switch_sets_list:
#                    if set(keepset).issubset(switch_set):
#                        print "found subset (%s) in switch_set (%s)" % (keepset, switch_set)
#            switch_sets_list = switch_sets_list + keepsets_list
#            print "keepsets_list (%s)" % (keepsets_list,)
            #print "len(switch_sets_list) (%s)" % (len(switch_sets_list),)
            #print "switch_sets_list (%s)" % (switch_sets_list,)
            #print "before final redundancy check"
            #for switch1index, switch1 in enumerate(switch_sets_list):
            #    for switch2index, switch2 in enumerate(switch_sets_list):
            #        if switch1index == switch2index:
            #            continue
            #        if set(switch1).issubset(switch2) or set(switch2).issubset(switch1):
            #            print "switch1 (%s) switch2 (%s) redundancy found!" % (switch1, switch2)
            #            #raise "SwitchSetRedundancy"
            #print "after final redundancy check"
        
        #print "switch_sets_list (%s)" % (switch_sets_list,)
        node_sets_list = []
        for switch_set in switch_sets_list:
            node_set = []
            for nodename in switch_node_list:
                if not node_switch_dict.has_key(nodename) or node_switch_dict[nodename] in switch_set:
                    # if nodename not in node_switch_dict, assume it can
                    # participate in all node_sets
                    node_set.append(nodename)
            #node_sets_list.append(copy.deepcopy(node_set))
            node_sets_list.append(node_set)
        #print "node_sets_list (%s)" % (node_sets_list,)
        return node_sets_list

if not vars().has_key('QJ_SIM') :
    QJ_SIM = None
if not vars().has_key('QM_SIM') :
    QM_SIM = None
DEBUGJOB = None
FUDGE_FACTOR = float(FUDGE_FACTOR)
if not vars().has_key('MACHINE_REFRESH_INTERVAL') :
    MACHINE_REFRESH_INTERVAL = 1
else :
    try :
        MACHINE_REFRESH_INTERVAL = int(MACHINE_REFRESH_INTERVAL)
    except :
        print "Could not convert MACHINE_REFRESH_INTERVAL (%s) to int.  Using 1!" % MACHINE_REFRESH_INTERVAL
        MACHINE_REFRESH_INTERVAL = 1
MAXJOBOVERRUN = float(MAXJOBOVERRUN)
MAXPRIORITY = long(MAXPRIORITY)
if RESERVATION_DEPTH == 'None' :
    RESERVATION_DEPTH = None
else :
    RESERVATION_DEPTH = int(RESERVATION_DEPTH)
DBSIZE_LIMIT = int(DBSIZE_LIMIT)
if not vars().has_key('FORCETZ') or FORCETZ == '' :
    FORCETZ = 'NOFORCE'
if FORCETZ != 'NOFORCE' :
    print "forcing TZ to (%s)" % FORCETZ
    os.environ['TZ'] = FORCETZ
try :
    time.tzset()
except :
    TZSET = None
if not vars().has_key('LOGGER') :
    LOGGER = '___LOGGER_PLACEHOLDER___'
if not vars().has_key('LOGGER_FACILITY') :
    LOGGER_FACILITY = 'daemon'
if not vars().has_key('RUN_AT_RISK_CLEANUP_TIME') :
    RUN_AT_RISK_CLEANUP_TIME = 300.0
else :
    try :
        RUN_AT_RISK_CLEANUP_TIME = float(RUN_AT_RISK_CLEANUP_TIME)
    except :
        RUN_AT_RISK_CLEANUP_TIME = 300.0
if not vars().has_key('RUN_AT_RISK_MIN_RUNTIME') :
    RUN_AT_RISK_MIN_RUNTIME = 900.0
else :
    try :
        RUN_AT_RISK_MIN_RUNTIME = float(RUN_AT_RISK_MIN_RUNTIME)
    except :
        RUN_AT_RISK_MIN_RUNTIME = 900.0
if not vars().has_key('PREEMPT_CLEANUP_TIME') :
    PREEMPT_CLEANUP_TIME = 300.0
else :
    try :
        PREEMPT_CLEANUP_TIME = float(PREEMPT_CLEANUP_TIME)
    except :
        PREEMPT_CLEANUP_TIME = 300.0
if not vars().has_key('PREEMPT_MIN_RUNTIME') :
    PREEMPT_MIN_RUNTIME = 1800.0
else :
    try :
        PREEMPT_MIN_RUNTIME = float(PREEMPT_MIN_RUNTIME)
    except :
        PREEMPT_MIN_RUNTIME = 1800.0
if not vars().has_key('JOB_START_WARN_LIMIT') :
    JOB_START_WARN_LIMIT = None
else :
    JOB_START_WARN_LIMIT = int(JOB_START_WARN_LIMIT)
DB_WARN_COUNT = 0
if not vars().has_key('DB_WARN_LIMIT') :
    DB_WARN_LIMIT = None
else :
    DB_WARN_LIMIT = int(DB_WARN_LIMIT)
JOB_START_TIME_LIMIT = float(JOB_START_TIME_LIMIT)
if not sys.__dict__['modules']['Catalina'].__dict__.has_key('LOST_JOB_WARN') :
    LOST_JOB_WARN = 'FALSE'
if sys.__dict__['modules']['Catalina'].__dict__.has_key('LOST_JOB_LIMIT') :
    try :
        LOST_JOB_LIMIT = float(LOST_JOB_LIMIT)
    except :
        LOST_JOB_LIMIT = None
else :
    LOST_JOB_LIMIT = None
RESOURCE_DOWN_TIME_LIMIT = float(RESOURCE_DOWN_TIME_LIMIT)
if not vars().has_key('FIFOSCREEN') :
    FIFOSCREEN = None
MAXJOBPERUSERCOUNT = int(MAXJOBPERUSERCOUNT)
MAXJOBQUEUEDPERUSERCOUNT = int(MAXJOBQUEUEDPERUSERCOUNT)

priority_weights_tuple = (
  'RESOURCE_WEIGHT',
  'EXPANSION_FACTOR_WEIGHT',
  'SYSTEM_QUEUE_TIME_WEIGHT',
  'SUBMIT_TIME_WEIGHT',
  'LOCAL_USER_WEIGHT',
  'LOCAL_ADMIN_WEIGHT',
  'WALL_TIME_WEIGHT',
  'QOS_PRIORITY_WEIGHT',
  'QOS_TARGET_EXPANSION_FACTOR_WEIGHT',
  'QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT'
  )
max_places = 0
dec_pattern = r"\d*\.(?P<fraction>\d+)"
dec_reo = re.compile(dec_pattern)
for priority_weight in priority_weights_tuple :
    if sys.__dict__['modules']['Catalina'].__dict__.has_key(priority_weight) :
        this_weight_string = sys.__dict__['modules']['Catalina'].__dict__[priority_weight]
        prio_mo = dec_reo.match(this_weight_string)
        if prio_mo != None :
            this_places = len(prio_mo.group('fraction'))
            if this_places > max_places :
                max_places = this_places
if max_places == 0 :
    max_places_float = 1.0
else :
    max_places_float = pow(10,max_places)
RESOURCE_WEIGHT = long(float(RESOURCE_WEIGHT) * max_places_float)
if sys.__dict__['modules']['Catalina'].__dict__.has_key('LOCAL_ADMIN_WEIGHT') :
    try :
        LOCAL_ADMIN_WEIGHT = long(float(LOCAL_ADMIN_WEIGHT) * max_places_float)
    except :
        LOCAL_ADMIN_WEIGHT = 0L
else :
    LOCAL_ADMIN_WEIGHT = 0L
if sys.__dict__['modules']['Catalina'].__dict__.has_key('LOCAL_USER_WEIGHT') :
    try :
        LOCAL_USER_WEIGHT = long(float(LOCAL_USER_WEIGHT) * max_places_float)
    except :
        LOCAL_USER_WEIGHT = 0L
else :
    LOCAL_USER_WEIGHT = 0L
if sys.__dict__['modules']['Catalina'].__dict__.has_key('WALL_TIME_WEIGHT') :
    try :
        WALL_TIME_WEIGHT = long(float(WALL_TIME_WEIGHT) * max_places_float)
    except :
        WALL_TIME_WEIGHT = 0L
else :
    WALL_TIME_WEIGHT = 0L
EXPANSION_FACTOR_WEIGHT = long(float(EXPANSION_FACTOR_WEIGHT) * max_places_float)
SYSTEM_QUEUE_TIME_WEIGHT = long(float(SYSTEM_QUEUE_TIME_WEIGHT)* max_places_float)
SUBMIT_TIME_WEIGHT = long(float(SUBMIT_TIME_WEIGHT) * max_places_float)
QOS_PRIORITY_WEIGHT = long(float(QOS_PRIORITY_WEIGHT) * max_places_float)
QOS_TARGET_EXPANSION_FACTOR_WEIGHT = long(float(QOS_TARGET_EXPANSION_FACTOR_WEIGHT) * max_places_float)
QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT = long(float(QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT) * max_places_float)

DEFAULT_PROC_CHARGE = float(DEFAULT_PROC_CHARGE)
TEST_SHORTPOOL_AMOUNT = int(TEST_SHORTPOOL_AMOUNT)
TEST_SHORTPOOL_DURATION = int(TEST_SHORTPOOL_DURATION)
TEST_STANDING_AMOUNT = int(TEST_STANDING_AMOUNT)
TEST_STANDING_DURATION = int(TEST_STANDING_DURATION)
TEST_USERRES_AMOUNT = int(TEST_USERRES_AMOUNT)
TEST_USERRES_MOD_AMOUNT = int(TEST_USERRES_MOD_AMOUNT)
TEST_USERRES_END = int(TEST_USERRES_END)
TEST_BINDING_AMOUNT = int(TEST_BINDING_AMOUNT)
TEST_NOEARLIEST_AMOUNT = int(TEST_NOEARLIEST_AMOUNT)
exec('QOS_PRIORITY_dict = ' + QOS_PRIORITY_STRING)
exec('QOS_PRIORITY_dict = ' + QOS_PRIORITY_STRING)
exec('QOS_MAX_PRIORITY_dict = ' + QOS_MAX_PRIORITY_STRING)
exec('QOS_TARGETXF_dict = ' + QOS_TARGETXF_STRING)
exec('QOS_TARGETQT_dict = ' + QOS_TARGETQT_STRING)
exec('QOS_MAXJOBPERUSERPOLICY_dict = ' + QOS_MAXJOBPERUSERPOLICY_STRING)
exec('QOS_MAXJOBQUEUEDPERUSERPOLICY_dict = ' + QOS_MAXJOBQUEUEDPERUSERPOLICY_STRING)
if vars().has_key('QOS_MAXJOBPERACCOUNTPOLICY_STRING') :
    exec('QOS_MAXJOBPERACCOUNTPOLICY_dict = ' + QOS_MAXJOBPERACCOUNTPOLICY_STRING)
else :
    QOS_MAXJOBPERACCOUNTPOLICY_dict = {}
if vars().has_key('QOS_MAXJOBPERUSERPERQOSPOLICY_STRING') :
    exec('QOS_MAXJOBPERUSERPERQOSPOLICY_dict = ' + QOS_MAXJOBPERUSERPERQOSPOLICY_STRING)
else :
    QOS_MAXJOBPERUSERPERQOSPOLICY_dict = {}
if vars().has_key('QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_STRING') :
    exec('QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict = ' + QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_STRING)
else :
    QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict = {}
if vars().has_key('QOS_MAXJOBQUEUEDPERUSERPERQOSPOLICY_STRING') :
    exec('QOS_MAXJOBQUEUEDPERUSERPERQOSPOLICY_dict = ' + QOS_MAXJOBQUEUEDPERUSERPERQOSPOLICY_STRING)
else :
    QOS_MAXJOBQUEUEDPERUSERPERQOSPOLICY_dict = {}
if vars().has_key('QOS_MAXNODESECQUEUEDPERUSERPOLICY_STRING') :
    exec('QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict = ' + QOS_MAXNODESECQUEUEDPERUSERPOLICY_STRING)
else :
    QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict = {}
if vars().has_key('QOS_MAXNODESECRUNNINGPERUSERPOLICY_STRING') :
    exec('QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict = ' + QOS_MAXNODESECRUNNINGPERUSERPOLICY_STRING)
else :
    QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict = {}
if vars().has_key('QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_STRING') :
    exec('QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict = ' + QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_STRING)
else :
    QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict = {}
if vars().has_key('QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_STRING') :
    exec('QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict = ' + QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_STRING)
else :
    QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict = {}
exec('RM_TO_CAT_RESOURCE_dict = ' + RM_TO_CAT_RESOURCE_DICT_STRING)
exec('RM_TO_CAT_JOB_dict = ' + RM_TO_CAT_JOB_DICT_STRING)
if vars().has_key('USER_SET_LIMITS_DICT_STRING') :
    exec('USER_SET_LIMITS_dict = ' + USER_SET_LIMITS_DICT_STRING)
if vars().has_key('CLASS_PRIORITY_DICT_STRING') :
    exec('CLASS_PRIORITY_dict = ' + CLASS_PRIORITY_DICT_STRING)
NODERESTCODE = re.sub("\n ","\n", NODERESTCODE_STRING)

#if SERVERMODE == 'SIM' :
#    Now_float = float(os.system(TIME_SIM))
#else :
#    Now_float = time.time()

def get_scheduler_time() :
    if SERVERMODE == 'SIM' :
        Now_float = float(os.popen(TIME_SIM).read())
    else :
        Now_float = time.time()
    return Now_float

#Now_float = Catalina____RESOURCEMANAGER_PLACEHOLDER___.get_scheduler_time()
Now_float = get_scheduler_time()
END_OF_SCHEDULING = Now_float + float(SCHEDULING_WINDOW)
FIRST_AVAILABLE = """
import string
def sortfunc (first, second) :
    if first[0] < second[0] :
        return -1
    if first[0] == second[0] :
        return 0
    if first[0] > second[0] :
        return 1
windows = input_tuple[0]
nodes = input_tuple[1]
new_res = input_tuple[2]
windows.sort(sortfunc)
result = windows
"""
LAST_AVAILABLE = """
import string
def sortfunc (first, second) :
    if first[0] > second[0] :
        return -1
    if first[0] == second[0] :
        return 0
    if first[0] < second[0] :
        return 1
windows = input_tuple[0]
nodes = input_tuple[1]
new_res = input_tuple[2]
windows.sort(sortfunc)
result = windows
"""
# This policy is used by standing reservations to choose
# nodes.  If we are in the standing reservation period,
# the code should choose first nodes that have jobs running
# on them.  This is to prevent undesired shortpool behaviour.
# If the standing reservation instance is in the future, then
# we want to choose the last available, in order to keep our
# backfill window as large as possible.
LAST_AVAILABLE_IGNORED_FIRST = """
import string
def sortfunc_immediate (first, second) :
    if DEBUGJOB != None :
        print "first[1] (%s), second[1] (%s)" % (first[1], second[1])
    if first[1] == 'Running' and second[1] == 'Idle' :
        if DEBUGJOB != None :
            print "first[1] == 'Running' and second[1] == 'Idle', returning -1"
        return -1
    if first[1] == 'Idle' and second[1] == 'Running' :
        if DEBUGJOB != None :
            print "first[1] == 'Idle' and second[1] == 'Running', returning 1"
        return 1
    if first[0][0] > second[0][0] :
        return -1
    if first[0][0] == second[0][0] :
        return 0
    if first[0][0] < second[0][0] :
        return 1
def sortfunc_future (first, second) :
    #if first[0][2] in first[2] and not second[0][2] in second[2] :
    #    return -1
    #if second[0][2] in second[2] and not first[0][2] in first[2] :
    #    return 1
    if first[0][0] > second[0][0] :
        return -1
    if first[0][0] == second[0][0] :
        return 0
    if first[0][0] < second[0][0] :
        return 1
windows = input_tuple[0]
nodes = input_tuple[1]
new_res = input_tuple[2]
resources_dict = input_tuple[5]
windows_state_list = []
#original_nodes = ___ORIGINAL_NODE_PLACEHOLDER___
future = 0
for window in windows :
    windows_state_list.append((window, resources_dict[window[2]]['State']))
    if DEBUGJOB != None :
        print "window[0] (%s) window[1] (%s) window[2] (%s)" % (time.asctime(time.localtime(window[0])), time.asctime(time.localtime(window[1])), window[2])
    if window[0] > Now_float :
        future = 1
if future == 0 :
    windows_state_list.sort(sortfunc_immediate)
else :
    windows_state_list.sort(sortfunc_future)
windows = []
for window_tuple in windows_state_list :
    windows.append(window_tuple[0])
result = windows
"""

username_string = pwd.getpwuid(os.getuid())[0]

class InsufficientNodes(Exception):
    def __init__(self, res):
        self.res = res

class ResIsNone(Exception):
    pass

class NoResStartTime(Exception):
    pass

class KeyNotFound(Exception):
    pass

class FoundJob(Exception):
    pass

class NameNotInDB(Exception):
    def __init__(self, name):
        self.name = name

class CancelJobFailure(Exception):
    def __init__(self, job):
        self.job = job

class SchedulerSuspended(Exception):
    pass

class SchedulerStopped(Exception):
    pass

class IncompleteQJ(Exception):
    pass

class IncompleteQM(Exception):
    pass

class CatalinaDBError(Exception):
    def __init__(self, detail):
        self.detail = detail

#class FoundConflict(Exception):
#    pass

class KeyExists(Exception):
    def __init__(self, key):
        self.key = key

def timedrun(command,timeout) :
    child_obj = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
    returncode = None
    iterations = 0
    while returncode == None and iterations <= timeout :
        returncode = child_obj.poll()
        if returncode == None :
            iterations = iterations + 1
            time.sleep(1)
        else :
            break
    if returncode == None :
        # child failed to beat timeout
        print "(%s) failed to return in (%s)!" % (command, timeout)
        os.kill(child_obj.pid)
        return ("timedrun (%s) failed!" % (command,), -1, '')
    else :
        (stdout, stderr) = child_obj.communicate()
        return (stdout, returncode, stderr)
    #return_tuple = os.waitpid(childpid,os.WNOHANG)

def initialize_reservation(reservation_name) :
    new_reservation = {}
    new_reservation['name'] = reservation_name
    new_reservation['job_restriction'] = None
    new_reservation['job_binding'] = None
    new_reservation['purpose_type_string'] = None
    new_reservation['earliest_start_float'] = None
    new_reservation['latest_end_float'] = None
    new_reservation['duration_float'] = None
    new_reservation['latency_float'] = None
    new_reservation['resource_amount_int'] = None
    new_reservation['purpose_type_string'] = None
    new_reservation['account_string'] = None
    new_reservation['creator_string'] = None
    new_reservation['comment_string'] = None
    new_reservation['conflict_policy'] = None
    new_reservation['node_sort_policy'] = None
    new_reservation['node_restriction'] = None
    new_reservation['node_usage'] = None
    new_reservation['resource_dict_list'] = None
    new_reservation['affinity_calculation'] = None
    new_reservation['job_runID'] = None
    new_reservation['start_time_float'] = None
    new_reservation['end_time_float'] = None
    new_reservation['node_list'] = None
    new_reservation['resource_amount_int'] = None
    new_reservation['start_spec_string'] = None
    new_reservation['ignore_reservations_list'] = None
    new_reservation['overlap_running_int'] = 0
    new_reservation['start_count_int'] = 0
    new_reservation['notify_count_int'] = 0
    new_reservation['notify_string'] = None
    new_reservation['allocated_dict_list'] = None
    new_reservation['maxhops'] = None
    return new_reservation

def get_new_db_key(db_handle) :
    shelf = db_handle[0]
    new_id_number_float = math.floor(Now_float)
    new_id_string = "%i" % new_id_number_float
    while shelf.has_key(new_id_string) :
        new_id_number_float = new_id_number_float + 1
        new_id_string = "%i" % new_id_number_float
    return new_id_string

def get_file_code(filename_string) :
    input = open(filename_string, 'r')
    file_string = input.read()
    input.close()
    return file_string

#def lock_db_files (db_name, db_mode='read') :
#    LOCK = DBDIR + '/' + db_name + LOCK_SUFFIX
#    print "Waiting for %s db lock" % db_name
#    if db_mode == 'write' :
#        # open the lock file for append, set exclusive lock
#        LOCKFO = open(LOCK, 'a')
#        fcntl.lockf(LOCKFO.fileno(), fcntl.LOCK_EX)
#    else :
#        # open the reservations file for read, set non-exclusive lock
#        LOCKFO = open(LOCK, 'r')
#        fcntl.lockf(LOCKFO.fileno(), fcntl.LOCK_SH)
#    print "Received %s db lock" % db_name
#    db_tuple = ( LOCKFO, db_name, db_mode )
#    return db_tuple
#
#def unlock_db_files (db_tuple) :
#    db_name = db_tuple[1]
#    LOCK = DBDIR + '/' + db_name + LOCK_SUFFIX
#    LOCKFO = db_tuple[0]
#    uid_int = pwd.getpwnam(CAT_LOCK_OWNER)[2]
#    gid_int = grp.getgrnam(CAT_LOCK_GROUP)[2]
#    try :
#        os.chmod(LOCK,0664)
#        os.chown(LOCK,uid_int,gid_int)
#    except :
#        try :
#            if DEBUG == 'locks' :
#                info_tuple = sys.exc_info()
#                print "(%s) (%s) (%s)" % info_tuple
#                info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
#                traceback.print_tb(info_tuple[2])
#                tb_list = traceback.format_tb(info_tuple[2])
#                info_list = info_list + tb_list
#                tb_text = string.join(info_list)
#                print tb_text
#        except :
#            print "print of sys.exc_info() failed!"
#    # release lock, close the file
#    fcntl.lockf(LOCKFO.fileno(), fcntl.LOCK_UN)
#    print "Released %s db %s lock" % (db_tuple[1], db_tuple[2])
#    LOCKFO.close()

def check_lock(db_name):
    myserver = socket.gethostname()
    mypid = os.getpid()
    LOCK = DBDIR + '/' + db_name + LOCK_SUFFIX
    MYLOCK = DBDIR + '/' + db_name + LOCK_SUFFIX + '.' + myserver + '.' + "%s" % mypid
    try:
        mystat = os.stat(MYLOCK)
        lockstat = os.stat(LOCK)
    except exceptions.OSError:
        print "OSError opening (%s)" % (MYLOCK,)
        return False
    except exceptions.IOError:
        print "IOError opening (%s)" % (MYLOCK,)
        return False
    if mystat.st_ino == lockstat.st_ino and mystat.st_rdev == lockstat.st_rdev:
        #still locked, 
        return True

def lock_db_files (db_name, db_mode='read', timeout=LOCKTIMEOUT, breakable=True) :
    #print "db_mode (%s)" % (db_mode,)
    LOCK = DBDIR + '/' + db_name + LOCK_SUFFIX
    myserver = socket.gethostname()
    mypid = os.getpid()
    #print "lock mypid (%s)" % mypid
    MYLOCK = DBDIR + '/' + db_name + LOCK_SUFFIX + '.' + myserver + '.' + "%s" % mypid
    ulock_pat = db_name + LOCK_SUFFIX + '\.' + "\S+" + '\.' + "\d+"
    ulock_reo = re.compile(ulock_pat)
    #print "ulock_pat (%s)" % (ulock_pat,)
    #print "lock MYLOCK (%s)" % (MYLOCK,)
    print "Waiting for %s db lock" % db_name
    LOCKLOCK = 0
    if db_mode == 'write' :
        # open the lock file for append, set exclusive lock
        #LOCKFO = open(LOCK, 'a')
        #fcntl.lockf(LOCKFO.fileno(), fcntl.LOCK_EX)
        # if i need a write lock, i need both read and 
        # and write lock.  if i only need a read lock,
        # ignore the write lock.
        # actually, just ignore locking for read clients,
        # let them retry the read, if it fails.
        while LOCKLOCK == 0:
            try:
                FO = open(MYLOCK,'w')
                FO.close()
            except exceptions.OSError:
                print "OSError opening (%s)" % (MYLOCK,)
                continue
            except exceptions.IOError:
                print "IOError opening (%s)" % (MYLOCK,)
                continue
            try:
                rc = os.link(MYLOCK,LOCK)
            except:
                pass
            # if someone else has unlinked LOCK, or some other
            # problem, keep going
            try:
                lockstat = os.stat(LOCK)
                mystat = os.stat(MYLOCK)
            except exceptions.OSError:
                continue
            if mystat.st_ino == lockstat.st_ino and mystat.st_rdev == lockstat.st_rdev:
                #print "lock match mystat.st_ino (%s) lockstat.st_ino (%s) mystat.st_rdev (%s) lockstat.st_rdev (%s)" % (mystat.st_ino, lockstat.st_ino,mystat.st_rdev,lockstat.st_rdev)
                # same unique file and lockfile, so I have the lock
                #print "got lock for (%s)" % (LOCK,)
                LOCKLOCK = 1
                # clean out all old locks
                rawfiles = os.listdir(DBDIR)
                for filename in rawfiles :
                    #print "filename (%s)" % (filename,)
                    ulock_mo = ulock_reo.match(filename)
                    if ulock_mo != None and DBDIR + '/' + filename != MYLOCK:
                        #print "unlinking (%s)" % (filename,)
                        #try:
                        #    os.unlink(DBDIR + '/' + filename)
                        #except:
                        #    continue_var = raw_input('continue? ')
                        try:
                            os.unlink(DBDIR + '/' + filename)
                        except exceptions.OSError:
                            pass
                break
            else:
                #print "lock mismatch mystat.st_ino (%s) lockstat.st_ino (%s) mystat.st_rdev (%s) lockstat.st_rdev (%s)" % (mystat.st_ino, lockstat.st_ino,mystat.st_rdev,lockstat.st_rdev)
                #print "failed to get lock for (%s)" % (LOCK,)
                # I did not get the lock.  check to see if the LOCK file
                # has timed out
                locktime = time.time()
                #print "locktime - float(lockstat.st_ctime) (%s) LOCKTIMEOUT (%s)" % (locktime - float(lockstat.st_ctime), float(LOCKTIMEOUT))
                if breakable == False:
                    break
                if locktime - float(lockstat.st_ctime) > float(timeout):
                    # get the lock to unlock
                    print "breaking lock for (%s) lockstat.st_ctime (%s) LOCKTIMEOUT (%s)" % (db_name, lockstat.st_ctime, LOCKTIMEOUT)
                    #recipient = MAIL_RECIPIENT
                    #subject = "Attempting to break old lock for DB (%s)" % (db_name,)
                    #message = """Catalina database lock break attempt for
        #db: (%s)""" % db_name + '\n'
                    #print "message (%s)" % message
                    #warn(message, subject, recipient)
                    #catsyslog(message,'warning')

                    #UNLOCK = LOCK + '.unlocking'
                    #MYUNLOCK = DBDIR + '/' + db_name + '.unlocking.' + myserver + '.' + "%s" % mypid
                    #FO = open(MYUNLOCK,'w')
                    #FO.close()
                    #try:
                    #    rc = os.link(MYUNLOCK,UNLOCK)
                    #except:
                    #    pass
                    #myunlockstat = os.stat(MYUNLOCK)
                    #unlockstat = os.stat(UNLOCK)
                    #if myunlockstat.st_ino == unlockstat.st_ino and myunlockstat.st_rdev == unlockstat.st_rdev:
                    #    # got the unlock lock
                    #    UNLOCKLOCK = 1
                    #    os.unlink(LOCK)
                    #     # release the unlock lock
                    #    os.unlink(UNLOCK)
                    #    os.unlink(MYUNLOCK)
                    try:
                        os.unlink(LOCK)
                    except exceptions.OSError:
                        continue
                else:
                    pass
            time.sleep(1)
    else :
        # open the reservations file for read, set non-exclusive lock
        #LOCKFO = open(LOCK, 'r')
        #fcntl.lockf(LOCKFO.fileno(), fcntl.LOCK_SH)
        pass
    if db_mode != 'write' or LOCKLOCK == 1:
        print "Received %s db lock" % db_name
        #db_tuple = ( LOCKFO, db_name, db_mode )
        db_tuple = ( LOCK, db_name, db_mode )
    else:
        print "Failed %s db lock!" % db_name
        raise 'LockDBFile', "open of writable db failed"
    return db_tuple

def unlock_db_files (db_tuple) :
    db_mode = db_tuple[2]
    if db_mode == 'write':
        db_name = db_tuple[1]
        #print "unlocking (%s)" % db_name
        ulock_pat = db_name + '\.unlocking\.' + "\S+" + '\.' + "\d+"
        ulock_reo = re.compile(ulock_pat)
        myserver = socket.gethostname()
        mypid = os.getpid()
        #print "unlock mypid (%s)" % mypid
        LOCK = DBDIR + '/' + db_name + LOCK_SUFFIX
        MYLOCK = DBDIR + '/' + db_name + LOCK_SUFFIX + '.' + myserver + '.' + "%s" % mypid
        #print "unlock MYLOCK (%s)" % (MYLOCK,)
        UNLOCK = LOCK + '.unlocking'
        MYUNLOCK = DBDIR + '/' + db_name + '.unlocking.' + myserver + '.' + "%s" % mypid
        #LOCKFO = db_tuple[0]
        uid_int = pwd.getpwnam(CAT_LOCK_OWNER)[2]
        gid_int = grp.getgrnam(CAT_LOCK_GROUP)[2]
        try :
            os.chmod(LOCK,0664)
            os.chown(LOCK,uid_int,gid_int)
            os.chmod(UNLOCK,0664)
            os.chown(UNLOCK,uid_int,gid_int)
        except :
            try :
                if DEBUG == 'locks' :
                    info_tuple = sys.exc_info()
                    print "(%s) (%s) (%s)" % info_tuple
                    info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                    traceback.print_tb(info_tuple[2])
                    tb_list = traceback.format_tb(info_tuple[2])
                    info_list = info_list + tb_list
                    tb_text = string.join(info_list)
                    print tb_text
            except :
                print "print of sys.exc_info() failed!"
        # release lock, close the file
        #fcntl.lockf(LOCKFO.fileno(), fcntl.LOCK_UN)
        try:
            mystat = os.stat(MYLOCK)
            lockstat = os.stat(LOCK)
        except:
            # could not stat one of these, assume someone else broke my locks
            pass
        else:
            if mystat.st_ino == lockstat.st_ino and mystat.st_rdev == lockstat.st_rdev:
                # still my lock, so unlock it
                # get the lock to unlock
                #FO = open(MYUNLOCK,'w')
                #FO.close()
                #try:
                #    rc = os.link(MYUNLOCK,UNLOCK)
                #except:
                #    pass
                #myunlockstat = os.stat(MYUNLOCK)
                #unlockstat = os.stat(UNLOCK)
                #if myunlockstat.st_ino == unlockstat.st_ino and myunlockstat.st_rdev == unlockstat.st_rdev:
                #    # got the unlock lock
                #    UNLOCKLOCK = 1
                #    rawfiles = os.listdir(DBDIR)
                #    for filename in rawfiles :
                #        #print "filename (%s)" % (filename,)
                #        ulock_mo = ulock_reo.match(filename)
                #        if ulock_mo != None and DBDIR + '/' + filename != MYUNLOCK:
                #            #print "unlinking (%s)" % (filename,)
                #            #try:
                #            #    os.unlink(DBDIR + '/' + filename)
                #            #except:
                #            #    continue_var = raw_input('continue? ')
                #            os.unlink(DBDIR + '/' + filename)
                #    #print "unlinking (%s)" % (LOCK,)
                #    os.unlink(LOCK)
                #    #print "unlinking (%s)" % (MYLOCK,)
                #    os.unlink(MYLOCK)
                #     # release the unlock lock
                #    os.unlink(UNLOCK)
                #    os.unlink(MYUNLOCK)
                #os.unlink(UNLOCK)
                #os.unlink(MYUNLOCK)
                os.unlink(LOCK)
                os.unlink(MYLOCK)
                #else:
                #    print "unlock mismatch myunlockstat.st_ino (%s) unlockstat.st_ino (%s) myunlockstat.st_rdev (%s) unlockstat.st_rdev (%s)" % (myunlockstat.st_ino, unlockstat.st_ino,myunlockstat.st_rdev,unlockstat.st_rdev)
    print "Released %s db %s lock" % (db_tuple[1], db_tuple[2])
    #LOCKFO.close()

def initialize_db(db_name) :
    #LOCK = DBDIR + '/' + db_name + LOCK_SUFFIX
    #LOCKFO = open(LOCK, 'a')
    #LOCKFO.close()
    lock_uid = pwd.getpwnam(CAT_LOCK_OWNER)[2]
    lock_gid = grp.getgrnam(CAT_LOCK_GROUP)[2]
    #os.chown(LOCK, lock_uid, lock_gid)
    #os.chmod(LOCK, 0664)
    db_tuple = lock_db_files(db_name, db_mode='write')
    FILE = DBDIR + '/' + db_name
    FO = open(FILE, 'wb')
    empty_db = {}
    
    # high performance pickler in binary mode, with fast mode
    pickle = cPickle.Pickler(FO,1)
    pickle.fast = 1
    
    pickle.dump(empty_db)
    FO.close()
    os.chown(FILE, lock_uid, lock_gid)
    os.chmod(FILE, 0664)
    unlock_db_files(db_tuple)
    RO_DB_NAME = db_name + '_readonly'
    #ROLOCK = DBDIR + '/' + RO_DB_NAME + LOCK_SUFFIX
    #ROLOCKFO = open(ROLOCK, 'a')
    #ROLOCKFO.close()
    #os.chown(ROLOCK, lock_uid, lock_gid)
    #os.chmod(ROLOCK, 0664)
    ro_db_tuple = lock_db_files(RO_DB_NAME, db_mode='write')
    ROFILE = DBDIR + '/' + RO_DB_NAME
    ROFO = open(ROFILE, 'wb')
    empty_db = {}
    
    # high performance pickler in binary mode, with fast mode
    pickle = cPickle.Pickler(ROFO,1)
    pickle.fast = 1
    
    pickle.dump(empty_db)
    ROFO.close()
    os.chown(ROFILE, lock_uid, lock_gid)
    os.chmod(ROFILE, 0664)
    unlock_db_files(ro_db_tuple)

def open_db (db_name, db_mode) :
    db_tuple = lock_db_files(db_name, db_mode)
    REMOTEFILE = DBDIR + '/' + db_name
    global DB_WARN_COUNT
    try :
        nonempty = 0
        for i in range(60) :
            FO = open(REMOTEFILE, 'rb')
            #FO = os.open(REMOTEFILE, 'rb')
            contents = FO.read()
            if len(contents) == 0 :
                FO.close()
                continue
            else :
                contents = None
                #FO.seek(0)
                FO.close()
                nonempty = 1
                break
        contents = None
        if nonempty == 1 :
            FO = open(REMOTEFILE, 'rb')
            good_pickle = 0
            for i in range(60) :
                try :
                    dict = cPickle.load(FO)
                except :
                    if i == 59 :
                        info_tuple = sys.exc_info()
                        info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                        tb_list = traceback.format_tb(info_tuple[2])
                        info_list = info_list + tb_list
                        tb_text = string.join(info_list)
                        recipient = MAIL_RECIPIENT
                        subject = "Attempting to correct DB corruption"
                        message = """Catalina database corruption detected for
db: (%s)""" % REMOTEFILE + '\n' + "file.tell() (%s)\n" % FO.tell() + tb_text
                        print "message (%s)" % message
                        warn(message, subject, recipient)
                        catsyslog(message,'warning')
                    FO.seek(0)
                    continue
                else :
                    good_pickle = 1
                    break
            if good_pickle == 0 :
                shutil.copy(REMOTEFILE,'/tmp')
                FO.close()
                raise 'BadPickle', "open of writable db failed, 60x bad pickle (saved to /tmp)"
            FO.close()
        else :
            #FO.close()
            raise 'EmptyDBFile', "open of writable db failed, 60x empty file"
    except :
        if DB_WARN_LIMIT == None or DB_WARN_COUNT < DB_WARN_LIMIT :
            info_tuple = sys.exc_info()
            info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
            tb_list = traceback.format_tb(info_tuple[2])
            info_list = info_list + tb_list
            tb_text = string.join(info_list)
            recipient = MAIL_RECIPIENT
            subject = "Attempting to correct DB corruption"
            message = """Catalina database corruption detected for
db: (%s)""" % REMOTEFILE + '\n' + tb_text
            print "message (%s)" % message
            warn(message, subject, recipient)
            catsyslog(message,'warning')
            DB_WARN_COUNT = DB_WARN_COUNT + 1
        try :
            RO_DB_NAME = db_name + '_readonly'
            ROFILE = DBDIR + '/' + RO_DB_NAME
            FO = open(ROFILE, 'rb')
            print "uncollectable objects (%s)" % gc.collect()
            dict = cPickle.load(FO)
            FO.close()
            FO = open(REMOTEFILE, 'wb')
            pickle = cPickle.Pickler(FO,1)
            pickle.fast = 1
            pickle.dump(dict)
            FO.flush()
            FO.close()
            FO = open(REMOTEFILE, 'rb')
            dict = cPickle.load(FO)
            FO.close()
        except :
            try :
                info_tuple = sys.exc_info()
                print "(%s) (%s) (%s)" % info_tuple
                info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                traceback.print_tb(info_tuple[2])
                tb_list = traceback.format_tb(info_tuple[2])
                info_list = info_list + tb_list
                tb_text = string.join(info_list)
                print tb_text
                if DB_WARN_LIMIT == None or DB_WARN_COUNT < DB_WARN_LIMIT :
                    recipient = MAIL_RECIPIENT
                    subject = "Failed to correct DB corruption"
                    message = """Catalina database corruption could not be fixed for
    db: (%s)""" % REMOTEFILE
                    warn(message, subject, recipient)
                    catsyslog(message,'warning')
            except :
                print "print of sys.exc_info() failed!"
            if DB_WARN_LIMIT == None or DB_WARN_COUNT < DB_WARN_LIMIT :
                recipient = MAIL_RECIPIENT
                subject = "Failed to correct DB corruption"
                message = """Catalina database corruption could not be fixed for
db: (%s)""" % REMOTEFILE
                warn(message, subject, recipient)
                catsyslog(message,'warning')
                DB_WARN_COUNT = DB_WARN_COUNT + 1
            raise 'CatalinaDBError', "open of writable db failed, uncorrectable"
        else :
            if DB_WARN_LIMIT == None or DB_WARN_COUNT < DB_WARN_LIMIT :
                recipient = MAIL_RECIPIENT
                subject = "DB recovery completed"
                message = """Catalina database recovery completed for
db: (%s)
The _readonly verion of the db has been copied back.
Some information from the most recent scheduling iteration
may have been lost.""" % REMOTEFILE
                warn(message, subject, recipient)
                catsyslog(message,'notice')
                DB_WARN_COUNT = 0
    db_handle = ( dict, db_tuple )
    return db_handle

def open_ro_db (db_name, db_mode) :
    RO_DB_NAME = db_name + '_readonly'
    db_tuple = (None, db_name, 'read')
    ROFILE = DBDIR + '/' + RO_DB_NAME
    ROFO = None
    tries = 0
    while tries < 60 :
        try :
            ROFO = open(ROFILE, 'rb')
            dict = cPickle.load(ROFO)
            ROFO.close()
        except :
            if ROFO != None :
                ROFO.close()
            print "Error retrying"
            time.sleep(1)
            print "after time.sleep(1)"
            tries = tries + 1
        else :
            break
    if tries >= 60 :
        raise 'CatalinaDBError', "open of read-only db failed"
    db_handle = ( dict, db_tuple )
    return db_handle

def close_ro_db (db_handle) :
    dict = db_handle[0]
    db_tuple = db_handle[1]
    db_name = db_tuple[1]
    db_tuple = db_handle[1]

def close_db (db_handle) :
    dict = db_handle[0]
    db_tuple = db_handle[1]
    db_name = db_tuple[1]
    db_mode = db_tuple[2]
    db_tuple = db_handle[1]
    if db_mode == 'write':
        # if another process has broken the lock for db_name,
        # we skip writing out our version.  There is still a
        # window where the other process has broken the lock,
        # but we miss it, so we inadvertantly write our stuff out
        still_locked = check_lock(db_name)
        # get a write lock
        if still_locked == False:
            raise 'CatalinaDBError', "lost lock for db (%s)" % (db_name,)
        #writing_db_tuple = lock_db_files(db_name+'-writing', db_mode, timeout=0, breakable=False)

        ROFILE = DBDIR + '/' + db_name + '_readonly'
        #shutil.copyfile(REMOTEFILE, ROFILE)
        #FD = os.open(ROFILE, os.O_DIRECT|os.O_SYNC|os.O_WRONLY)
        FD = os.open(ROFILE, os.O_SYNC|os.O_WRONLY)
        #print "ROFILE (%s)" % (ROFILE,)
        #print "type(FD) (%s)" % (type(FD),)
        #FO = os.fdopen(FD,'w',0)
        #FO = os.fdopen(FD,'wb')
        FO = os.fdopen(FD,'wb')
        #print "type(FO) (%s)" % (type(FO),)
        #pickle = cPickle.Pickler(FO,1)
        pickle = cPickle.Pickler(FO,cPickle.HIGHEST_PROTOCOL)
        #pickle.fast = 1
        if DEBUG == 'cd' :
            print "writing out (%s)" % db_name
        pickle.dump(dict)
        #print "type(FO) (%s)" % (type(FO),)
        FO.flush()
        os.fsync(FO.fileno())
        FO.close()
        #os.close(FD)

        # if other broke my lock, bail out before
        # the primary database is corrupted.
        # if I still have the lock, the backup
        # database should be good.
        still_locked = check_lock(db_name)
        if still_locked == False:
            raise 'CatalinaDBError', "lost lock for db (%s)" % (db_name,)

        REMOTEFILE = DBDIR + '/' + db_name
        #FO = open(REMOTEFILE, 'wb')
        #FD = os.open(REMOTEFILE, os.O_DIRECT|os.O_SYNC|os.O_WRONLY)
        FD = os.open(REMOTEFILE, os.O_SYNC|os.O_WRONLY)
        #FO = os.fdopen(FD,'w',0)
        FO = os.fdopen(FD,'wb')
        #pickle = cPickle.Pickler(FO,1)
        pickle = cPickle.Pickler(FO,cPickle.HIGHEST_PROTOCOL)
        #pickle.fast = 1
        if DEBUG == 'cd' :
            print "writing out (%s)" % db_name
        pickle.dump(dict)
        FO.flush()
        os.fsync(FO.fileno())
        FO.close()
        #os.close(FD)

        uid_int = pwd.getpwnam(CAT_LOCK_OWNER)[2]
        gid_int = grp.getgrnam(CAT_LOCK_GROUP)[2]
        try :
            os.chmod(REMOTEFILE,0664)
            os.chmod(ROFILE,0664)
            os.chown(REMOTEFILE,uid_int,gid_int)
            os.chown(ROFILE,uid_int,gid_int)
        except :
            try :
                if DEBUG == 'locks' :
                    info_tuple = sys.exc_info()
                    print "(%s) (%s) (%s)" % info_tuple
                    info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                    traceback.print_tb(info_tuple[2])
                    tb_list = traceback.format_tb(info_tuple[2])
                    info_list = info_list + tb_list
                    tb_text = string.join(info_list)
                    print tb_text
            except :
                print "print of sys.exc_info() failed!"
        # stat of the file seems to sync it up in NFS...
        file_stat = os.stat(REMOTEFILE)
        file_stat = os.stat(ROFILE)
        # seeing DB corruption, try to open db at close.
        try :
            FO = open(REMOTEFILE, 'rb')
            db = cPickle.load(FO)
            FO.close()
        except :
            info_tuple = sys.exc_info()
            info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
            tb_list = traceback.format_tb(info_tuple[2])
            info_list = info_list + tb_list
            tb_text = string.join(info_list)
            recipient = MAIL_RECIPIENT
            subject = "Close DB corruption"
            message = """Catalina database corruption detected for
db: %s""" % REMOTEFILE + '\n' + "file.tell() (%s)\n" % FO.tell() + tb_text
            print "message (%s)" % message
            warn(message, subject, recipient)
            catsyslog(message,'warning')
        #unlock_db_files(writing_db_tuple)
    else:
        #print "db_mode (%s)" % (db_mode,)
        pass
    unlock_db_files(db_tuple)

def apply_policy_code(code_string,input_tuple) :
    result = None
    # if code_string is compiled code, result does not get set...
    #if DEBUGJOB != None :
    #print "code_string (%s)" % code_string
    exec code_string
    #if DEBUGJOB != None :
    #print "result (%s) for input_tuple[0] (%s)" % (result, input_tuple[0])
    return result

def get_broken_reservations_tuple(
  reservations_db_handle,
  resource_db_handle,
  jobs_db_handle,
  res_id=None) :
    exit_status = 0
    # Retrieve a dictionary of reservations:
    reservations_dict = reservations_db_handle[0]
    if res_id == None :
        check_list = filter(lambda x : x['purpose_type_string'] in ['generic', 'user_set'], get_object_list(reservations_db_handle))
    else :
        if not reservations_dict.has_key(res_id) :
            raise 'KeyNotFound', res_id
        check_list = [reservations_dict[res_id],]
    reservations_list = get_object_list(reservations_db_handle)
    jobs_dict = jobs_db_handle[0]
    report_dict = {}
    exit_status = 0
    for res in check_list :
        report_dict[res['name']] = {'overlap_dict' : {}, 'down_nodes_list' : [] , 'reservation' : None}
        report_dict[res['name']]['reservation'] = res
        # initialize overlap dictionary.  key is reservation name, value
        # is reservation
        overlap_dict = {}
        time_overlap_list = filter(
          lambda x, res=res : \
          ((res['start_time_float'] <= x['start_time_float'] < res['end_time_float']) or\
           (res['start_time_float'] < x['end_time_float'] <= res['end_time_float']) or \
           (x['start_time_float'] <= res['start_time_float'] < x['end_time_float']) or \
           (x['start_time_float'] < res['end_time_float'] < x['end_time_float'])) \
           and x['name'] != res['name'],
          reservations_list
          )
        for reservation in time_overlap_list :
            for node in reservation['node_list'] :
                if node not in res['node_list'] :
                    continue
                if reservation['purpose_type_string'] == 'job' :
                    continue
                if reservation['purpose_type_string'] == 'running' :
                    runID_string = reservation['job_runID']
                    job_step = jobs_dict[runID_string]
                    input_tuple = ( job_step, )
                    result = apply_policy_code(res['job_restriction'],
                      input_tuple)
                    if result == 0 :
                        continue
                overlap_dict[reservation['name']] = reservation
        if len(overlap_dict.keys()) > 0 :
            report_dict[res['name']]['overlap_dict'] = overlap_dict
        # Check for down nodes
        resource_list = get_object_list(resource_db_handle)
        down_nodes_list = filter(lambda x, res=res : \
          x['name'] in res['node_list'] and x['State'] not in ['Idle','Running'], \
          resource_list)
        if len(down_nodes_list) > 0 :
            report_dict[res['name']]['down_nodes_list'] = down_nodes_list
        if len(overlap_dict.keys()) > 0 or len(down_nodes_list) > 0 :
            exit_status = 1
    return (report_dict, exit_status)

def get_accepted_nodes_list(node_restriction_code, resource_db_handle) :
    #normal_re = re.compile(".*normal.*")
    dict = resource_db_handle[0]
    resource_list = dict.values()
    accepted_nodes_list = []
    for resource in resource_list :
        #print "doing node (%s)" % (resource['name'],)
        if node_restriction_code != None :
            #if DEBUGJOB != None :
            #print "node_restriction_code (%s)" % node_restriction_code
            input_tuple = (resource,)
            acceptance = apply_policy_code(node_restriction_code, input_tuple)
            #if DEBUGJOB != None :
            #print "acceptance (%s) for resource (%s)" % (acceptance,resource['name'])
        elif resource['State'] == 'Down' :
            acceptance = 'Down'
        elif resource['State'] == 'Drain' :
            acceptance = 'Drain'
        elif resource['State'] == 'Drained' :
            acceptance = 'Drained'
        elif resource['State'] == 'None' :
            acceptance = 'None'
        elif resource['State'] == None :
            acceptance = None
        elif resource['State'] == 'Unknown' :
            acceptance = 'Unknown'
        #elif resource['Max_Starters'] == 0 :
        #    acceptance = 'Max_Starters=0'
        #elif normal_re.search(resource['ConfiguredClasses']) == None :
        #    acceptance = 'NoNormalClass'
        else :
            acceptance = 0
        if acceptance != 0 :
            #print "acceptance (%s)" % (acceptance,)
            continue
        #print "appending (%s)" % (resource['name'],)
        accepted_nodes_list.append(resource['name'])
    return accepted_nodes_list

def get_object_list(db_handle) :
    dict = db_handle[0]
    list = dict.values()
    return list

def get_object_names_list(object_list) :
    def get_name(object) : return object['name']
    list = map( get_name, object_list )
    return list

def get_object(key, db_handle) :
    dict = db_handle[0]
    if dict.has_key(key) :
        return dict[key]
    else :
        raise 'KeyNotFound', key

#def get_screened_nodes(job_step_id, accepted_nodes_list, jobs_db_handle, resources_db_handle) :
#    jobs_dict = jobs_db_handle[0]
#    job_step = jobs_dict[job_step_id]
#    if job_step.has_key('resource_list') :
#        screened_resource_list = job_step['resource_list']
#    else :
#        raise 'NoResourceList', job_step
#    screened_resource_name_list = get_resource_name_list(screened_resource_list)
#    screened_nodes_list = []
#    for node in accepted_nodes_list :
#        if node in screened_resource_name_list :
#            screened_nodes_list.append(node)
#    return screened_nodes_list

def get_open_windows_list(accepted_nodes_list, new_res, temp_reservations_list, resources_db_handle) :
    def get_nonconflicting(input_tuple) :
        # This python code fragment can be used to obtain a list of available
        # node reservation windows.
        # It works on input_tuple containing
        # ( <nodes matching requirements>, <the requested reservation>,
        #   <all existing reservations> , Now)
        # returns a list, each element of which is a tuple:
        # ( <window start_time_float>, <window end_time_float>, <node name> )
        # To accomodate cpu and memory scheduling, need to return
        # ( <window start_time_float>, <window end_time_float>, { 'nodename' : <node name>, 'cpu' : <cpus>, 'memory' : <memory>} )
        def sort_by_start(first, second) :
            if first[0] < second[0] :
                return -1
            if first[0] == second[0] :
                return 0
            if first[0] > second[0] :
                return 1
        accepted_nodes = input_tuple[0]
        new_res = input_tuple[1]
        reservations = input_tuple[2]
        resources_db_handle = input_tuple[4]
        resources_dict = resources_db_handle[0]
        end_limit = END_OF_SCHEDULING
        reservation_windows = []
        node_reservations = {}
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "len(accepted_nodes) (%s)" % len(accepted_nodes)
        #if new_res['purpose_type_string'] in ['preempted_job',] :
        #    print "reservations (%s)" % (reservations,)
        for reservation in reservations :
            #print "reservation (%s)" % reservation
            #if reservation['purpose_type_string'] == 'running' :
            #    print "reservation name (%s) purpose (%s) start (%s) end (%s)" % \
            #      (reservation['name'], reservation['purpose_type_string'], reservation['start_time_float'], reservation['end_time_float'])
            #    print "running reservation (%s)" % reservation
            #print "reservation name (%s) purpose (%s) start (%s) end (%s)" % \
            #  (reservation['name'], reservation['purpose_type_string'], reservation['start_time_float'], reservation['end_time_float'])
            res_id = reservation['name']
            node_list = reservation['node_list']
            if reservation.has_key('allocated_dict_list') and reservation['allocated_dict_list'] != None :
                allocated_dict_list = reservation['allocated_dict_list']
                #Kenneth patch leaking reservation. 7/24/06
                for allocated_dict in allocated_dict_list:
                    #print "considering allocated_dict (%s)" % (allocated_dict,)
                    if allocated_dict.has_key('type') and allocated_dict['type'] == 'node_exclusive' and allocated_dict.has_key('nodename'):
                        #print "handling allocated_dict (%s)" % (allocated_dict,)
                        #if not resources_dict.has_key(allocated_dict['nodename']) :
                        #    print "resources_dict does not have (%s)" % (allocated_dict['nodename'],)
                        #    sys.exit(1)
                        #if resources_dict.has_key(allocated_dict['nodename']) :
                        if resources_dict.has_key(allocated_dict['nodename']):
                            if resources_dict[allocated_dict['nodename']].has_key('consumable_dict'):
                                #print "resources_dict[allocated_dict['nodename']] (%s)" % (resources_dict[allocated_dict['nodename']],)
                                for rkey in resources_dict[allocated_dict['nodename']]['consumable_dict'].keys():
                                    allocated_dict[rkey] = resources_dict[allocated_dict['nodename']]['consumable_dict'][rkey]
                                    # should allocated_dict['node'] = 1 here?

                                #if resources_dict[allocated_dict['nodename']].has_key('ConsumableCpus'):
                                #    allocated_dict['cpu'] = resources_dict[allocated_dict['nodename']]['ConsumableCpus']
                                #if resources_dict[allocated_dict['nodename']].has_key('ConsumableMemory'):
                                #    allocated_dict['memory'] = resources_dict[allocated_dict['nodename']]['ConsumableMemory']
                            else:
                                allocated_dict = {}
                        else :
                            #for rkey in resources_dict[allocated_dict['nodename']]['consumable_dict'].keys():
                            #    allocated_dict[rkey] = 0
                            allocated_dict = {}
                            #allocated_dict['cpu'] = 0
                            #allocated_dict['memory'] = 0
                #end of patch
                    #else:
                    #    print "not doing anything for (%s)" % (allocated_dict,)

                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "existing allocated_dict_list (%s)" % (allocated_dict_list,)
            else :
                allocated_dict_list = []
                for node in reservation['node_list'] :
                    #if DEBUGJOB != None :
                    #print "node (%s)" % node
                    #if resources_dict.has_key(node) and resources_dict[node].has_key('ConsumableCpus') and resources_dict[node].has_key('ConsumableMemory') :
                    #    allocated_consumablecpus = resources_dict[node]['ConsumableCpus']
                    #    allocated_consumablememory = resources_dict[node]['ConsumableMemory']
                    #else :
                    #    allocated_consumablecpus = 0
                    #    allocated_consumablememory = 0
                    #allocated_dict_list.append(
                    #  { 'nodename' : node,
                    #    'type' : 'node_exclusive',
                    #    'node' : 1,
                    #    'cpu' : allocated_consumablecpus,
                    #    'memory' : allocated_consumablememory }
                    #  )

                    allocated_dict = { 'nodename' : node,
                      'type' : 'node_exclusive',
                      'node' : 1,}
                    for rkey in resources_dict[node]['consumable_dict'].keys():
                        allocated_dict[rkey] = resources_dict[node]['consumable_dict'][rkey]
                    allocated_dict_list.append(copy.deepcopy(allocated_dict))
                    #allocated_dict_list.append(allocated_dict)
                    #print "allocated_dict_list (%s)" % (allocated_dict_list,)

                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "generated allocated_dict_list (%s)" % (allocated_dict_list,)
            node_resource_list = filter(lambda x : x.has_key('nodename'), allocated_dict_list)
            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "node_resource_list (%s)" % (node_resource_list,)
            start_time_float = reservation['start_time_float']
            if reservation.has_key('end_time_float') :
                end_time_float = reservation['end_time_float']
            else :
                end_time_float = END_OF_SCHEDULING
            #print "getting open windows for node_resource_list (%s)" % (node_resource_list,)
            for index in range(len(node_resource_list)) :
                node = node_resource_list[index]['nodename']
                if node_resource_list[index].has_key('type'):
                    type = node_resource_list[index]['type']
                else:
                    type = 'node_exclusive'
                if not resources_dict.has_key(node) or not node in accepted_nodes :
                    #print "not in accepted_nodes_list, continuing. node_resource_list[index] (%s)" % (node_resource_list[index],)
                    continue
                if not node_reservations.has_key(node) :
                    node_reservations[node] = []
                    nr_dict = {}
                    nr_dict['nodename'] = node
                    #nr_dict['type'] = 'node_exclusive'
                    nr_dict['type'] = type
                    nr_dict['node'] = 1
                    for rkey in resources_dict[node]['consumable_dict'].keys():
                        nr_dict[rkey] = resources_dict[node]['consumable_dict'][rkey]
 
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        #print "node_reservations[node].append (%s)" % ( (0.0, 'increment', 
                      #{ 'nodename' : node, 'type' : 'node_exclusive',
                      #  'node' : 1,
                      #  'cpu' : resources_dict[node]['ConsumableCpus'],
                      #  'memory' : resources_dict[node]['ConsumableMemory']
                      #}
                      #),
                      #)
                        print "node_reservations[node].append (%s)" % (nr_dict,)
                    #print "node_reservations[node].append (%s)" % (nr_dict,)
                    #node_reservations[node].append( (0.0, 'add', 
                    #  { 'nodename' : node, 'type' : 'node_exclusive',
                    #    'node' : 1,
                    #    'cpu' : resources_dict[node]['ConsumableCpus'],
                    #    'memory' : resources_dict[node]['ConsumableMemory']
                    #  }
                    #  )
                    #  )
                    node_reservations[node].append( (0.0, 'add', copy.deepcopy(nr_dict)) )
                    #node_reservations[node].append( (END_OF_SCHEDULING, 'decrement', 
                    #  { 'nodename' : node, 'type' : 'node_exclusive',
                    #    'node' : 1,
                    #    'cpu' : resources_dict[node]['ConsumableCpus'],
                    #    'memory' : resources_dict[node]['ConsumableMemory']
                    #  } )
                    #  )
                    node_reservations[node].append( (END_OF_SCHEDULING, 'decrement', copy.deepcopy(nr_dict)) )
                if node in accepted_nodes :
                    if node_reservations.has_key(node) :
                        node_reservations[node].append( (start_time_float, 'decrement', node_resource_list[index]) )
                        node_reservations[node].append( (end_time_float, 'add', node_resource_list[index]) )
                    else :
                        node_reservations[node] = [ (start_time_float, 'decrement', node_resource_list[index]) ]
                        node_reservations[node].append( (end_time_float, 'add', node_resource_list[index]) )
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "appending node_reservation (%s, %s, %s)" % (start_time_float, end_time_float, node)
                        print "node_resource_list[index] (%s)" % (node_resource_list[index],)
                    #print "appending node_reservation (%s, %s, %s)" % (start_time_float, end_time_float, node)
                    #print "node_resource_list[index] (%s)" % (node_resource_list[index],)
        #potential_start_time_float = new_res['earliest_start_float']
        #if potential_start_time_float == None :
        #    potential_start_time_float = Now_float
        potential_start_time_float = 0.0
        #print "accepted_nodes (%s)" % (accepted_nodes,)
        for accepted_node in accepted_nodes :
            if not node_reservations.has_key(accepted_node) :
                accumulated_dict = {}
                # node is free, create window for Now_float
                #reservation_windows.append( (potential_start_time_float, end_limit, accepted_node) )
                # I need resource_dict here, to see ConsumableCpus and ConsumableMemory for the node...
                for rkey in resources_dict[accepted_node]['consumable_dict'].keys():
                    accumulated_dict[rkey] = resources_dict[accepted_node]['consumable_dict'][rkey]
                #if resources_dict[accepted_node].has_key('ConsumableCpus') :
                #    acConsumableCpus = resources_dict[accepted_node]['ConsumableCpus']
                #else :
                #    acConsumableCpus = 0
                #if resources_dict[accepted_node].has_key('ConsumableMemory') :
                #    acConsumableMemory = resources_dict[accepted_node]['ConsumableMemory']
                #else :
                #    acConsumableMemory = 0
                reservation_windows.append( (potential_start_time_float, end_limit, {'nodename' : accepted_node,
     'node' : 1}) )
     #           reservation_windows.append( (potential_start_time_float, end_limit, {'nodename' : accepted_node,
     #'cpu' : acConsumableCpus}) )
     #           reservation_windows.append( (potential_start_time_float, end_limit, {'nodename' : accepted_node,
     #'memory' : acConsumableMemory}) )

                for rkey in accumulated_dict.keys():
                    reservation_windows.append( (potential_start_time_float, end_limit, {'nodename' : accepted_node,
         rkey : accumulated_dict[rkey]}) )

                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "appending open window (%s, %s, %s)" % (potential_start_time_float, end_limit, accepted_node)
                    print "end_limit is (%s)" % time.asctime(time.localtime(end_limit))
                #print "appending open window (%s, %s, %s)" % (potential_start_time_float, end_limit, accepted_node)
                #print "end_limit is (%s)" % time.asctime(time.localtime(end_limit))
        # sort the reservations for each node, in ascending start_time_float order
        # keep a potential start time for the reservation window.
        # for each reservation, if the start_time_float - the potential start_time_float
        # is less than the duration_float, the window is too small (or negative)
        # instead of doing this by reservation, need to do it by event
        # + or - of cpus or memory.  Create an open window between each event
        for node in node_reservations.keys() :
            node_reservations[node].sort()
            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "node_reservations[node] (%s)" % (node_reservations[node],)
            #print "node_reservations[node] (%s)" % (node_reservations[node],)
            #if node == LICENSENODE:
            #    print "node_reservations[node] (%s)" % (node_reservations[node],)
            #potential_start_time_float = new_res['earliest_start_float']
            #if potential_start_time_float == None :
            #    potential_start_time_float = Now_float
            potential_start_time_float = 0.0
            #freenodes = 0
            #freecpus = 0
            #freememory = 0
            free_dict = {'node' : 0}
            for rkey in resources_dict[node]['consumable_dict'].keys():
                free_dict[rkey] = 0
            started_windows_list = []
            #negative_node = 0
            #negative_cpu = 0
            #negative_memory = 0
            negative_dict = {'node' : 0}
            for rkey in resources_dict[node]['consumable_dict'].keys():
                negative_dict[rkey] = 0
            for event in node_reservations[node] :
                if event[1] == 'add' :
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "add event (%s)" % (event,)
                    #print "add event (%s)" % (event,)
                    if node == LICENSENODE:
                        event_usage = 'node_shared'
                        #print "add event (%s)" % (event,)
                    else:
                        event_usage = new_res['node_usage']
                    #if event[2]['type'] == 'node_exclusive' :
                    #if new_res['node_usage'] == 'node_exclusive' :
                    if event_usage == 'node_exclusive' :
                        node_amount = 1
                    else :
                        if event[2].has_key('node') :
                            node_amount = event[2]['node']
                        else :
                            # FIXME? should this be 0 for shared?
                            node_amount = 1
                            #node_amount = 0
                    #if event[2].has_key('node') :
                    #    started_windows_list.append(
                    #      ( event[0],
                    #        { 'start' : event[0], 'type' : 'node',
                    #          'amount' : node_amount, 'state' : 'active'
                    #        }
                    #      )
                    #      )
                    #else :
                    #    # is this right?  should we really say this is the
                    #    # start of a time window for a free node?
                    #    started_windows_list.append(
                    #      ( event[0],
                    #        { 'start' : event[0], 'type' : 'node',
                    #          'amount' : 1, 'state' : 'active'
                    #        }
                    #      )
                    #      )
                    #started_windows_list.append(
                    #  ( event[0],
                    #    { 'start' : event[0], 'type' : 'cpu',
                    #      'amount' : event[2]['cpu'], 'state' : 'active'
                    #    }
                    #  )
                    #  )
                    #started_windows_list.append(
                    #  ( event[0],
                    #    { 'start' : event[0], 'type' : 'memory',
                    #      'amount' : event[2]['memory'], 'state' : 'active'
                    #    }
                    #  )
                    #  )
                    #if event[2].has_key('node') :
                    #    started_windows_list.append(
                    #      ( event[0],
                    #        { 'start' : event[0], 'type' : 'node',
                    #          'amount' : node_amount, 'state' : 'active'
                    #        }
                    #      )
                    #      )
                    #else :
                    #    started_windows_list.append(
                    #      ( event[0],
                    #        { 'start' : event[0], 'type' : event[2].keys()[0],
                    #          'amount' : event[2][event[2].keys()[0]], 'state' : 'active'
                    #        }
                    #      )
                    #      )
                    #add event ((0.0, 'add', {'node': 1, 'cpu': 16, 'type': 'node_exclusive', 'nodename': 'debian', 'memory': 1}))
                    for eventkey in event[2].keys():
                        if eventkey in ['type','nodename']:
                            continue
                        started_windows_list.append(
                          ( event[0],
                            { 'start' : event[0], 'type' : eventkey,
                              'amount' : event[2][eventkey], 'state' : 'active'
                            }
                          )
                          )
                if event[1] == 'decrement' :
                    # sort started_windows_list LASTAVAILABLE
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "decrement event(%s)" % (event,)
                    #print "decrement event(%s)" % (event,)
                    #if node == LICENSENODE:
                    #    print "decrement event(%s)" % (event,)
                    #sys.stderr.write("decrement event(%s)\n" % (event,))
                    #if negative_cpu > 0 :
                    #    # choose early cpu starts first, to get
                    #    # rid of cpu debt
                    #    cpu_started_windows_list.sort()
                    #else :
                    #    cpu_started_windows_list.sort()
                    #    cpu_started_windows_list.reverse()
                    #if negative_memory > 0 :
                    #    # choose early memory starts first, to get
                    #    # rid of memory debt
                    #    memory_started_windows_list.sort()
                    #else :
                    #    memory_started_windows_list.sort()
                    #    memory_started_windows_list.reverse()
                    started_windows_list.sort()
                    started_windows_list.reverse()
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "started_windows_list (%s)" % (started_windows_list,)
                    #foundnodes = 0
                    #foundcpus = 0
                    #foundmemory = 0
                    found_dict = {'node' : 0}
                    for rkey in resources_dict[node]['consumable_dict'].keys():
                        found_dict[rkey] = 0
                    #if event[2]['type'] == 'node_exclusive' :
                    #    neednode = event[2]['node']
                    #else :
                    #    neednode = 0
                    need_dict = {}
                    #if new_res['node_usage'] == 'node_exclusive' :
                    if event_usage == 'node_exclusive' :
                        #neednodes = 1
                        need_dict['node'] = 1
                    else :
                        if event[2].has_key('node') :
                            #neednodes = event[2]['node']
                            need_dict['node'] = event[2]['node']
                        else :
                            #neednodes = 1
                            # FIXME? should this be 0 for shared?
                            need_dict['node'] = 1
                            #need_dict['node'] = 0
                    #needcpus = event[2]['cpu']
                    #needmemory = event[2]['memory']
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "adding event[2] (%s) to need_dict" % (event[2],)
                    #sys.stderr.write("adding event[2] (%s) to need_dict\n" % (event[2],))
                    for ekey in event[2].keys():
                        if ekey == 'nodename':
                            continue
                        need_dict[ekey] = event[2][ekey]
                    new_windows_list = []

                    # pay rkey debt
                    #if negative_node > 0 :
                    extra_dict = {}
                    for rkey in negative_dict.keys():
                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "negative_dict[%s] (%s)" % (rkey,negative_dict[rkey])
                        # choose early rkey starts first, to get
                        # rid of rkey debt
                        started_windows_list.sort()
                        #extranodes = 0
                        extra_dict[rkey] = 0
                        for index in range(len(started_windows_list)) :
                            started_window = started_windows_list[index]
                            if started_window[1]['state'] == 'inactive' :
                                continue
                            #if started_window[1]['type'] == 'node' :
                            if started_window[1]['type'] == rkey :
                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "started_window (%s)" % (started_window,)
                                if negative_dict[rkey] == 0 :
                                    break
                                found_dict[rkey] = started_window[1]['amount']
                                if found_dict[rkey] >= negative_dict[rkey] :
                                    extra_dict[rkey] = found_dict[rkey] - negative_dict[rkey]
                                    negative_dict[rkey] = 0
                                    if extra_dict[rkey] > 0 :
                                        new_started_window = copy.deepcopy(started_window)
                                        new_started_window[1]['amount'] = extra_dict[rkey]
                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "new_started_window (%s)" % (new_started_window,)
                                        new_windows_list.append(new_started_window)
                                else :
                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "setting negative_dict[%s] to negative_dict[%s] (%s) - found_dict[%s] (%s)" % (rkey, rkey, negative_dict[rkey], rkey, found_dict[rkey])
                                    negative_dict[rkey] = negative_dict[rkey] - found_dict[rkey]
                                started_windows_list[index][1]['state'] = 'inactive'

#                    # pay node debt
#                    if negative_node > 0 :
#                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                            print "negative_node (%s)" % negative_node
#                        # choose early node starts first, to get
#                        # rid of cpu debt
#                        started_windows_list.sort()
#                        extranodes = 0
#                        for index in range(len(started_windows_list)) :
#                            started_window = started_windows_list[index]
#                            if started_window[1]['state'] == 'inactive' :
#                                continue
#                            if started_window[1]['type'] == 'node' :
#                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                    print "started_window (%s)" % (started_window,)
#                                if negative_node == 0 :
#                                    break
#                                foundnodes = started_window[1]['amount']
#                                if foundnodes >= negative_node :
#                                    extranodes = foundnodes - negative_node
#                                    negative_node = 0
#                                    if extranodes > 0 :
#                                        new_started_window = copy.deepcopy(started_window)
#                                        new_started_window[1]['amount'] = extranodes
#                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                            print "new_started_window (%s)" % (new_started_window,)
#                                        new_windows_list.append(new_started_window)
#                                else :
#                                    negative_node = negative_node - foundnodes
#                                started_windows_list[index][1]['state'] = 'inactive'
#
#                    # pay cpu debt
#                    if negative_cpu > 0 :
#                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                            print "negative_cpu (%s)" % negative_cpu
#                        # choose early cpu starts first, to get
#                        # rid of cpu debt
#                        started_windows_list.sort()
#                        extracpus = 0
#                        for index in range(len(started_windows_list)) :
#                            started_window = started_windows_list[index]
#                            if started_window[1]['state'] == 'inactive' :
#                                continue
#                            if started_window[1]['type'] == 'cpu' :
#                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                    print "started_window (%s)" % (started_window,)
#                                if negative_cpu == 0 :
#                                    break
#                                foundcpus = started_window[1]['amount']
#                                if foundcpus >= negative_cpu :
#                                    extracpus = foundcpus - negative_cpu
#                                    negative_cpu = 0
#                                    if extracpus > 0 :
#                                        new_started_window = copy.deepcopy(started_window)
#                                        new_started_window[1]['amount'] = extracpus
#                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                            print "new_started_window (%s)" % (new_started_window,)
#                                        new_windows_list.append(new_started_window)
#                                else :
#                                    negative_cpu = negative_cpu - foundcpus
#                                started_windows_list[index][1]['state'] = 'inactive'
#
#
#
#
#                    # pay memory debt
#                    if negative_memory > 0 :
#                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                            print "negative_memory (%s)" % negative_memory
#                        # choose early memory starts first, to get
#                        # rid of memory debt
#                        started_windows_list.sort()
#                        extramemory = 0
#                        for index in range(len(started_windows_list)) :
#                            started_window = started_windows_list[index]
#                            if started_window[1]['state'] == 'inactive' :
#                                continue
#                            if started_window[1]['type'] == 'memory' :
#                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                    print "started_window (%s)" % (started_window,)
#                                if negative_memory == 0 :
#                                    break
#                                foundmemory = started_window[1]['amount']
#                                if foundmemory >= negative_memory :
#                                    extramemory = foundmemory - negative_memory
#                                    negative_memory = 0
#                                    if extramemory > 0 :
#                                        new_started_window = copy.deepcopy(started_window)
#                                        new_started_window[1]['amount'] = extramemory
#                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                            print "new_started_window (%s)" % (new_started_window,)
#                                        new_windows_list.append(new_started_window)
#                                else :
#                                    negative_memory = negative_memory - foundmemory
#                                started_windows_list[index][1]['state'] = 'inactive'


                    started_windows_list = started_windows_list + new_windows_list
                    new_windows_list = []
                    started_windows_list.sort()
                    started_windows_list.reverse()
                    #foundnodes = 0
                    #foundcpus = 0
                    #foundmemory = 0
                    found_dict = {'node' : 0}
                    #print "node (%s)" % (resources_dict[node],)
                    for rkey in resources_dict[node]['consumable_dict'].keys():
                        found_dict[rkey] = 0
                        if not rkey in need_dict.keys():
                            need_dict[rkey] = 0
                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        #print "negative_dict['node'] (%s) negative_dict['cpu'] (%s) negative_dict['memory'] (%s)" % (negative_dict['node'], negative_dict['cpu'], negative_dict['memory'])
                        print "started_windows_list (%s)" % (started_windows_list,)
                    for index in range(len(started_windows_list)) :
                        started_window = started_windows_list[index]
                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "started_window (%s)" % (started_window,)
                        #print "%s started_window (%s)" % (new_res['job_runID'],started_window,)
                        #sys.stderr.write("started_window (%s)\n" % (started_window,))
                        if started_window[1]['state'] == 'inactive' :
                            continue

                        #print "looping through rkeys for started_window(%s)" % (started_window,)
                        for rkey in found_dict.keys():
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "rkey (%s)" % rkey
                                print "found_dict[rkey] (%s) need_dict[rkey] (%s)" % (found_dict[rkey], need_dict[rkey])
                                print "started_window[1] (%s)" % (started_window[1],)
                            #sys.stderr.write("rkey (%s)\n" % rkey)
                            #sys.stderr.write("found_dict[rkey] (%s) need_dict[rkey] (%s)\n" % (found_dict[rkey], need_dict[rkey]))
                            #sys.stderr.write("need_dict (%s)\n" % (need_dict,))
                            #sys.stderr.write("found_dict (%s)\n" % (found_dict,))
                            #sys.stderr.write("started_window[1] (%s)\n" % (started_window[1],))
                            if started_window[1]['type'] == rkey :
                                if found_dict[rkey] >= need_dict[rkey] :
                                    continue
                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "adding started_window[1]['amount'] (%s) to found_dict[%s] (%s)" % (started_window[1]['amount'], rkey, found_dict[rkey])
                                found_dict[rkey] = found_dict[rkey] + started_window[1]['amount']
                                if found_dict[rkey] >= 0 :
                                    negative_dict[rkey] = 0
                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "found_dict[rkey] (%s)" % found_dict[rkey]
                                    print "need_dict[rkey] (%s)" % need_dict[rkey]
                                if found_dict[rkey] >= need_dict[rkey] :
                                    extra_dict[rkey] = found_dict[rkey] - need_dict[rkey]
                                    if extra_dict[rkey] > 0 :
                                        new_started_window = copy.deepcopy(started_window)
                                        #new_started_window[1]['amount'] = extranodes
                                        new_started_window[1]['amount'] = extra_dict[rkey]
                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "new_started_window (%s)" % (new_started_window,)
                                        new_windows_list.append(new_started_window)
                                    if started_window[1]['start'] != event[0] :
                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "1. appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                              {'nodename' : node,
                                               rkey : started_window[1]['amount'] - extra_dict[rkey]}
                                              ),
                                              )
                                            print "start (%s) end (%s)" % (time.asctime(time.localtime(started_window[1]['start'])), time.asctime(time.localtime(event[0])))
                                        #print "%s 1. appending reservation_window (%s)" % (                                    new_res['job_runID'], (started_window[1]['start'], event[0],
                                        #  {'nodename' : node,
                                        #   rkey : started_window[1]['amount'] - extra_dict[rkey]}
                                        #  ),
                                        #  )
                                        #print "start (%s) end (%s)" % (time.asctime(time.localtime(started_window[1]['start'])), time.asctime(time.localtime(event[0])))
                                        reservation_windows.append(
                                          (started_window[1]['start'], event[0],
                                           {'nodename' : node,
                                            rkey : started_window[1]['amount'] - extra_dict[rkey]}
                                          )
                                          )
                                else :
                                    # remove started_window from list,
                                    # end the previously started windows
                                    # append to reservation_windows
                                    if started_window[1]['amount'] > 0 and found_dict[rkey] > 0 :
                                        if started_window[1]['start'] != event[0] :
                                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                                print "2. appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
                                                  {'nodename' : node,
                                                   rkey : started_window[1]['amount']}
                                                  ),
                                                  )
                                            #print "%s 2. appending reservation_window (%s)" % (                                     new_res['job_runID'],(started_window[1]['start'], event[0],
                                            #  {'nodename' : node,
                                            #   rkey : started_window[1]['amount']}
                                            #  ),
                                            #  )
                                            reservation_windows.append(
                                              (started_window[1]['start'], event[0],
                                               {'nodename' : node,
                                                rkey : started_window[1]['amount']}
                                              )
                                              )
                                started_windows_list[index][1]['state'] = 'inactive'
                        #print "after looping through rkeys for started_window(%s)" % (started_window,)

#                        if started_window[1]['type'] == 'node' :
#                            if foundnodes >= neednodes :
#                                continue
#                            foundnodes = foundnodes + started_window[1]['amount']
#                            if foundnodes >= 0 :
#                                negative_node = 0
#                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                print "foundnodes (%s)" % foundnodes
#                                print "neednodes (%s)" % neednodes
#                            if foundnodes >= neednodes :
#                                extranodes = foundnodes - neednodes
#                                if extranodes > 0 :
#                                    new_started_window = copy.deepcopy(started_window)
#                                    new_started_window[1]['amount'] = extranodes
#                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                        print "new_started_window (%s)" % (new_started_window,)
#                                    new_windows_list.append(new_started_window)
#                                if started_window[1]['start'] != event[0] :
#                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                        print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
#                                          {'nodename' : node,
#                                           'node' : started_window[1]['amount'] - extranodes}
#                                          ),
#                                          )
#                                        print "start (%s) end (%s)" % (time.asctime(time.localtime(started_window[1]['start'])), time.asctime(time.localtime(event[0])))
#                                    reservation_windows.append(
#                                      (started_window[1]['start'], event[0],
#                                       {'nodename' : node,
#                                        'node' : started_window[1]['amount'] - extranodes}
#                                      )
#                                      )
#                            else :
#                                # remove started_window from list,
#                                # end the previously started windows
#                                # append to reservation_windows
#                                if started_window[1]['amount'] > 0 and foundnodes > 0 :
#                                    if started_window[1]['start'] != event[0] :
#                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                            print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
#                                              {'nodename' : node,
#                                               'node' : started_window[1]['amount']}
#                                              ),
#                                              )
#                                        reservation_windows.append(
#                                          (started_window[1]['start'], event[0],
#                                           {'nodename' : node,
#                                            'node' : started_window[1]['amount']}
#                                          )
#                                          )
#                            started_windows_list[index][1]['state'] = 'inactive'
#
#
#                        if started_window[1]['type'] == 'cpu' :
#                            if foundcpus >= needcpus :
#                                continue
#                            foundcpus = foundcpus + started_window[1]['amount']
#                            if foundcpus >= 0 :
#                                negative_cpu = 0
#                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                print "foundcpus (%s)" % foundcpus
#                                print "needcpus (%s)" % needcpus
#                            if foundcpus >= needcpus :
#                                extracpus = foundcpus - needcpus
#                                if extracpus > 0 :
#                                    new_started_window = copy.deepcopy(started_window)
#                                    new_started_window[1]['amount'] = extracpus
#                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                        print "new_started_window (%s)" % (new_started_window,)
#                                    new_windows_list.append(new_started_window)
#                                if started_window[1]['start'] != event[0] :
#                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                        print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
#                                          {'nodename' : node,
#                                           'cpu' : started_window[1]['amount'] - extracpus}
#                                          ),
#                                          )
#                                    reservation_windows.append(
#                                      (started_window[1]['start'], event[0],
#                                       {'nodename' : node,
#                                        'cpu' : started_window[1]['amount'] - extracpus}
#                                      )
#                                      )
#                            else :
#                                # remove started_window from list,
#                                # end the previously started windows
#                                # append to reservation_windows
#                                if started_window[1]['amount'] > 0 and foundcpus > 0 :
#                                    if started_window[1]['start'] != event[0] :
#                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                            print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
#                                              {'nodename' : node,
#                                               'cpu' : started_window[1]['amount']}
#                                              ),
#                                              )
#                                        reservation_windows.append(
#                                          (started_window[1]['start'], event[0],
#                                           {'nodename' : node,
#                                            'cpu' : started_window[1]['amount']}
#                                          )
#                                          )
#                            started_windows_list[index][1]['state'] = 'inactive'
#
#
#
#                        if started_window[1]['type'] == 'memory' :
#                            if foundmemory >= needmemory :
#                                continue
#                            foundmemory = foundmemory + started_window[1]['amount']
#                            if foundmemory >= 0 :
#                                negative_memory = 0
#                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                print "foundmemory (%s)" % foundmemory
#                                print "needmemory (%s)" % needmemory
#                            if foundmemory >= needmemory :
#                                extramemory = foundmemory - needmemory
#                                if extramemory > 0 :
#                                    new_started_window = copy.deepcopy(started_window)
#                                    new_started_window[1]['amount'] = extramemory
#                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                        print "new_started_window (%s)" % (new_started_window,)
#                                    new_windows_list.append(new_started_window)
#                                if started_window[1]['start'] != event[0] :
#                                    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                        print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
#                                          {'nodename' : node,
#                                           'memory' : started_window[1]['amount'] - extramemory}
#                                          ),
#                                          )
#                                    reservation_windows.append(
#                                      (started_window[1]['start'], event[0],
#                                       {'nodename' : node,
#                                        'memory' : started_window[1]['amount'] - extramemory}
#                                      )
#                                      )
#                            else :
#                                # remove started_window from list,
#                                # end the previously started windows
#                                # append to reservation_windows
#                                if started_window[1]['amount'] > 0 and foundmemory > 0 :
#                                    if started_window[1]['start'] != event[0] :
#                                        if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                            print "appending reservation_window (%s)" % (                                     (started_window[1]['start'], event[0],
#                                              {'nodename' : node,
#                                               'memory' : started_window[1]['amount']}
#                                              ),
#                                              )
#                                        reservation_windows.append(
#                                          (started_window[1]['start'], event[0],
#                                           {'nodename' : node,
#                                            'memory' : started_window[1]['amount']}
#                                          )
#                                          )
#                            #del started_windows_list[index]
#                            started_windows_list[index][1]['state'] = 'inactive'
                        foundall = 1
                        for rkey in found_dict.keys():
                            if found_dict[rkey] < need_dict[rkey]:
                                if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "failed to fulfil (%s) found_dict (%s) need_dict (%s)" % (rkey, found_dict, need_dict)
                                foundall = 0
                        if foundall == 1:
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "found nodes and cpus and memory (%s)" % (found_dict,)
                            break
                        else:
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "foundall != 1 found_dict (%s) need_dict (%s)" % (found_dict,need_dict)
                           
                        #if foundnodes >= neednodes and foundcpus >= needcpus and foundmemory >= needmemory :
                        #    if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        #        print "found nodes and cpus and memory (%s) (%s) (%s)" % (foundnodes, foundcpus, foundmemory)
                        #    break

                    #if foundnodes < neednodes or foundcpus < needcpus or foundmemory < needmemory :
                    #print "looping through found_dict (%s)" % (found_dict,)
                    for rkey in found_dict.keys():
                        if found_dict[rkey] < need_dict[rkey]:
                        # nodes or cpus or memory are overcommitted.  Create
                        # a negative amount started window.
                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "overcommitted %s need_dict[%s] - found_dict[%s] (%s)" % (rkey, rkey, rkey, need_dict[rkey] - found_dict[rkey],)
                            negative_dict[rkey] = negative_dict[rkey] + need_dict[rkey] - found_dict[rkey]
                    #print "after looping through found_dict (%s)" % (found_dict,)



#                        if foundnodes < neednodes :
#                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                print "overcommitted nodes neednodes - foundnodes (%s)" % (neednodes - foundnodes,)
#                            negative_node = negative_node + neednodes - foundnodes
#                        if foundcpus < needcpus :
#                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                print "overcommitted cpus needcpus - foundcpus (%s)" % (needcpus - foundcpus,)
#                            negative_cpu = negative_cpu + needcpus - foundcpus
#                        if foundmemory < needmemory :
#                            if DEBUGJOB != None  and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                                print "overcommitted cpus needmemory - foundmemory (%s)" % (needmemory - foundmemory,)
#                            negative_memory = negative_memory + needmemory - foundmemory
                    started_windows_list = started_windows_list + new_windows_list
                #if event[0] > potential_start_time_float :
                #    # create open window here
                #reservation_windows.append( 
                #  (potential_start_time_float,
                #  reservation[0], { 'nodename' : node, 'cpu' : ) ) 
#            for reservation in node_reservations[node] :
#                if reservation[0] < end_limit :
#                    # reservation starts before end_limit
#                    if reservation[0] - potential_start_time_float >= \
#                    new_res['duration_float'] :
#                        # There is enough space ( duration )
#                        # between the potential start time and the start of this
#                        # blocking node reservation
#                        reservation_windows.append( 
#                          (potential_start_time_float,
#                          reservation[0], { 'nodename' : node, 'cpu' : ) ) 
#                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                            print "appending open window (%s, %s, %s)" % (potential_start_time_float, reservation[0], node)
#                            print "appending open window (%s, %s, %s)" % (time.asctime(time.localtime(potential_start_time_float)), time.asctime(time.localtime(reservation[0])), node)
#                else :
#                    # reservations starts after end_limit
#                    if end_limit - potential_start_time_float >= \
#                    new_res['duration_float'] :
#                        # There is enough space between the potential
#                        # start time and the end_limit for duration
#                        reservation_windows.append(
#                        (potential_start_time_float,
#                        end_limit, node) ) 
#                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
#                            print "appending open window (%s, %s, %s)" % (potential_start_time_float, end_limit, node)
#                if reservation[1] > potential_start_time_float :
#                    # The node reservation ends after the potential start time
#                    # advance the potential start time for this node
#                    # for cpu/memory scheduling the next cpu/memory open
#                    # window might overlap this reservations end time...
#                    potential_start_time_float = reservation[1]
            # handle the last window for the node
            #last_window = None
            #if end_limit >= potential_start_time_float + new_res['duration_float'] :
            #    last_window = ( potential_start_time_float, end_limit, node )
            #if last_window != None :
            #    reservation_windows.append(last_window)
            #    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            #        print "appending open window (%s, %s, %s)" % (potential_start_time_float, end_limit, node)
        reservation_windows.sort()
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "checking reservation_windows before returning"
            for new_window in reservation_windows :
                print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
                  (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])
        #print "checking reservation_windows before returning"
        for new_window in reservation_windows :
            #print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
              (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])
        result = reservation_windows
        return result
        # End of get_nonconflicting function

    conflict_policy_code = new_res['conflict_policy']
    temp_nodes_list = accepted_nodes_list[:]
    input_tuple = (temp_nodes_list, new_res, temp_reservations_list, Now_float, resources_db_handle)
    if conflict_policy_code == None :
        if DEBUGJOB !=None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "calling get_nonconflicting"
        open_windows_list = get_nonconflicting(input_tuple)
    else :
        # if a nonstandard conflict policy was provided,
        # create the get_noncoflicting function out of that
        # this allows arbitrary code to be used to determine
        # reservation windows...
        if DEBUGJOB !=None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "calling custom conflict code"
        open_windows_list = apply_policy_code(conflict_policy_code, input_tuple)
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "checking open_windows_list before returning"
        for new_window in open_windows_list :
            print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
              (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])
    #print "open_windows_list (%s)" % (open_windows_list,)
    return open_windows_list

def get_big_windows_list(open_windows_list, new_res) :
    duration = new_res['duration_float']
    big_windows_list = filter(lambda x, duration=duration : x[1] - x[0] >= duration, open_windows_list)
    #big_windows_list = []
    #for window in open_windows_list :
    #    if float(window[1] - window[0]) >= duration :
    #        big_windows_list.append(window)
    #    else :
    #        print "window[0] (%s), window[1] (%s), duration (%s)" % \
    #          (time.asctime(time.localtime(window[0])), time.asctime(time.localtime(window[1])), duration)
    return big_windows_list

def get_sized_windows_list(new_res, open_windows_list, resources_db_handle, jobs_db_handle, resource_dict_list = None, requested_resource_list = [] ) :
    def getfirstindex(x) :
        return(x[1])
    # ( <window start_time_float>, <window end_time_float>, { 'nodename' : <node name>, 'node' : <nodes>,} )
    # or
    # ( <window start_time_float>, <window end_time_float>, { 'nodename' : <node name>, 'cpu' : <cpus>,} )
    # or
    # ( <window start_time_float>, <window end_time_float>, { 'nodename' : <node name>, 'memory' : <memory>,} )
    # (1307638800.0, 1315252556.620523, {'node': 1, 'nodename': 'trestles-10-24'}), (1307638862.62795, 1315252556.620523, {'nodename': 'trestles-10-8', 'cpu': 32}), (1307638862.62795, 1315252556.620523, {'nodename': 'trestles-10-8', 'memory': 67592482816}), (1307638862.62795, 1315252556.620523, {'node': 1, 'nodename': 'trestles-10-8'}), (1307639186.62795, 1315252556.620523, {'nodename': 'trestles-12-23', 'cpu': 32}), (1307639186.62795, 1315252556.620523, {'nodename': 'trestles-12-24', 'cpu': 32})
    # From open windows, choose a set of nodes and a start_time_float
    # for the reservation.  open_windows_list should have been sorted
    # by the code in conflict_policy_code in ascending start order
    # To do: 1. if resource_amount not provided, loop to find _all_ collections
    # of windows with sufficient duration.  Choose the largest of these.
    # 2.  To work with arbitrary job geometries and multiple jobs/node,
    # convert resource to proc, create a dictionary of windows for
    # each set of windows, with the key as node name and the value a list
    # of proc windows.  Select sets of proc windows, based on the length
    # of the proc window list for each node.
    # need to accomodate license scheduling and topology
    # node exclusive job desc:
    #'node_usage': 'node_exclusive'
    #'resource_amount_int': 1
    #'req_dict_list': [{'amount_int': 1, 'this_req_string': "resource.has_key('Cpus') and (resource['Cpus'] >= 8)", 'found_ppn': 1, 'ppn_int': 8}]
    #'requested_resource_list': [{'req_list': [{'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}], 'type': 'node_exclusive'}]
    #'resource_dict_list': [{'resource_dict': {'trestles-12-23': {}, 'trestles-12-22': {},...
    # node shared job desc:
    #'node_usage': 'node_shared'
    #'resource_amount_int': 1
    #'req_dict_list': [{'amount_int': 1, 'this_req_string': "resource.has_key('Cpus') and (resource['Cpus'] >= 8) ", 'found_ppn': 1, 'ppn_int': 8}]
    #'requested_resource_list': [{'req_list': [{'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}, {'cpu': 1, 'memory': 0}], 'type': 'node_shared'}]
    #'resource_dict_list': [{'resource_dict': {'trestles-12-23': {}, 'trestles-12-22': {},...
    # what would license and topology look like?
    #'requested_resource_list': [
    #    {'req_list': [{'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0}],
    #     'type': 'node_shared'}
    #    {'req_list': [{'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0},
    #                  {'cpu': 1, 'memory': 0}],
    #     'type': 'node_shared'}
    #  ]
    # node_spec_list -> req_dict_list, requested_resource_list
    # req_dict_list leads to initiator_map, resource_dict_list
    # maybe give each node an attribute:
    # 'switchlist': ['switch2', 'switch1', 'switch3', 'switch4'....
    # when sizing windows, check for same switch requirements,
    # or close switch requirements...
    # this kind of proximity list could work for banks of memory
    # and cores also
    # 'proxlist' : ['bank1', 'module2', 'board1', 'node0', 'frame1', 'row2'...
    # if we need to schedule vSMP numabind cores, we may need to
    # schedule by contiguousness
    # for linear cpus, on cpu1:
    # 'neighborlist' : ['cpu0', 'cpu2']
    # then recurse through all neighbors to see if there is a contiguous set...
    # I think proximity list won't do contiguous...
    # Actually, proximity list should do contiguous.  Just check for same
    # bank...
    def get_longer(first_list, second_list) :
        if len(first_list[1]) >= len(second_list[1]) :
            return first_list
        else :
            return second_list
    #print "requested_resource_list (%s)" % (requested_resource_list,)
    #print "topotune: start of get_sized_windows_list"
    resources_dict = resources_db_handle[0]
    # Assume that if any resource_dict has 'amount_int' == None,
    # then we are supposed to return the largest set of sized
    # windows for that resource list.
    #resource_amount_int = new_res['resource_amount_int']
    #max_resource_int = new_res['max_resource_int']
    earliest_start_float = new_res['earliest_start_float']
    sized_windows_lists_list = []
    sized_windows_list = []
    if earliest_start_float != None :
        current_start_time_float = earliest_start_float
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "current_start_time_float from earliest_start_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
        #print "current_start_time_float from earliest_start_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
    else :
        current_start_time_float = Now_float
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "current_start_time_float from Now_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
        #print "current_start_time_float from Now_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
    # resource_dict_list for PBS supports multireq.  Each dict
    # represents one type of node, the nodes that can fit the req,
    # and how many of those needs were requested.  To support multiple
    # job/node, the allocated_dict_list needs to be mapped onto the
    # resource_dict_list.  Hmmm, maybe they should be the same from
    # the start?
    if resource_dict_list == None :
        dict = {}
        for open_window in open_windows_list :
            #dict[resources_dict[open_window[2]['nodename']]['name']] = resources_dict[open_window[2]['nodename']]
            dict[resources_dict[open_window[2]['nodename']]['name']] = {}
        resource_dict_list = [{
          'amount_int' : new_res['resource_amount_int'],
          'resource_dict' : dict
          },]
    sized_dict_list = []
    getlongest = 0
    # map requested_resource_list, if present to
    # resource_dict_list.  Both resource_dict_list and
    # requested_resource_list should be based on initiatormap.
    #if new_res.has_key('job_runID') and jobs_db_handle[0][new_res['job_runID']].has_key['requested_resource_list'] :
    #    requested_resource_list = jobs_db_handle[0][new_res['job_runID']]['requested_resource_list']
    #for dict in resource_dict_list :
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "len(resource_dict_list) (%s)" % len(resource_dict_list)
    nodeindex = 0
    #print "1. requested_resource_list (%s)" % (requested_resource_list,)
    # requested_resource_list (one element per node spec) has structure:
    # [
    #  {'type' : 'node_shared'|'node_exclusive',
    #   'req_list' : [{'cpu' : 1, 'memory' : memory_per_task }, ...]
    #  },
    #  ...
    # ]
    # resource_dict_list (one element per node spec) has structure:
    # [
    #   {'resource_dict' : {nodename : {}},
    #   ...
    # ]
    #print "1. resource_dict_list (%s)" % (resource_dict_list,)
    #print "1. requested_resource_list (%s)" % (requested_resource_list,)
    #print "len(resource_dict_list) (%s) len(requested_resource_list) (%s)" % (len(resource_dict_list), len(requested_resource_list))
    for index in range(len(resource_dict_list)) :
        # 20130507
        # need to set node = 0, usage = node_shared for LICENSENODE,
        # regardless of new_res['node_usage']
        #print "topotune: index (%s) range(len(resource_dict_list))" % (index,)
        # need to add 'req_list' to each dict...
        #print "resource_dict_list[%s] (%s)" % (index, resource_dict_list[index])
        dict = resource_dict_list[index]
        #if dict['resource_dict'].has_key(LICENSENODE):
        #    dict_usage = 'node_shared'
        #else:
        #    dict_usage = new_res['node_usage']
        #if index <= len(requested_resource_list) - 1:
        #    dict_usage = requested_resource_list[index]['type']
        #else:
        #    #FIXME what happens if we are out of range, and we don't
        #    # want to use new_res['node_usage'] for each dict?
        #    dict_usage = new_res['node_usage']
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "processing dict (%s)" % (dict,)
        if dict['amount_int'] == None :
            getlongest = 1
            sized_dict = {
              'initiatormap_index' : index,
              'amount_int' : dict['amount_int'],
              'saved_windows_list' : [],
              'requested_dict' : {},
              'resource_dict' : dict['resource_dict']
              }
            if nodeindex < len(requested_resource_list) :
                sized_dict['req_list'] = requested_resource_list[nodeindex]['req_list']
                dict_usage = requested_resource_list[nodeindex]['type']
            else :
                print "ran out of requested_resource_list, setting req_list to []"
                if len(requested_resource_list) >= 1 :
                    sized_dict['req_list'] = requested_resource_list[-1]['req_list']
                    dict_usage = requested_resource_list[-1]['type']
                else :
                    sized_dict['req_list'] = []
                    print "no requested_resource_list, defaulting to node_exclusive"
                    dict_usage = 'node_exclusive'

            # insert requested_dict here
            # for each term (representing a single node) in the
            # initiatormap, sum requested cpu and memory for all
            # initiators for that node.
            # each entry of the req_list is:
            # { 'cpu' : <cpus requested>,
            #   'memory' : <MB requested>}
            #requested_node = 0
            #requested_cpu = 0
            #requested_memory = 0
            requested_dict = {}
            #FIXME for rkey, node_exclusive need to either
            # consider a node reservation as including all
            # resources associated with the node, or explicitly
            # create requested_dict entries for all resources...
            #if new_res['node_usage'] == 'node_exclusive' :
            if dict_usage == 'node_exclusive' :
                #requested_node = 1
                requested_dict['node'] = 1
            else :
                #requested_node = 0
                requested_dict['node'] = 0
            #sized_dict['node_usage'] = new_res['node_usage']
            sized_dict['node_usage'] = dict_usage
            for indexb in range(len(sized_dict['req_list'])) :
                requested_resource = sized_dict['req_list'][indexb]
                #if requested_resource.has_key('cpu') :
                #    requested_cpu = requested_cpu + requested_resource['cpu']
                #if requested_resource.has_key('memory') :
                #    requested_memory = requested_memory + requested_resource['memory']
                for rkey in requested_resource.keys():
                    if requested_dict.has_key(rkey):
                        requested_dict[rkey] = requested_dict[rkey] + requested_resource[rkey]
                    else:
                        requested_dict[rkey] = requested_resource[rkey]
            #dict['requested_node'] = requested_node
            #dict['requested_cpu'] = requested_cpu
            #dict['requested_memory'] = requested_memory
            sized_dict['requested_dict'] = copy.deepcopy(requested_dict)
            #print "inserting requested_dict (%s)" % (requested_dict,)

            sized_dict_list.append(sized_dict)
            nodeindex = nodeindex + 1
        else :
            # FIXTHIS
            # need to correctly map requested_resource_list elements to
            # resource_dict['amount_int'] specs...
            # May need to filter, if floating licenses or storage are
            # part of this...
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "dict['amount_int'] (%s)" % dict['amount_int']
            for sizedindex in range(dict['amount_int']) :
                #print "topotune: sizedindex (%s) range(dict['amount_int'])" % (sizedindex,)
                sized_dict = {
                  'initiatormap_index' : index,
                  'amount_int' : 1,
                  'saved_windows_list' : [],
                  'requested_dict' : {},
                  'resource_dict' : dict['resource_dict']
                  }
                if nodeindex < len(requested_resource_list) :
                    #print "requested_resource_list[nodeindex]['req_list'] (%s)" % (requested_resource_list[nodeindex]['req_list'],)
                    sized_dict['req_list'] = requested_resource_list[nodeindex]['req_list']
                    dict_usage = requested_resource_list[nodeindex]['type']
                else :
                    #print "ran out of requested_resource_list, setting req_list to %s" % requested_resource_list[-1]['req_list']
                    if len(requested_resource_list) >= 1 :
                        sized_dict['req_list'] = requested_resource_list[-1]['req_list']
                        dict_usage = requested_resource_list[-1]['type']
                    else :
                        sized_dict['req_list'] = []
                        print "no requested_resource_list, defaulting to node_exclusive"
                        dict_usage = 'node_exclusive'
                # insert requested_dict here
                # for each term (representing a single node) in the
                # initiatormap, sum requested cpu and memory for all
                # initiators for that node.
                # each entry of the req_list is:
                # { 'cpu' : <cpus requested>,
                #   'memory' : <MB requested>}
                #requested_node = 0
                #requested_cpu = 0
                #requested_memory = 0
                requested_dict = {}
                #FIXME for rkey, node_exclusive need to either
                # consider a node reservation as including all
                # resources associated with the node, or explicitly
                # create requested_dict entries for all resources...
                #if new_res['node_usage'] == 'node_exclusive' :
                if dict_usage == 'node_exclusive' :
                    #requested_node = 1
                    requested_dict['node'] = 1
                else :
                    #requested_node = 0
                    requested_dict['node'] = 0
                #print "dict_usage (%s) requested_dict (%s)" % (dict_usage, requested_dict)
                #sized_dict['node_usage'] = new_res['node_usage']
                sized_dict['node_usage'] = dict_usage
                for indexb in range(len(sized_dict['req_list'])) :
                    #print "topotune: indexb (%s) range(len(sized_dict['req_list']))" % (indexb,)
                    requested_resource = sized_dict['req_list'][indexb]
                    #if requested_resource.has_key('cpu') :
                    #    requested_cpu = requested_cpu + requested_resource['cpu']
                    #if requested_resource.has_key('memory') :
                    #    requested_memory = requested_memory + requested_resource['memory']
                    for rkey in requested_resource.keys():
                        #print "topotune: rkey (%s) requested_resource.keys() (%s)" % (rkey,requested_resource.keys())
                        #print "topotune: rkey %s requested_dict (%s)" % (rkey,requested_dict)
                        if requested_dict.has_key(rkey):
                            requested_dict[rkey] = requested_dict[rkey] + requested_resource[rkey]
                        else:
                            requested_dict[rkey] = requested_resource[rkey]
                        #print "topotune: rkey %s requested_dict (%s)" % (rkey,requested_dict)
                #dict['requested_node'] = requested_node
                #dict['requested_cpu'] = requested_cpu
                #dict['requested_memory'] = requested_memory

                sized_dict['requested_dict'] = copy.deepcopy(requested_dict)
                sized_dict_list.append(copy.deepcopy(sized_dict))

                #sized_dict['requested_dict'] = requested_dict
                #sized_dict_list.append(sized_dict)


                #print "sized_dict_list (%s)" % (sized_dict_list,)

                nodeindex = nodeindex + 1
                #print "1. sized_dict (%s) has no requested_dict" % ( sized_dict,)
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "len(sized_dict_list) (%s)" % len(sized_dict_list)
        print "sized_dict_list (%s)" % (sized_dict_list,)
    candidate_sized_list = []
    counting_dict_list = []
    #ns_sized_windows_list = []
    need_more = 1
    want_more = 0
    found_one_set = 0
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "open_windows_list (%s)" % (open_windows_list,)
        print "len(open_windows_list) (%s)" % (len(open_windows_list),)
    node_bin_dict = {}
    last_window_index = len(open_windows_list) - 1
    for new_window_index in range(len(open_windows_list)) :
        #print "topotune: new_window_index (%s) range(len(open_windows_list))" % (new_window_index,)
        #print "current_start_time_float (%s)" % \
        #  ( time.asctime(time.localtime(current_start_time_float)), )
        #print "checking window (%s)" % (open_windows_list[new_window_index],)
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "new_window_index current_start_time_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
        new_window = open_windows_list[new_window_index]
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            #print "new_window (%s)" % (new_window,)
            print "new_window (%s) (%s) (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y",time.localtime(new_window[0])),time.strftime("%H:%M:%S_%m/%d/%Y",time.localtime(new_window[1])), new_window[2])
        #print "new_window (%s) (%s) (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y",time.localtime(new_window[0])),time.strftime("%H:%M:%S_%m/%d/%Y",time.localtime(new_window[1])), new_window[2])
            #print "considering this window (%s) - (%s) < (%s)" % \
            #  (time.asctime(time.localtime(new_window[1])), time.asctime(time.localtime(current_start_time_float)), new_res['duration_float'])
        # Break if we have reached our target resource_amount_int and the next window
        # starts later than the current crop
        # - update the start and end times, if necessary
        # - remove any windows that end before the new end time
        # Break if latest_end_float exists, and there is not enough time
        # ...
        # For topology-awareness, need to only break if a set of
        # topology-compatible windows has been found.
        # resource requirements should include a function for
        # checking topology-compatibility.  input should be
        # candidate sized windows, output should be a dictionary
        # with one entry for each set of topology-compatible
        # resources.  How do licenses fit this?  Do they get
        # included as a separate entry or in each topo entry?
        if new_res['latest_end_float'] != None and \
          ( current_start_time_float + \
          new_res['duration_float'] > new_res['latest_end_float'] or \
          new_window[0] + new_res['duration_float'] > new_res['latest_end_float'] ) :
            if DEBUGJOB !=None and new_res['job_runID'] == DEBUGJOB :
                print "breaking, not enough time left before end of window"
                print "current_start (%s) new_res['duration_float'] (%s) new_res['latest_end_float'] (%s) new_window[0]+new_res['duration_float'] (%s)" % \
                  (time.asctime(time.localtime(current_start_time_float)), new_res['duration_float'], time.asctime(time.localtime(new_res['latest_end_float'])), time.asctime(time.localtime(new_window[0] + new_res['duration_float'])))
            break
        # Skip if there is not enough time in this window
        # need to get a node count check in...
        if new_window[1] - current_start_time_float < \
          new_res['duration_float'] :
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "skipping this window due to lack of time (%s) - (%s) < (%s)" % \
                  (time.asctime(time.localtime(new_window[1])), time.asctime(time.localtime(current_start_time_float)), new_res['duration_float'])
            #print "skipping this window due to lack of time (%s) - (%s) < (%s)" % \
            #  (time.asctime(time.localtime(new_window[1])), time.asctime(time.localtime(current_start_time_float)), new_res['duration_float'])
            #if new_window[0] > current_start_time_float :
            #    current_start_time_float = new_window[0]
            #continue
            #FIXME? this looks like a bug, to not continue here.
            # what happens in the rest of the loop with new_window
            # not appended?  I guess this hits the index increment
            # at the end...
        else :
            # Add the new window to the list of sized_windows_list
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "sized_windows_list.append(%s)" % (new_window,)
            #print "sized_windows_list.append(%s)" % (new_window,)
            sized_windows_list.append(new_window)
 
        if new_window_index == last_window_index or open_windows_list[new_window_index + 1][0] > current_start_time_float :

            #print "get_sized: doing a check with current_start_time_float (%s) for new_window (%s) and len(sized_windows_list) (%s)" % (time.asctime(time.localtime(current_start_time_float)), new_window, len(sized_windows_list))
            #  MOVE THIS check only if the window under consideration is later
            #  than the current_start
            # - check to see if the sized_dict is fully populated
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "current_start_time_float (%s) len(sized_windows_list) (%s) new_window[2] (%s)" % \
                  ( time.asctime(time.localtime(current_start_time_float)), len(sized_windows_list), new_window[2] )
                print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
                  (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])
            #print "current_start_time_float (%s) len(sized_windows_list) (%s) new_window[2] (%s)" % \
              #( time.asctime(time.localtime(current_start_time_float)), len(sized_windows_list), new_window[2] )
            #print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
              #(time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])

            # If the new window start time is greater than the current_start_time_float
            # reset the current_start_time_float.  Delete any of the sized_windows_list
            # that have an end_time_float too early to accomodate the reservation
            ## If resource_amount_int == None, save sized_windows_list with
            ## current start time, so that the largest of the sets can be returned
            #if getlongest == 1 :
            #    candidate_windows_list = [current_start_time_float, sized_windows_list, sized_dict_list]
            #    if len(sized_windows_list) > 0 :
            #        sized_windows_lists_list.append(candidate_windows_list)
            # Should current_start_time_float be set to the next window?
            # No, leave like this.  We are evaluating the current windows,
            # and current_start_time_float should reflect the current set,
            # rather than the next set.
            # Probably don't need this check, since the new window should
            # never be later than current_start_time_float
            if new_window[0] > current_start_time_float :
                current_start_time_float = new_window[0]
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "current_start_time reset to (%s)" % time.asctime(time.localtime(current_start_time_float))
                #print "current_start_time reset to (%s)" % time.asctime(time.localtime(current_start_time_float))
            #if new_window_index != last_window_index and open_windows_list[new_window_index + 1][0] > current_start_time_float :
            #    current_start_time_float = open_windows_list[new_window_index + 1][0]
            # Do I need to filter out short windows at this point?
            # That should have happened above...
            saved_windows_list = []
            for i in range(len(sized_windows_list)) :
                #print "topotune: i (%s) range(len(sized_windows_list))" % (i,)
                if sized_windows_list[i][1] - current_start_time_float >= \
                new_res['duration_float'] :
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "appending to saved_windows_list (%s)" % (sized_windows_list[i],)
                    saved_windows_list.append(sized_windows_list[i])
            sized_windows_list = saved_windows_list

    
            ## For multiple-requirement reservation, need to see if each
            ## resource_dict['amount'] has been reached with the current
            ## set of open windows...
            ##counting_dict_list = copy.deepcopy(sized_dict_list)
            #counting_dict_list = sized_dict_list
            #for sized_dict in counting_dict_list :
            #    sized_dict['saved_windows_list'] = []
    
            # aggregate all the cpu and memory windows into bins by node
            # I lose start and end info for the individual open_window
            # here...I need to keep a window_list for use by get_sorted
            # later on....
            #oldway#node_bin_dict = {}
            #oldway#for sized_window in sized_windows_list :
            #oldway#    if node_bin_dict.has_key(sized_window[2]['nodename']) :
            #oldway#        node_bin_dict[sized_window[2]['nodename']]['window_list'].append(sized_window)
            #oldway#        if sized_window[2].has_key('node') :
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['node'] = node_bin_dict[sized_window[2]['nodename']]['node'] + sized_window[2]['node']
            #oldway#        if sized_window[2].has_key('cpu') :
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['cpu'] = node_bin_dict[sized_window[2]['nodename']]['cpu'] + sized_window[2]['cpu']
            #oldway#        if sized_window[2].has_key('memory') and sized_window[2]['memory'] != None :
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['memory'] = node_bin_dict[sized_window[2]['nodename']]['memory'] + sized_window[2]['memory']
            #oldway#    else :
            #oldway#        node_bin_dict[sized_window[2]['nodename']] = {'window_list' : [sized_window,]}
            #oldway#        if sized_window[2].has_key('cpu') and sized_window[2]['cpu'] != None :
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['node'] = 0
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['cpu'] = sized_window[2]['cpu']
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['memory'] = 0
            #oldway#        elif sized_window[2].has_key('memory') :
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['node'] = 0
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['memory'] = sized_window[2]['memory']
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['cpu'] = 0
            #oldway#        elif sized_window[2].has_key('node') :
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['node'] = sized_window[2]['node']
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['cpu'] = 0
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['memory'] = 0
            #oldway#        else :
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['node'] = 0
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['cpu'] = 0
            #oldway#            node_bin_dict[sized_window[2]['nodename']]['memory'] = 0
            #oldway#for nodename in node_bin_dict.keys() :
            #oldway#    node_bin_dict[nodename]['viable_request_count'] = 0

            node_bin_dict = {}
            switch_node_list = []
            available_licenses_dict = {}
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "populating node_bin_dict with sized_windows_list (%s)" % (sized_windows_list,)
            for sized_window in sized_windows_list :
                #print "topotune: sized_window (%s) sized_windows_list" % (sized_window,)
                if not sized_window[2]['nodename'] in switch_node_list:
                    switch_node_list.append(sized_window[2]['nodename'])
                if sized_window[2]['nodename'] == LICENSENODE:
                    for licensename in sized_window[2].keys():
                        if licensename in ('nodename','node'):
                            continue
                        if available_licenses_dict.has_key(licensename):
                            available_licenses_dict[licensename] = available_licenses_dict[licensename] + sized_window[2][licensename]
                        else:
                            available_licenses_dict[licensename] = sized_window[2][licensename]
                #print "available_licenses_dict (%s)" % available_licenses_dict
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "processing sized_window (%s)" % (sized_window,)
                if node_bin_dict.has_key(sized_window[2]['nodename']) :
                    node_bin_dict[sized_window[2]['nodename']]['window_list'].append(sized_window)
                else :
                    node_bin_dict[sized_window[2]['nodename']] = {'window_list' : [sized_window,]}

                for rkey in sized_window[2].keys():
                    if rkey == 'nodename':
                        continue
                    #if sized_window[2][rkey] == None:
                    #    continue
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "processing rkey (%s)" % (rkey,)
                    if node_bin_dict[sized_window[2]['nodename']].has_key(rkey) :
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "adding node_bin_dict[sized_window[2]['nodename']][%s] (%s) to sized_window[2][%s] (%s)" % (rkey, node_bin_dict[sized_window[2]['nodename']][rkey], rkey, sized_window[2][rkey])
                        node_bin_dict[sized_window[2]['nodename']][rkey] = node_bin_dict[sized_window[2]['nodename']][rkey] + sized_window[2][rkey]
                    else:
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "setting node_bin_dict[sized_window[2]['nodename']][%s] to sized_window[2][%s] (%s)" % (rkey, rkey, sized_window[2][rkey])
                        node_bin_dict[sized_window[2]['nodename']][rkey] = sized_window[2][rkey]
                if not node_bin_dict[sized_window[2]['nodename']].has_key('node'):
                    node_bin_dict[sized_window[2]['nodename']]['node'] = 0

#                allused = 0
#                for rkey in sized_window[2].keys():
#                    if rkey == 'node' and sized_window[2][rkey] != None and sized_window[2][rkey] > 0:
#                        allused = 1
#                if allused == 1:
#                    # node sized_window means all resources on the node are
#                    # available, so create node_bin_dict entries for them
#                    # actually, looks like node sized_window does _not_
#                    # mean all resources on the node are free
#
#                    #for rkey in resources_dict[nodename]['consumable_list']:
#                    #print "resources_dict[sized_window[2]['nodename']]['consumable_list'] (%s)" % (resources_dict[sized_window[2]['nodename']]['consumable_list'],)
#                    #print "resources_dict[sized_window[2]['nodename']] (%s)" % (resources_dict[sized_window[2]['nodename']],)
#                    node_bin_dict[sized_window[2]['nodename']]['node'] = 1
#                    for rkey in resources_dict[sized_window[2]['nodename']]['consumable_dict'].keys():
#                        node_bin_dict[sized_window[2]['nodename']][rkey] = resources_dict[sized_window[2]['nodename']]['consumable_dict'][rkey]
#                else:
#                    if not node_bin_dict[sized_window[2]['nodename']].has_key('node'):
#                        node_bin_dict[sized_window[2]['nodename']]['node'] = 0
#                    for rkey in sized_window[2].keys():
#                        if rkey == 'nodename':
#                            continue
#                        #if sized_window[2][rkey] == None:
#                        #    continue
#                        if node_bin_dict[sized_window[2]['nodename']].has_key(rkey) :
#                            node_bin_dict[sized_window[2]['nodename']][rkey] = node_bin_dict[sized_window[2]['nodename']][rkey] + sized_window[2][rkey]
#                        else:
#                            node_bin_dict[sized_window[2]['nodename']][rkey] = sized_window[2][rkey]
    

            # check to see if we have enough nodes for each sized_dict
            #insufficient_nodes = 0
            #for sized_dict in sized_dict_list:
            #    amount_needed = sized_dict['amount_int']
            #    print "amount_needed (%s)" % amount_needed
            #    suitable_nodes = sized_dict['resource_dict'].keys()
            #    amount_now = 0
            #    for amount_index in range(amount_needed):
            #        for suitable_node in suitable_nodes:
            #            if suitable_node == LICENSENODE:
            #                print "looking for (%s)" % LICENSENODE
            #            if suitable_node in switch_node_list:
            #                amount_now = amount_now + 1
            #                if suitable_node == LICENSENODE:
            #                    print "found (%s)" % LICENSENODE
            #    if amount_now < amount_needed:
            #        insufficient_nodes = 1
            #        break
            #if insufficient_nodes == 1:
            #    print "should bail out here"
            #else:
            #    print "insufficient_nodes (%s)" % insufficient_nodes
            #if not new_res.has_key('job_step'):
            #    print "no job_step"
            #if new_res.has_key('job_step') and new_res['job_step'].has_key('license_request_list'):
            #    print "license requested"
            #else:
            #    print "no license requested"
            #if LICENSENODE in switch_node_list:
            #    print "LICENSENODE (%s) in switch_node_list (%s)" % (LICENSENODE, switch_node_list)
            #else:
            #    print "LICENSENODE (%s) not in switch_node_list (%s)!" % (LICENSENODE, switch_node_list)
            insufficient_licenses = False
            if new_res.has_key('job_step') and new_res['job_step'] !=None and new_res['job_step'].has_key('license_request_list'):
                # before looking for node_sets, check to see if we have
                # enough licenses at this time.
                needed_licenses_dict = {}
                license_request_split = string.split(new_res['job_step']['license_request_list'], '+')
                for license_request in license_request_split:
                    #print "license_request (%s)" % (license_request,)
                    license_name, license_value = string.split(license_request,'_')
                    needed_licenses_dict[license_name] = int(license_value)
                #print "needed_licenses_dict (%s)" % needed_licenses_dict
                for needed_license in needed_licenses_dict.keys():
                    if not available_licenses_dict.has_key(needed_license) or needed_licenses_dict[needed_license] > available_licenses_dict[needed_license]:
                        #print "insufficient license (%s) (%s) avail (%s)" % (needed_license, needed_licenses_dict[needed_license], available_licenses_dict[needed_license])
                        insufficient_licenses = True
                        break
            else:
                insufficient_licenses = False
            if insufficient_licenses == True:
                #print "should bail out here"
                continue
            #sorted_node_sets_list = get_node_sets(new_res, node_bin_dict, resources_db_handle, jobs_db_handle)
            #print "getting node_sets"
            sorted_node_sets_list = get_node_sets(new_res, switch_node_list, resources_db_handle, jobs_db_handle)
           
            #print "topotune: len(sorted_node_sets_list) (%s)" % (len(sorted_node_sets_list),)
            # instead of a for loop, should maintain a dictionary of
            # node_set_counting_dict[node_set] = counting_dict
            # That way, won't have to loop over all nodes sets for
            # every sized_window
            found_one_set = 0
            last_good_counting_dict_list = None
            last_good_node_bin_dict = None
            for node_set in sorted_node_sets_list:
                #print "topotune: node_set (%s) sorted_node_sets_list" % (node_set,)
                #print "len(node_set) (%s)" % len(node_set)

                ns_sized_windows_list = []
                #print "node_set (%s)" % (node_set,)
                # For multiple-requirement reservation, need to see if each
                # resource_dict['amount'] has been reached with the current
                # set of open windows...
                #counting_dict_list = copy.deepcopy(sized_dict_list)
                counting_dict_list = copy.deepcopy(sized_dict_list)
                for sized_dict in counting_dict_list :
                    #print "topotune: sized_dict (%s) counting_dict_list" % (sized_dict,)
                    sized_dict['saved_windows_list'] = []
                    if not sized_dict.has_key('requested_dict'):
                        print "sized_dict (%s) has no requested_dict" % ( sized_dict,)

                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "node_bin_dict (%s)" % (node_bin_dict,)
                for nodename in node_bin_dict.keys() :
                    #print "topotune: nodename (%s) node_bin_dict.keys()" % (nodename,)
                    node_bin_dict[nodename]['viable_request_count'] = 0
                #print "node_bin_dict (%s)" % (node_bin_dict,)
                saved_sorting_list = []
                request_sorting_list = []
                # counting_dict has structure of requested_resource:
                # {'type' : <node_shared|node_exclusive>,
                #  'req_list' : [ {'cpu' : <cpus>, 'memory' : <memory>}...]}
                # req_list has one cpu-memory dictionary for each initiator
                # in the initiator map
                # indexa is which node in initiator map
                # indexb is which initiator in that node
                # For each initiator request, how many windows can satisfy
                # that request?
                for indexa in range(len(counting_dict_list)) :
                    #print "topotune: indexa (%s) range(len(counting_dict_list))" % (indexa,)
                    #print "len(counting_dict_list) (%s) indexa (%s) dict (%s)" % (len(counting_dict_list), indexa, dict)
                    dict = counting_dict_list[indexa]
                    requested_dict = dict['requested_dict']
                    #print "indexa requested_dict (%s)" % (requested_dict,)

#                    # for each term (representing a single node) in the
#                    # initiatormap, sum requested cpu and memory for all
#                    # initiators for that node.
#                    # each entry of the req_list is:
#                    # { 'cpu' : <cpus requested>,
#                    #   'memory' : <MB requested>}
#                    #requested_node = 0
#                    #requested_cpu = 0
#                    #requested_memory = 0
#                    requested_dict = {}
#                    #FIXME for rkey, node_exclusive need to either
#                    # consider a node reservation as including all
#                    # resources associated with the node, or explicitly
#                    # create requested_dict entries for all resources...
#                    if new_res['node_usage'] == 'node_exclusive' :
#                        #requested_node = 1
#                        requested_dict['node'] = 1
#                    else :
#                        #requested_node = 0
#                        requested_dict['node'] = 0
#                    for indexb in range(len(dict['req_list'])) :
#                        requested_resource = dict['req_list'][indexb]
#                        #if requested_resource.has_key('cpu') :
#                        #    requested_cpu = requested_cpu + requested_resource['cpu']
#                        #if requested_resource.has_key('memory') :
#                        #    requested_memory = requested_memory + requested_resource['memory']
#                        for rkey in requested_resource.keys():
#                            if requested_dict.has_key(rkey):
#                                requested_dict[rkey] = requested_dict[rkey] + requested_resource[rkey]
#                            else:
#                                requested_dict[rkey] = requested_resource[rkey]
#                    dict['node_usage'] = new_res['node_usage']
#                    #dict['requested_node'] = requested_node
#                    #dict['requested_cpu'] = requested_cpu
#                    #dict['requested_memory'] = requested_memory
#                    dict['requested_dict'] = copy.deepcopy(requested_dict)
#                    print "inserting requested_dict (%s)" % (requested_dict,)

                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        #print "requested_node (%s) requested_cpu (%s) requested_memory (%s)" % (requested_node, requested_cpu, requested_memory)
                        print "requested_dict (%s)" % (requested_dict,)
                        print "dict (%s)" % dict
                    # now, requested_cpu and requested_memory is the total for this
                    # initiator term.  how many entries in node_bin_dict can
                    # meet this request?
                    viable_request_count = 0
                    for nodename in node_bin_dict.keys() :
                        #print "topotune: nodename (%s) node_bin_dict.keys()" % (nodename,)
    
                        # for TOPOLOGY_MODULE, only consider nodes in current
                        # node_set
                        if not nodename in node_set:
                            #print "nodename (%s) not in node_set, skipping" % (nodename,)
                            continue
    
                        sufficient_resource = 1
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "requested_dict (%s), node_bin_dict[nodename] (%s)" % (requested_dict, node_bin_dict[nodename])
                        for rkey in requested_dict.keys():
                            #print "topotune: rkey (%s) requested_dict.keys()" % (rkey,)
                            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "checking rkey (%s) in requested_dict (%s)" % (rkey, requested_dict)
                            # should this check be skipped for rkey == 'node'?
                            #if not node_bin_dict[nodename].has_key(rkey) or requested_dict[rkey] > node_bin_dict[nodename][rkey]:
                            if not rkey == 'node' and (not node_bin_dict[nodename].has_key(rkey) or requested_dict[rkey] > node_bin_dict[nodename][rkey]):
                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    if not node_bin_dict[nodename].has_key(rkey):
                                        print "not node_bin_dict[%s].has_key(%s) (%s)" % (nodename, rkey, node_bin_dict[nodename])
                                    elif requested_dict[rkey] > node_bin_dict[nodename][rkey]:
                                        print "requested_dict[%s] (%s) < node_bin_dict[%s][%s] (%s)" % (rkey, requested_dict[rkey], nodename, rkey, node_bin_dict[nodename][rkey])
                                sufficient_resource = 0
                            if rkey == 'node' and requested_dict[rkey] > 0:
                                #assume a node request of > 0 means all
                                #consumables on the node must be free
                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "rkey == node, so check all consumables (%s) node_bin_dict (%s) consumable_dict (%s)" % (rkey, node_bin_dict, resources_dict[nodename]['consumable_dict'],)
                                for conskey in resources_dict[nodename]['consumable_dict'].keys():
                                    #print "topotune: conskey (%s) resources_dict[nodename]['consumable_dict'].keys()" % (conskey,)
                                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "conskey (%s)" % (conskey,)
                                    if conskey == 'node':
                                        continue
                                    if not node_bin_dict[nodename].has_key(conskey) or resources_dict[nodename]['consumable_dict'][conskey] > node_bin_dict[nodename][conskey]:
                                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "failed sufficient resources check"
                                            if node_bin_dict[nodename].has_key(conskey) and DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        
                                                #print "consumable check failed for (%s) node_bin_dict[%s][%s] (%s) resources_dict[nodename]['consumable_dict'][%s] (%s)" % (conskey, nodename, conskey, node_bin_dict[nodename][conskey], conskey, resources_dict[nodename]['consumable_dict'][conskey])
                                                print "node_bin_dict[%s] (%s)" % (nodename, node_bin_dict[nodename])
                                                print "node_bin_dict[%s][%s] (%s)" % (nodename, conskey, node_bin_dict[nodename][conskey])
                                                print "resources_dict[%s]['consumable_dict'][%s] (%s)" % (nodename, conskey, resources_dict[nodename]['consumable_dict'][conskey])
                                            else:
                                                print "node_bin_dict[%s] (%s)" % (nodename, node_bin_dict[nodename])
                                        sufficient_resource = 0
                            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "checked rkey (%s) in requested_dict (%s) sufficient_resource (%s)" % (rkey, requested_dict, sufficient_resource)
                                    
    
    #                    if requested_node <= node_bin_dict[nodename]['node'] :
    #                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
    #                            print "sufficient node is 1 for (%s) requested_node (%s) node_bin_dict[nodename]['node'] (%s)" % (nodename, requested_node, node_bin_dict[nodename]['node'])
    #                        sufficient_node = 1
    #                    else :
    #                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
    #                            print "sufficient node is 0 for (%s)" % nodename
    #                        sufficient_node = 0
    #                    if requested_cpu <= node_bin_dict[nodename]['cpu'] :
    #                        sufficient_cpu = 1
    #                    else :
    #                        sufficient_cpu = 0
    #                    if requested_memory <= node_bin_dict[nodename]['memory'] :
    #                        sufficient_memory = 1
    #                    else :
    #                        sufficient_memory = 0
    #                    if sufficient_node == 1 and sufficient_cpu == 1 and sufficient_memory == 1 :
    #                        viable_request_count = viable_request_count + 1
    #                        # how many of the requested_resources could fit in the node open
    #                        # window aggregate?  Need to fit all req_list into the aggregate
    #                        # node cpu and memory
    #                        # If this dict can fit, then increment the viable_request_count
    #                        # for each nodename.  node_bin_dict needs a 'viable_request_count'
    #                        # key
    #                        node_bin_dict[nodename]['viable_request_count'] = \
    #                          node_bin_dict[nodename]['viable_request_count'] + 1
    
                        if sufficient_resource == 1:
                            node_bin_dict[nodename]['viable_request_count'] = \
                              node_bin_dict[nodename]['viable_request_count'] + 1
                    request_sorting_list.append( (viable_request_count, dict ) )
        
                #print "after indexa loop: counting_dict_list (%s)" % (counting_dict_list,)
                #print "after indexa loop: request_sorting_list (%s)" % (request_sorting_list,)
                # at this point, we have taken counting_dict_list,
                # and generated request_sorting_list, that maps 1:1
                # with counting_dict_list, but with a total cpu and
                # memory request for each node, and headed by a
                # viable_request_count, to fill difficult-to-satisfy nodes
                # first
    
                # For TOPOLOGY_MODULE, if a node is in more node_sets,
                # then is should see a higher viable_request_count,
                # then should be chosen later, right?  Is that what we
                # want?  Nodes should be chosen from smallest node_set
                # first...
    
                # need to allow for the situation where two initiatormap entries
                # could fit on a single node....
                for nodename in node_bin_dict.keys() :
                    #print "topotune: nodename (%s) node_bin_dict.keys()" % (nodename,)
                    # TOPOLOGY_MODULE skip
                    if not nodename in node_set:
                        continue
                    saved_sorting_list.append( (node_bin_dict[nodename]['viable_request_count'], nodename) )
                #if len(saved_sorting_list) > 0 :
                #    if getlongest == 1 :
                #        candidate_windows_list = [current_start_time_float, sized_windows_list, sized_dict_list]
                #        if len(sized_windows_list) > 0 and (new_res['node_usage'] =='node_shared' or (sufficient_node == 1 and sufficient_cpu == 1 and sufficient_memory == 1)) :
                #            # This appends a set of windows that does not fulfill
                #            # the sharedmap requirement.  This should happen
                #            # After the check of counting_dict_list
                #            print "appending candidate_windows_list with current_start_time (%s)" % time.asctime(time.localtime(current_start_time_float))
                #            print "sized_windows_list (%s)" % (sized_windows_list,)
                #            sized_windows_lists_list.append(candidate_windows_list)
                request_sorting_list.sort()
                request_sorting_list.reverse()
                #counting_dict_list = map(lambda x : x[1], request_sorting_list)
                counting_dict_list = map(getfirstindex, request_sorting_list)
                #print "after sorting: counting_dict_list (%s)" % (counting_dict_list,)
                saved_sorting_list.sort()
                #saved_sorting_list.reverse()
                # this will break, next time through loop, sized_windows_list
                # elements will be wrong...
                #sized_windows_list = map(lambda x : x[1], saved_sorting_list)
                # This check should be whether there are sufficient
                # proc and memory windows to satisfy all
                # dict['requested_resource'] entries
        
                # populate saved_windows_list for each dict in counting_dict_list
                # It's a little complicated, because it originally allowed
                # more than one dict on a node.  Iterating through all
                # windows for all dicts allows each dict to find a place
                # on a previously used node, if resources allow.
                # The assigned_nodes_list now prevents that...
    
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "populating counting_dict_list"
                    #print "len(counting_dict_list) (%s)" % len(counting_dict_list)
                assigned_nodes_list = []
                required_dict_count = len(counting_dict_list)
                found_dict_count = 0
                for window in saved_sorting_list :
                    #found_dict_count = 0
                    #print "topotune: window (%s) saved_sorting_list" % (window,)
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "window (%s)" % (window,)
                    # only windows with nodename in node_set should
                    # be here for TOPOLOGY_MODULE
                    # at this point, saved_sorting_list consists of tuples:
                    # (viable_request_count, nodename), so window[1] is
                    # a nodename
                    # This assumes all nodes have the same req_keys...
                    assigned_dict = {}
                    assigned_dict['node'] = 0
    
                    #for dict in counting_dict_list:
                    #    #print "topotune: dict (%s) counting_dict_list" % (window,)
                    #    print "topotune: dict (%s) counting_dict_list" % (dict,)
                    #    if dict.has_key('req_list'):
                    #        for req in dict['req_list']:
                    #            print "topotune: req (%s) dict['req_list']" % (req,)
                    #            for req_key in req.keys():
                    #                print "topotune: req_key (%s) req.keys()" % (req_key,)
                    #                if not assigned_dict.has_key(req_key):
                    #                    assigned_dict[req_key] = 0

                    for conskey in resources_dict[window[1]]['consumable_dict'].keys():
                        if not assigned_dict.has_key(conskey):
                            assigned_dict[conskey] = 0
    
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "assigned_dict (%s)" % (assigned_dict,)
                    #assigned_node = 0
                    #assigned_cpu = 0
                    #assigned_memory = 0
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "saved_sorting_list window (%s)" % (window,)
                    if DEBUGJOB != None and new_res['job_runID'] == DEBUGJOB :
                        print "start of window for loop for (%s)" % window[1]
                        #print "filling counting_dict_list (%s)" % (counting_dict_list,)
                    for dict in counting_dict_list :
                        #print "topotune: dict (%s) counting_dict_list" % (dict,)
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "filling dict (%s) in counting_dict_list" % (dict,)
                        requested_dict = dict['requested_dict']
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "requested_dict (%s)" % (requested_dict,)
                        #if dict['amount_int'] == None :
                        #    # need to if dict['node_usage'] == 'node_exclusive'
                        #    # need to check to make sure all resources are available
                        #    if dict['resource_dict'].has_key(window[1]) :
                        #        assigned_nodes_list.append(window[1])
                        #        dict['saved_windows_list'].append(window)
                        #        found_dict_count = found_dict_count + 1
                        #        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        #            print "appending window (%s)" % (window,)
                        #            print "breaking..."
                        #        # should I really break here?  won't that
                        #        # skip the subsequent dicts?
                        #        #break
                        #        continue
                        #    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        #        print "continuing..."
                        #    continue
                        if dict['amount_int'] != None and len(dict['saved_windows_list']) >= dict['amount_int'] :
                            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "1. len(dict['saved_windows_list']) >= dict['amount_int'], continuing"
                                print "1. len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                                print "1. dict['amount_int'] (%s)" % dict['amount_int']
                            continue
                        else :
                            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "2. len(dict['saved_windows_list']) < dict['amount_int']"
                                print "2. len(dict['saved_windows_list']) (%s)" % len(dict['saved_windows_list'])
                                print "2. dict['amount_int'] (%s)" % dict['amount_int']
                            if dict['resource_dict'].has_key(window[1]) :
                                # if node_bin_dict[nodename]['cpu'] and memory more
                                # than total requested cpu and memory for the req dict,
                                # add the node
    
                                # The assigned_nodes_list check is to prevent
                                # scheduling on the same node twice for shared
                                # jobs.  I think this should be allowed, but
                                # LoadLeveler, at least, complains...
                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "assigned_dict (%s) requested_dict (%s) node_bin_dict (%s)" % (assigned_dict, requested_dict, node_bin_dict)
                                    print "checking assigned_keys (%s)" % (assigned_dict.keys(),)
                                    print "found_dict_count (%s)" % (found_dict_count,)
                                allopen = 1
                                #for assigned_key in assigned_dict.keys():
                                for assigned_key in requested_dict.keys():
                                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "assigned_key start (%s)" % (assigned_key,)
                                    if assigned_key != 'node' and not assigned_key in resources_dict[window[1]]['consumable_dict'].keys():
                                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "assigned_key(%s) not in resources_dict[window[1]]['consumable_dict']keys() (%s)" % (assigned_key, resources_dict[window[1]]['consumable_dict'].keys())
                                        #continue
                                        allopen = 0
                                        break
                                    if assigned_key != 'node' and not assigned_key in requested_dict.keys():
                                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "assigned_key(%s) not in requested_dict.keys() (%s)" % (assigned_key, requested_dict.keys())
                                        continue
                                    #print "topotune: assigned_key (%s) assigned_dict.keys()" % (assigned_key,)
                                    if assigned_key != 'node':
                                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "checking assigned_key (%s)" % (assigned_key,)
                                            #print "checking requested_dict[%s] %s > node_bin_dict[%s][%s] %s - assigned_dict[%s] %s" % (assigned_key, requested_dict[assigned_key], window[1], assigned_key, node_bin_dict[window[1]][assigned_key], assigned_key, assigned_dict[assigned_key])
                                            #print "checking %s requested_dict (%s)  > node_bin_dict (%s) - assigned_dict (%s)" % (assigned_key, requested_dict, node_bin_dict, assigned_dict)
                                        #if not node_bin_dict[window[1]].has_key(assigned_key) or (requested_dict.has_key(assigned_key) and float(requested_dict[assigned_key]) > float(node_bin_dict[window[1]][assigned_key]) - float(assigned_dict[assigned_key])):
                                        if not node_bin_dict[window[1]].has_key(assigned_key) or (requested_dict.has_key(assigned_key) and float(requested_dict[assigned_key]) > float(node_bin_dict[window[1]][assigned_key]) - float(assigned_dict[assigned_key])):
                                            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                                if not node_bin_dict[window[1]].has_key(assigned_key):
                                                    print "node_bin_dict[%s] does not have assigned_key (%s)" % (window[1],assigned_key,)
                                                elif float(requested_dict[assigned_key]) > float(node_bin_dict[window[1]][assigned_key]) - float(assigned_dict[assigned_key]):
                                                    print "requested_dict[%s] (%s) > node_bin_dict[%s][%s] (%s) - assigned_dict[%s] (%s)" % (assigned_key, requested_dict[assigned_key], window[1], assigned_key, node_bin_dict[window[1]][assigned_key], assigned_key, assigned_dict[assigned_key])
                                                else:
                                                    print "failed check"
                                                print "allopen = 0"
                                            allopen = 0
                                            #print "allopen == 0, breaking"
                                            break
                                    if assigned_key == 'node' and requested_dict[assigned_key] > 0:
                                        # assign all consumable_dict resources for node
                                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "assigned_key == 'node', assigning all consumable_dict resources for node"
                                        for conskey in resources_dict[window[1]]['consumable_dict'].keys():
                                            #print "topotune: conskey (%s) resources_dict[window[1]]['consumable_dict'].keys()" % (conskey,)
                                            if conskey == 'node':
                                                continue
                                            if assigned_dict.has_key(conskey):
                                                assigned_count = assigned_dict[conskey]
                                            else:
                                                assigned_count = 0
                                            #if not node_bin_dict[window[1]].has_key(conskey) or requested_dict[conskey] > node_bin_dict[window[1]][conskey] - assigned_dict[conskey]:
                                            if not node_bin_dict[window[1]].has_key(conskey) or resources_dict[window[1]]['consumable_dict'][conskey] > node_bin_dict[window[1]][conskey] - assigned_count:
                                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                                    print "allopen = 0, breaking"
                                                allopen = 0
                                                break
                                            else:
                                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                                    print "allopen = (%s)" % (allopen,)
                                   
    
                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "before assigned_nodes_list check, allopen (%s)" % (allopen,)
                                #if not window[1] in assigned_nodes_list and dict['requested_node'] <= node_bin_dict[window[1]]['node'] - assigned_node and dict['requested_cpu'] <= node_bin_dict[window[1]]['cpu'] - assigned_cpu and dict['requested_memory'] <= node_bin_dict[window[1]]['memory'] - assigned_memory :
                                if not window[1] in assigned_nodes_list and allopen == 1:
                                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "allopen == 1"
                                        print "appending window (%s)" % (window,)
                                    assigned_nodes_list.append(window[1])
                                    dict['saved_windows_list'].append(window)
                                    found_dict_count = found_dict_count + 1
                                        #assigned_node = assigned_node + dict['requested_node']
                                        #assigned_node = assigned_node + 1
                                        #assigned_cpu = assigned_cpu + dict['requested_cpu']
                                        #assigned_memory = assigned_memory + dict['requested_memory']
    
                                    #for assigned_key in assigned_dict.keys():
                                    for assigned_key in requested_dict.keys():
                                        #if assigned_key != 'node' and not assigned_key in resources_dict[window[1]]['consumable_dict'].keys():
                                        #    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        #        print "assigned_key(%s) not in resources_dict[window[1]]['consumable_dict']keys() (%s)" % (assigned_key, resources_dict[window[1]]['consumable_dict'].keys())
                                        #    continue
                                        #if assigned_key != 'node' and not assigned_key in requested_dict.keys():
                                        #    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        #        print "assigned_key(%s) not in requested_dict.keys() (%s)" % (assigned_key, requested_dict.keys())
                                        #    continue
                                        assigned_dict[assigned_key] = assigned_dict[assigned_key] + int(requested_dict[assigned_key])
                                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                            print "assigned_dict (%s) resources_dict[window[1]]['consumable_dict'] (%s)" % (assigned_dict, resources_dict[window[1]]['consumable_dict'])
                                        if assigned_key == 'node' and requested_dict[assigned_key] > 0:
                                            # assign all consumable_dict resources for node
                                            for conskey in resources_dict[window[1]]['consumable_dict'].keys():
                                                #print "topotune: conskey (%s) resources_dict[window[1]]['consumable_dict'].keys()" % (conskey,)
                                                if conskey == 'node':
                                                    continue
                                                if assigned_dict.has_key(conskey):
                                                    assigned_dict[conskey] = assigned_dict[conskey] + resources_dict[window[1]]['consumable_dict'][conskey]
                                                else:
                                                    assigned_dict[conskey] = resources_dict[window[1]]['consumable_dict'][conskey]
                                        continue
                                else :
                                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                        print "not appending (%s)" % (window,)
                                        print "node_bin_dict[window[1]] (%s)" % node_bin_dict[window[1]]
                                        print "assigned_nodes_list (%s), allopen (%s)" % (assigned_nodes_list, allopen)
                        
                                        #print "dict['requested_cpu'] (%s)" % dict['requested_cpu']
                                        #print "node_bin_dict[window[1]]['cpu'] (%s)" % node_bin_dict[window[1]]['cpu']
                                        #print "dict['requested_memory'] (%s)" % dict['requested_memory']
                                        #print "node_bin_dict[window[1]]['memory'] (%s)" % node_bin_dict[window[1]]['memory']
                            else:
                                pass
                                #print "no (%s) in dict['resource_dict'] (%s)" % (window[1],dict['resource_dict'],)
    
                            # what is this continue for?
                            # continue to fill next counting_dict
                            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "continuing to next counting_dict, found_dict_count (%s)" % (found_dict_count,)
                            continue
                    #print "after cdl loop: counting_dict_list (%s)" % (counting_dict_list,)
                    # if all dicts are satisfied, stop looking at nodes
                    #allfound = 1
                    #for dict in counting_dict_list :
                    #    if len(dict['saved_windows_list']) < dict['amount_int'] :
                    #        allfound = 0
                    #if allfound == 1:
                    #    break
                    # would be better to keep a count of how many dicts have been
                    # satsified...
                    if found_dict_count == required_dict_count:
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "found_dict_count (%s) == required_dict_count (%s),  breaking" % (found_dict_count, required_dict_count)
                            if dict.has_key('saved_windows_list'):
                                print "dict['saved_windows_list'] (%s)" % (dict['saved_windows_list'],)
                           # print "assigned_nodes_list (%s)" % (assigned_nodes_list,)
                           # print "counting_dict_list (%s)" % (counting_dict_list,)
                            #assigned_nodes_list.append(window[1])
                            #dict['saved_windows_list'].append(window)
                        break
                        # if TOPOLOGY_MODULE specified, check topology
                        #if TOPOLOGY_MODULE != None:
                        #    # generate a list of switch_tuples and max_hop
                        #    #switch_tuple_list = []
                        #    switch_node_list = assigned_nodes_list
                        #    jobs_dict = jobs_db_handle[0]
                        #    node_sets_list = Topology.get_switch_sets(switch_node_list, resources_dict, jobs_dict)
                        #else:
                        #    node_sets_list = [assigned_nodes_list,]
                        #    break
                        #topology_satisfied = 0
                        #for node_set in node_sets_list:
                        #    if set(node_set).issubset(assigned_nodes_list):
                        #        topology_satisfied = 1
                        #if topology_satisfied
                    else:
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "found_dict_count (%s) != required_dict_count (%s)" % (found_dict_count, required_dict_count)
    
                # at this point, if found_dict_count == required_dict_count,
                # the we have filled all counting_dict requirements, right?
                # can we skip the whole counting_dict_list check?
                ns_sized_windows_list = filter(lambda x, node_set=node_set: x[2]['nodename'] in node_set, sized_windows_list)
                if found_dict_count >= required_dict_count:
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "setting need_more = 0, found_dict_count (%s) required_dict_count (%s)" % (found_dict_count, required_dict_count)
                        print "counting_dict_list (%s)" % (counting_dict_list,)
                    #print "setting need_more = 0, found_dict_count (%s) required_dict_count (%s)" % (found_dict_count, required_dict_count)
                    #print "counting_dict_list (%s)" % (counting_dict_list,)
                    need_more = 0
                    found_one_set = 1
                    last_good_counting_dict_list = copy.deepcopy(counting_dict_list)
                    last_good_node_bin_dict = copy.deepcopy(node_bin_dict)
                    #ns_sized_windows_list = filter(lambda x, node_set=node_set: x[2]['nodename'] in node_set, sized_windows_list)
                else:
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "setting need_more = 1, found_dict_count (%s) required_dict_count (%s)" % (found_dict_count, required_dict_count)
                    #print "setting need_more = 1, found_dict_count (%s) required_dict_count (%s)" % (found_dict_count, required_dict_count)
                    need_more = 1
                #want_more = 0
                for dict in counting_dict_list :
                    #print "dict (%s)" % (dict,)
                    #print "topotune: dict (%s) counting_dict_list" % (dict,)
                    if dict['amount_int'] == None :
                        want_more = 1
                #print "want_more from amount_int check is (%s)" % (want_more,)
    
                #SKIP# check for any under-populated counting_dict_list entries
                #SKIPif DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                #SKIP    print "checking counting_dict_list"
                #SKIP    for counting_dict in counting_dict_list :
                #SKIP        print "saved_windows_list (%s) amount (%s) resource_dict (%s)" % (counting_dict['saved_windows_list'],counting_dict['amount_int'], counting_dict['resource_dict'].keys())
                #SKIPneed_more = 0
                #SKIPwant_more = 0
                #SKIPfor dict in counting_dict_list :
                #SKIP    # counting_dict_list is sized_dict plus
                #SKIP    # 'saved_windows_list'
                #SKIP    #sized_dict = {
                #SKIP    #  'initiatormap_index' : index,
                #SKIP    #  'amount_int' : dict['amount_int'],
                #SKIP    #  'saved_windows_list' : [],
                #SKIP    #  'resource_dict' : dict['resource_dict'],
                #SKIP    #  'req_list' : <from requested_resource_list>
                #SKIP    #  }
                #SKIP    if dict['amount_int'] == None :
                #SKIP        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                #SKIP            print "dict['amount_int'] == None, need_more = 1"
                #SKIP        want_more = 1
                #SKIP    # Look for new_res['node_usage'] node_shared or node_exclusive
                #SKIP    # if node_exclusive, look for node_bin_dict entries of type
                #SKIP    # 'node'.  If node_shared, count up cpu and memory.  if
                #SKIP    # dict has enough, save the nodename in a used_nodes_list.
    
                #SKIP    #enough_cpu = 0
                #SKIP    #enough_memory = 0
                #SKIP    #accrued_cpu = 0
                #SKIP    #accrued_memory = 0
                #SKIP    used_nodes_list = []
    
                #SKIP    # weird.  looks like this just takes cpu and memory
                #SKIP    # requested for the first req...
                #SKIP    #if dict.has_key('req_list') and len(dict['req_list']) > 0 :
                #SKIP    #    if dict['req_list'][0].has_key('cpu') :
                #SKIP    #        requested_cpu = dict['req_list'][0]['cpu']
                #SKIP    #    else :
                #SKIP    #        requested_cpu = 0
                #SKIP    #    if dict['req_list'][0].has_key('memory') :
                #SKIP    #        requested_memory = dict['req_list'][0]['memory']
                #SKIP    #    else :
                #SKIP    #        requested_memory = 0
                #SKIP    # This one should count up all cpu, memory, etc. for this node:
                #SKIP    requested_dict = {}
                #SKIP    if dict.has_key('req_list'):
                #SKIP        for req in dict['req_list']:
                #SKIP            for req_key in req.keys():
                #SKIP                if requested_dict.has_key(req_key):
                #SKIP                    requested_dict[req_key] = requested_dict[req_key + req[req_key]
                #SKIP                else:
                #SKIP                    requested_dict[req_key] = req[req_key]
                #SKIP    for saved_window in dict['saved_windows_list'] :
                #SKIP        # saved_window is (usability number, nodename)
                #SKIP        nodename = saved_window[1]
                #SKIP        if nodename in used_nodes_list :
                #SKIP            # don't re-use a node, even if it has sufficient
                #SKIP            # resources
                #SKIP            continue
                #SKIP        if node_bin_dict.has_key(nodename) :
                #SKIP        #    if node_bin_dict[nodename].has_key('cpu') :
                #SKIP        #        accrued_cpu = node_bin_dict[nodename]['cpu']
                #SKIP        #    if node_bin_dict[nodename].has_key('memory') :
                #SKIP        #        accrued_memory = node_bin_dict[nodename]['memory']
                #SKIP            accrued_dict = {}
                #SKIP            for resource_key in node_bin_dict[nodename].keys():
                #SKIP                accrued_dict[resource_key] = node_bin_dict[nodename][resource_key]
                #SKIP            resource_key = None
                #SKIP        
                #SKIP        #if accrued_cpu >= requested_cpu and accrued_memory >= requested_memory :
                #SKIP        enough_all = 1
                #SKIP        for enough_key in enough_dict.keys():
                #SKIP            enough_dict[enough_key] = 0
                #SKIP        for enough_key in enough_dict.keys():
                #SKIP            if accrued_dict[enough_key] < requested_dict[enough_key]:
                #SKIP                enough_all = 0
                #SKIP                break
                #SKIP        if enough_all == 1:
                #SKIP            used_nodes_list.append(nodename)
                #SKIP            for enough_key in enough_dict.keys():
                #SKIP                enough_dict[enough_key] = 1
                #SKIP            #enough_cpu = 1
                #SKIP            #enough_memory = 1
                #SKIP            break
                #SKIP    #if enough_cpu == 0 or enough_memory == 0 :
                #SKIP    for enough_key in enough_dict.keys():
                #SKIP        if enough_dict[enough_key] == 0:
                #SKIP            need_more = 1
                #SKIP            break
                #SKIP    if need_more == 1:
                #SKIP        break
    
                if need_more == 0 :
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "need_more == 0"
                    #print "need_more == 0"
                    if len(saved_sorting_list) > 0 :
                        if getlongest == 1 :
                            #candidate_windows_list = [current_start_time_float, sized_windows_list, sized_dict_list]
                            candidate_windows_list = [current_start_time_float, ns_sized_windows_list, sized_dict_list]
                            #if len(sized_windows_list) > 0 and (new_res['node_usage'] =='node_shared' or (sufficient_node == 1 and sufficient_cpu == 1 and sufficient_memory == 1)) :
                            #if len(sized_windows_list) > 0 and (new_res['node_usage'] == 'node_shared' or found_dict_count >= required_dict_count) :
                            if len(ns_sized_windows_list) > 0 and (new_res['node_usage'] == 'node_shared' or found_dict_count >= required_dict_count) :
                                # This appends a set of windows that does not fulfill
                                # the sharedmap requirement.  This should happen
                                # After the check of counting_dict_list
                                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                    print "appending candidate_windows_list with current_start_time (%s)" % time.asctime(time.localtime(current_start_time_float))
                                    print "ns_sized_windows_list (%s)" % (ns_sized_windows_list,)
                                sized_windows_lists_list.append(candidate_windows_list)
                    if want_more == 0 :
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "not breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(current_start_time_float)))
                            print "want_more == 0"
                        #print "breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(current_start_time_float)))
                        # with TOPOLOGY_MODULE, this breaks out of the node_set
                        # loop, but it should also break out of the outer window
                        # loop...
                        #FIXME should not break here, since we want all sized
                        # windows for all nodes sets until get_chosen selects.
                        # actually, we should break, since we have all sized_windows
                        # for this current_start_float.  If at least one node_set
                        # is sufficient here, we'll pass all sized_windows on
                        # to get_chosen
                        #break
                    else :
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "not breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(current_start_time_float)))
                        #print "not breaking : new_window[0] (%s) > current_start_time_fload (%s)" % (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(current_start_time_float)))
            # break out of node_set, if need_more and want_more satisfied
                if want_more == 0 and need_more == 0:
                    #print "breaking out of node_set loop with current_start_time_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
                    break

    #for new_window_index in range(len(open_windows_list)) :
            if want_more == 0 and found_one_set == 1:
                # break out of new_window_index loop
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "breaking out of new_window_index loop with current_start_time_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
                #print "breaking out of new_window_index loop with current_start_time_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
                break
            # if this is not the last window, set current start time
            # and filter out all windows too short with the new current
            # current start time
            if new_window_index + 1 < len(open_windows_list) :
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "old current_start_time_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
                current_start_time_float = open_windows_list[new_window_index + 1][0]
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "set current_start_time_float to (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(current_start_time_float)),)
                saved_windows_list = []
                #for i in range(len(ns_sized_windows_list)) :
                #    #print "topotune: i (%s) range(len(ns_sized_windows_list))" % (i,)
                #    if ns_sized_windows_list[i][1] - current_start_time_float >= \
                #    new_res['duration_float'] :
                #        saved_windows_list.append(ns_sized_windows_list[i])
                #ns_sized_windows_list = saved_windows_list
                for i in range(len(sized_windows_list)) :
                    #print "topotune: i (%s) range(len(ns_sized_windows_list))" % (i,)
                    if sized_windows_list[i][1] - current_start_time_float >= \
                    new_res['duration_float'] :
                        saved_windows_list.append(sized_windows_list[i])
                sized_windows_list = saved_windows_list

    #if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
    #    continue_var = raw_input("continue? ")
    if getlongest == 1 :
        print "getlongest == 1"
        # Append the last sized_windows_list
        #candidate_windows_list = [current_start_time_float, ns_sized_windows_list]
        if len(sized_windows_lists_list) > 0 :
            # choose the set of sized windows that fulfills sized_dict_list
            # and has the most nodes for all 'amount_int' == None
            # sort sized_dict_list to have any 'amount_int' == None
            # dicts last
            sized_dict_tuple_list = []
            # instead of using counting_dict_list here, should
            # add 'requested_dict' to sized_dict_list at start of
            # function
            #for dict in sized_dict_list :
            #for dict in counting_dict_list :
            for dict in sized_dict_list :
                #print "topotune: dict (%s) sized_dict_list" % (dict,)
                if dict['amount_int'] == None :
                    sized_dict_tuple_list.append((1,len(dict['resource_dict'].keys()), dict))
                else :
                    sized_dict_tuple_list.append((0,len(dict['resource_dict'].keys()), dict))
            sized_dict_tuple_list.sort()
            tmplist = []
            for tuple in sized_dict_tuple_list :
                tmplist.append(tuple[2])
            sized_dict_list = tmplist
            candidate_tuple_list = []
            for start_and_list in sized_windows_lists_list :
                #print "topotune: start_and_list (%s) sized_windows_list_list" % (start_and_list,)
                window_list = start_and_list[1]
                if len(window_list) == 0 :
                    continue
                current_start_time_float = start_and_list[0]
                #counting_dict_list = copy.deepcopy(sized_dict_list)
                #FIXME losing requested_dict here
                counting_dict_list = sized_dict_list
                for sized_dict in counting_dict_list :
                    #print "topotune: sized_dict (%s) counting_dict_list" % (sized_dict,)
                    sized_dict['saved_windows_list'] = []
                for window in window_list :
                    #print "topotune: window (%s) window_list" % (window,)
                    for sized_dict in counting_dict_list :
                        #print "topotune: sized_dict (%s) counting_dict_list" % (sized_dict,)
                        if sized_dict['amount_int'] == None :
                            # Here's where I check to see if the resource is
                            # usable.  Hmmm, a has_key should be lots faster
                            # Than seeing if name is in list.  For performance
                            # maybe should use resource_dict lists, but use
                            # empty values?
                            if sized_dict['resource_dict'].has_key(window[2]['nodename']) :
                                sized_dict['saved_windows_list'].append(window)
                                break
                            continue
                        if len(sized_dict['saved_windows_list']) >= sized_dict['amount_int'] :
                            continue
                        else :
                            if sized_dict['resource_dict'].has_key(window[2]['nodename']) :
                                sized_dict['saved_windows_list'].append(window)
                                break
                            continue
                need_more = 0
                none_count = 0
                for dict in counting_dict_list :
                    #print "topotune: dict (%s) counting_dict_list" % (dict,)
                    if dict['amount_int'] == None :
                        none_count = none_count + len(dict['saved_windows_list'])
                        if len(dict['saved_windows_list']) > 0 :
                            continue
                        else :
                            need_more = 1
                            break
                    if len(dict['saved_windows_list']) < dict['amount_int'] :
                        need_more = 1
                        break
                if need_more == 0 :
                    if new_res.has_key('max_resource_int') and new_res['max_resource_int'] != None :
                        window_count = min(none_count, new_res['max_resource_int'])
                    else :
                        window_count = none_count
                    candidate_tuple_list.append((-(window_count), -(copy.copy(current_start_time_float)), copy.deepcopy(counting_dict_list)))
            if len(candidate_tuple_list) > 0 :
                candidate_tuple_list.sort()
                counting_dict_list = candidate_tuple_list[-1][2]
                #set current_start_time to last candidate_tuple_list
                current_start_time_float = -(candidate_tuple_list[-1][1])
                sized_windows_list = []
                for counting_dict in counting_dict_list :
                    #print "topotune: counting_dict (%s) counting_dict_list" % (counting_dict,)
                    sized_windows_list = sized_windows_list + counting_dict['saved_windows_list']
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "len(candidate_tuple_list) (%s)" % len(candidate_tuple_list)
                    print "len(sized_windows_list) (%s)" % len(sized_windows_list)
            else :
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "zero-length candidate_tuple_list"
                sized_windows_list = []
        else :
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "zero-length sized_windows_lists_list"
            sized_windows_list = []
    else:
        # FIXME should actually return all sized windows for all node sets
        #sized_windows_list = ns_sized_windows_list
        pass
    if found_one_set == 0:
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "need_more == 1, setting sized_windows_list to []"
        #print "need_more == 1, setting sized_windows_list to []"
        sized_windows_list = []
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "-------------------------------"
        for sized_window in sized_windows_list:
            print "sized_window: %s" % (sized_window,)
        print "-------------------------------"
        print "final current_start_time (%s)" % time.asctime(time.localtime(current_start_time_float))
    #print "counting_dict_list (%s)" % (counting_dict_list,)
    #for counting_dict in counting_dict_list :
    #    print "countind_dict['requested_dict'] (%s)" % (counting_dict['requested_dict'],)
    return ( sized_windows_list, current_start_time_float, counting_dict_list, node_bin_dict )

def get_sorted_windows_list(sized_windows_list, new_res, accepted_nodes_list, sort_policy_code, resources_db_handle, reservations_db_handle, jobs_db_handle, windows_and_reservations_list=None) :
    # The ideal way for the sort policy to work, is to allow the
    # administrator to specify a set of sort policies and the order in
    # which they are to be applied.  Since the built-in sort() will
    # sort lexicographically, each policy can be expressed as a number
    # for the window, and the order of each policy can be its place in
    # the sort tuple.
    def get_sorted(input_tuple) :
        # (start, end, node name)
        # this should sort first on window start time
        # and second on resource usability
        # 20101217: We should also consider window end time,
        # taking the lowest end time first.  This will use up
        # small windows, leaving larger ones.
        windows = input_tuple[0]
        nodes = input_tuple[1]
        new_res = input_tuple[2]
        resources_dict = input_tuple[5]
        temp_windows = []
        for window in windows :
            if resources_dict[window[2]['nodename']].has_key('resource_usability_int') :
                #temp_windows.append((- (window[0]), long(resources_dict[window[2]]['resource_usability_int']), window))
                #temp_windows.append((- (window[0]), window[1], long(resources_dict[window[2]]['resource_usability_int']), window))
                temp_windows.append((- (window[0]), long(resources_dict[window[2]]['resource_usability_int']), window[1], window))
            else :
                #temp_windows.append((- (window[0]), 0L, window))
                temp_windows.append((- (window[0]), window[1], 0L, window))
        temp_windows.sort()
        result = []
        for window in temp_windows :
            result.append(window[-1]['nodename'])
        return result

    def get_sorted_with_affinity(input_tuple) :
        windows = input_tuple[0]
        nodes = input_tuple[1]
        new_res = input_tuple[2]
        windows_and_reservations_list = input_tuple[4]
        resources_dict = input_tuple[5]
        reservations_dict = input_tuple[6]
        jobs_dict = input_tuple[7]
        temp_windows = []

#        # maybe move this down into the reservation in relevant_reservations
#        # loop?
#        # for shared job reservations, should pack onto used nodes
#        # do this by node_fraction_occupied
#        # instead of walking through the reservations twice, could
#        # use pointers in the sort tuple...
#        node_pack_dict = {}
#        for node in nodes:
#            node_pack_dict[node] = { 'ConsumableCpus' : max(1,resources_dict[node]['ConsumableCpus']),
#                                     'ConsumableMemory' : max(1,resources_dict[node]['ConsumableMemory']),
#                                     'ConsumedCpus' : 0,
#                                     'ConsumedMemory' : 0,
#                                   }
#        if new_res.has_key('job_runID') and jobs_dict[new_res['job_runID']]['node_usage'] == 'shared':
#            for window_tuple in windows_and_reservations_list :
#                relevant_reservations_list = window_tuple[1]
#                for relevant_reservation in relevant_reservations_list:
#                    for allocated_dict in relevant_reservation['allocated_dict_list']:
#                        if allocated_dict['nodename'] in nodes:
#                            if allocated_dict['type'] == 'node_exclusive':
#                                node_pack_dict[allocated_dict['nodename']]['ConsumedCpus'] = \
#                                  node_pack_dict[allocated_dict['nodename']]['ConsumableCpus']
#                                node_pack_dict[allocated_dict['nodename']]['ConsumedMemory'] = \
#                                  node_pack_dict[allocated_dict['nodename']]['ConsumableMemory']
#                            else:
#                                if allocated_dict.has_key('cpu') and allocated_dict['cpu'] != None:
#                                    node_pack_dict[allocated_dict['nodename']]['ConsumedCpus'] = \
#                                      node_pack_dict[allocated_dict['nodename']]['ConsumedCpus'] \
#                                      + allocated_dict['cpu']
#                                if allocated_dict.has_key('memory') and allocated_dict['memory'] != None:
#                                    node_pack_dict[allocated_dict['nodename']]['ConsumedMemory'] = \
#                                      node_pack_dict[allocated_dict['nodename']]['ConsumedMemory'] \
#                                      + allocated_dict['memory']
#        else:
#            pass
#
#        for node in nodes:
#            node_pack_dict[node]['fraction'] = \
#              max(float(node_pack_dict[node]['ConsumedCpus'])
#              /float(node_pack_dict[node]['ConsumableCpus']),
#              float(node_pack_dict[node]['ConsumedMemory'])
#              /float(node_pack_dict[node]['ConsumableMemory']))
        if TOPOLOGY_MODULE != None:
            used_switch_dict = {}
            nodenames_list = resources_dict.keys()
            #node_switch_dict = Topology.get_node_switch_dict(nodenames_list)
            node_switch_dict = get_node_switch_dict(nodenames_list)
            #if node_switch_dict == None:
            #    node_switch_dict = {}
            #    for resourcename in resources_dict.keys():
            #        node_switch_dict[resourcename] = 'dummyswitch'
            if new_res.has_key('job_runID') and new_res['job_runID'] != None:
                #print "new_res['job_runID'] (%s)" % new_res['job_runID']
                rest_input_tuple = (copy.deepcopy(jobs_dict[new_res['job_runID']]),)
                #rest_input_tuple = (jobs_dict[new_res['job_runID']],)
            else:
                job_step = Catalina____RESOURCEMANAGER_PLACEHOLDER___.initialize_job_step('temp_job_step')
                rest_input_tuple = ( job_step, )
            for reservation_key in reservations_dict.keys():
                reservation = reservations_dict[reservation_key]
                #if (new_res['start_time_float'] <= reservation['start_time_float'] < new_res['end_time_float']) or (new_res['start_time_float'] < reservation['end_time_float'] <= new_res['end_time_float']):
                #if reservation['end_time_float'] == None:
                #    print "None end_time_float (%s)" % (reservation,)
                #print "reservation['start_time_float'] (%s) reservation['duration_float'] (%s) new_res['start_time_float'] (%s) new_res['duration_float'] (%s)" % (reservation['start_time_float'], reservation['duration_float'], new_res['start_time_float'], new_res['duration_float'])
                if reservation['start_time_float'] == None:
                    continue
                if not (reservation['start_time_float'] + reservation['duration_float'] <= new_res['start_time_float'] or new_res['start_time_float'] + new_res['duration_float'] <= reservation['start_time_float']):
                    result = apply_policy_code(reservation['job_restriction'],
                        rest_input_tuple)
                    if result != 0 :
                        for allocated_dict in reservation['allocated_dict_list']:
                            nodename = allocated_dict['nodename']
                            if node_switch_dict.has_key(nodename):
                                switchname = node_switch_dict[nodename]
                                if used_switch_dict.has_key(switchname):
                                    if not nodename in used_switch_dict[switchname]:
                                        used_switch_dict[switchname].append(nodename)
                                else:
                                    used_switch_dict[switchname] = [nodename,]
                    else:
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "not restricted (%s) (%s)" % (reservation['job_restriction'], rest_input_tuple[0]['name'])
                else:
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "reservation (%s) (%s) does not overlap new_res (%s) (%s)" % (reservation['start_time_float'], reservation['end_time_float'], new_res['start_time_float'], new_res['end_time_float'],)
        #print "used_switch_dict (%s)" % (used_switch_dict,)
        
        for window_tuple in windows_and_reservations_list :
            window = window_tuple[0]
            #print "window (%s)" % (window,)
            nodename = window[2]['nodename']
            relevant_reservations_list = window_tuple[1]
            job_step = window_tuple[2]
            # find those "pushback" reservations on this node
            # decrement affinity for each pushback res on the node
            pushback_affinity = 0
            for res_key in reservations_dict.keys() :
                if reservations_dict[res_key]['purpose_type_string'] == "pushback" :
                    for allocated_dict in reservations_dict[res_key]['allocated_dict_list'] :
                        if allocated_dict['nodename'] == window_tuple[0][2]['nodename'] :
                            pushback_affinity = pushback_affinity + jobs_dict[reservations_dict[res_key]['job_runID']]['priority']
            affinity = 0
            nodepack_dict = {}
            for reservation in relevant_reservations_list :
                # this needs to be job step for res, not from each window...
                #input_tuple = (job_step,)
                if (new_res['start_time_float'] <= reservation['start_time_float'] < new_res['end_time_float']) or (new_res['start_time_float'] < reservation['end_time_float'] <= new_res['end_time_float']):
                    if new_res.has_key('job_runID') and new_res['job_runID'] != None:
                        input_tuple = (copy.deepcopy(jobs_dict[new_res['job_runID']]),)
                    else:
                        job_step = Catalina____RESOURCEMANAGER_PLACEHOLDER___.initialize_job_step('temp_job_step')
                        input_tuple = ( job_step, )
                    if reservation['affinity_calculation'] != None :
                        result = 0
                        reservation_affinity_code = reservation['affinity_calculation']
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "reservation (%s) reservation_affinity_code (%s)" % (reservation, reservation_affinity_code)
                        #temp_dict = locals()
                        #exec "%s" % reservation_affinity_code in globals(), temp_dict
                        result = apply_policy_code(reservation_affinity_code,
                          input_tuple)
                        #result = temp_dict['result']
                        #print "reservation_affinity_code (%s) input_tuple (%s) result (%s)" % (reservation_affinity_code, input_tuple, result)
                    else :
                        result = 1
                    if result != None:
                        affinity = affinity + result

                    # find consumable resource occupancy for this nodename
                    #print "reservation['allocated_dict_list'] (%s)" % (reservation['allocated_dict_list'],)
                    #rest_input_tuple = ( job_step, )
                    result = apply_policy_code(reservation['job_restriction'],
                        input_tuple)
                    if result != 0 :
                        # blocking, so count up allocated resources
                        for allocated_dict in reservation['allocated_dict_list']:
                            if nodename == allocated_dict['nodename']:
                                for ares in allocated_dict.keys():
                                    if resources_dict[nodename]['consumable_dict'].has_key(ares):
                                        if nodepack_dict.has_key(ares):
                                            nodepack_dict[ares] = nodepack_dict[ares] + allocated_dict[ares]
                                        else:
                                            nodepack_dict[ares] = allocated_dict[ares]
            # find max nodepack fraction
            nodepack = 0.0
            for cres in resources_dict[nodename]['consumable_dict'].keys():
                if nodepack_dict.has_key(cres):
                    crespack = float(nodepack_dict[cres]) / float(resources_dict[nodename]['consumable_dict'][cres])
                    #print "nodepack_dict[%s] (%s) resources_dict[nodename]['consumable_dict'][%s] (%s) nodepack_dict[%s]/resources_dict[nodename]['consumable_dict'][%s] (%s)" % (cres, nodepack_dict[cres], cres, resources_dict[nodename]['consumable_dict'][cres], cres, cres, crespack)
                    if crespack > nodepack:
                        nodepack = crespack

            if resources_dict[window_tuple[0][2]['nodename']].has_key('resource_usability_int') :
                resource_sort_int = resources_dict[window_tuple[0][2]['nodename']]['resource_usability_int']
            else :
                resource_sort_int = 0
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                #print "sort components (%s) (%s)" % (window[0], resource_sort_int)
                print " affinity (%s)" % (affinity,)

            if TOPOLOGY_MODULE != None:
                #topo = Topology.get_priority(
                #nodename = window[2]['nodename']
                #topo = Topology.get_nodepriority(nodename, resources_dict, jobs_dict, reservations_dict)
                #node_switch_dict = Topology.get_node_switch_dict()
                if node_switch_dict.has_key(nodename):
                    switchname = node_switch_dict[nodename]
                    if used_switch_dict.has_key(switchname):
                        topo = len(used_switch_dict[switchname])
                    else:
                        topo = 0
                else:
                    topo = 0
            else:
                topo = 0
            #print "topo (%s)" % (topo,)
            if window[0] >= Now_float:
                lastavail = window[0]
            else:
                lastavail = 0

            if affinity == 0 :
                #sort_tuple = ( 0, -(pushback_affinity), -(window[0]), long(resource_sort_int), -node_pack_dict[window_tuple[0][2]['nodename']]['fraction'], window)
                #sort_tuple = ( 0, -(pushback_affinity), -(window[0]), long(resource_sort_int), window)
                #sort_tuple = (0, -(pushback_affinity), -(window[0]), -(topo), -(nodepack), long(resource_sort_int), window)
                #sort_tuple = (0, -(pushback_affinity), -(window[0]), long(resource_sort_int), window)
                #sort_tuple = (0, -(pushback_affinity), -(window[0]), -(topo), -(nodepack), long(resource_sort_int), window)
                #sort_tuple = (0, -(pushback_affinity), -(lastavail), -(topo), -(nodepack), long(resource_sort_int), window)
                sort_tuple = (0, -(pushback_affinity), -(lastavail), long(resource_sort_int), -(topo), -(nodepack), window)
            else :
                #sort_tuple = (-(affinity), -(pushback_affinity), -(window[0]), long(resource_sort_int), -node_pack_dict[window_tuple[0][2]['nodename']]['fraction'], window)
                #sort_tuple = (-(affinity), -(pushback_affinity), -(window[0]), long(resource_sort_int), window)
                #sort_tuple = (-(affinity), -(pushback_affinity), -(window[0]), long(resource_sort_int), window)
                #sort_tuple = (-(affinity), -(pushback_affinity), -(window[0]), -(topo), -(nodepack), long(resource_sort_int), window)
                #sort_tuple = (-(affinity), -(pushback_affinity), -(window[0]), long(resource_sort_int), window)
                #sort_tuple = (-(affinity), -(pushback_affinity), -(window[0]), -(topo), -(nodepack), long(resource_sort_int), window)
                #sort_tuple = (-(affinity), -(pushback_affinity), -(lastavail), -(topo), -(nodepack), long(resource_sort_int), window)
                sort_tuple = (-(affinity), -(pushback_affinity), -(lastavail), long(resource_sort_int), -(topo), -(nodepack), window)
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "sort_tuple (%s)" % (sort_tuple,)
            temp_windows.append(sort_tuple)
        temp_windows.sort()
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            for sort_tuple in temp_windows :
                print "post-sort sort_tuple (%s)" % (sort_tuple,)
        windows = []
        for window_tuple in temp_windows :
            windows.append(window_tuple[-1])
        result = windows
        return result

    temp_nodes_list = accepted_nodes_list[:]
    resources_dict = resources_db_handle[0]
    reservations_dict = reservations_db_handle[0]
    jobs_dict = jobs_db_handle[0]
    input_tuple = (sized_windows_list,temp_nodes_list,new_res,Now_float,windows_and_reservations_list,resources_dict, reservations_dict, jobs_dict)
    if windows_and_reservations_list != None :
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "running get_sorted_with_affinity"
        sorted_windows_list_list = get_sorted_with_affinity(input_tuple)
    elif sort_policy_code == None :
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "running get_sorted"
        sorted_windows_list_list = get_sorted(input_tuple)
    else :
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "running custom sorting code"
        sorted_windows_list_list = apply_policy_code(sort_policy_code, input_tuple)
    if DEBUGJOB != None and new_res.has_key('job_runID') and DEBUGJOB == new_res['job_runID'] :
        print "sorted windows:"
        for window in sorted_windows_list_list :
            print "%s %s %s" % (time.asctime(time.localtime(window[0])),time.asctime(time.localtime(window[1])),window[2])
        print "end windows"
    return sorted_windows_list_list

def get_chosen_nodes_list(new_res, sorted_windows_list, counting_dict_list, node_bin_dict, resources_db_handle, jobs_db_handle) :
    #print "start of get_chosen_nodes_list"
    def getfirstindex(x) :
        return(x[1])
    # for multiple job/node, need to construct a resoure_dict_list based
    # on initiatormap.  For each initiator set, choose a node that
    # matches, deduct the requested_cpu and requested_memory from that
    # node, then go on to the next initiator set.  Windows should be
    # sorted already...Need to sort the initiator sets...Maybe should
    # save the sorted list from get_sized?
    #for counting_dict in counting_dict_list :
    #    print "countind_dict['requested_dict'] (%s)" % (counting_dict['requested_dict'],)
    resources_dict = resources_db_handle[0]
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "get_chosen_nodes_list: counting_dict_list (%s)" % (counting_dict_list,)
    chosen_nodes_list = []
    resource_amount_int = new_res['resource_amount_int']
    #resource_dict_list = new_res['resource_dict_list']
    #counting_dict_list = resource_dict_list
    for counting_dict in counting_dict_list :
        counting_dict['saved_windows_list'] = []
    max_resource_int = new_res['max_resource_int']
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "max_resource_int (%s)" % (max_resource_int,)
    # need to allow for case where a node has enough cpu and memory
    # for two initiator sets...
    #for nodename in node_bin_dict.keys() :
        #node_bin_dict[nodename]['remaining_node'] = node_bin_dict[nodename]['node']
        #node_bin_dict[nodename]['remaining_cpu'] = node_bin_dict[nodename]['cpu']
        #node_bin_dict[nodename]['remaining_memory'] = node_bin_dict[nodename]['memory']
    #    for rkey in resources_dict[nodename]['consumable_list']:
    #        node_bin_dict[nodename]['remaining_node'] = node_bin_dict[nodename]['node']
    #        node_bin_dict[nodename]['remaining_cpu'] = node_bin_dict[nodename]['cpu']
    #        node_bin_dict[nodename]['remaining_memory'] = node_bin_dict[nodename]['memory']
    # for each element in the counting_dict_list, accumulate sorted_windows
    # in order until the cpu and memory requirement for all initiators is
    # met.  Choose that node.  It may be possible to miss a good combination...
    # counting_dict_list should be sorted by viable_request_count from
    # get_sized...re-sort here, according to len(resource_dict.keys())
    # That should ensure that the reqs get filled.
    sorting_resource_dict_list = []
    for counting_dict in counting_dict_list :
        sorting_resource_dict_list.append((len(counting_dict['resource_dict'].keys()), counting_dict))
    sorting_resource_dict_list.sort()
    counting_dict_list = map(getfirstindex, sorting_resource_dict_list)
    #consumed_resource_dict = {}
    allocated_dict_list = []
    need_more = 1

    #sorted_node_sets_list = get_node_sets(new_res, node_bin_dict, resources_db_handle, jobs_db_handle)
    switch_node_list = []
    for sorted_window in sorted_windows_list:
        if not sorted_window[2]['nodename'] in switch_node_list:
            switch_node_list.append(sorted_window[2]['nodename'])
    sorted_node_sets_list = get_node_sets(new_res, switch_node_list, resources_db_handle, jobs_db_handle)
    for node_set in sorted_node_sets_list:
        consumed_resource_dict = {}
        allocated_dict_list = []
        chosen_nodes_list = []

        #print "check node_set (%s)" % (node_set,)
        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            print "counting_dict_list (%s)" % (counting_dict_list,)
        #print "counting_dict_list (%s) len(counting_dict_list) (%s)" % (counting_dict_list,len(counting_dict_list))
        #if new_res['purpose_type_string'] == 'standing_reservation':
        #    print "sdebug: checking node_set (%s)" % (node_set,)
        #for counting_dict in counting_dict_list :
        #    print "countind_dict['requested_dict'] (%s)" % (counting_dict['requested_dict'],)
        for counting_dict in counting_dict_list :
            #if new_res['purpose_type_string'] == 'standing_reservation':
            #    print "sdebug: processing counting_dict (%s)" % (counting_dict,)
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "processing counting_dict (%s)" % (counting_dict,)
                print "consumed_resource_dict (%s)" % (consumed_resource_dict,)
            
            # right here, may need to do something special if requested_cpu
            # or requested_memory is 0.
            # if amount_int for this dict is None, then grab as many
            # nodes as are available.
            # if requested_cpu is None or not present, grab all cpu
            # on each node.  same for memory
    
            #if not counting_dict.has_key('requested_node') or counting_dict['requested_node'] == None :
            #    requested_node = None
            #else :
            #    requested_node = counting_dict['requested_node']
            #if not counting_dict.has_key('requested_cpu') or counting_dict['requested_cpu'] == None :
            #    requested_cpu = None
            #else :
            #    requested_cpu = counting_dict['requested_cpu']
            #if not counting_dict.has_key('requested_memory') or counting_dict['requested_memory'] == None :
            #    requested_memory = None
            #else :
            #    requested_memory = counting_dict['requested_memory']
            #if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            #    print "requested_node (%s)" % requested_node
            #    print "requested_cpu (%s)" % requested_cpu
            #    print "requested_memory (%s)" % requested_memory
            absrequested_dict = {}
            #if not counting_dict.has_key('requested_dict'):
            #    print "counting_dict has no requested_dict (%s)" % (counting_dict,)
            #else:
            #    print "counting_dict (%s)" % (counting_dict,)
            #print "counting_dict (%s)" % (counting_dict,)
            for rkey in counting_dict['requested_dict'].keys():
                if not counting_dict['requested_dict'].has_key(rkey) or counting_dict['requested_dict'][rkey] == None :
                    absrequested_dict[rkey] = None
                else :
                    absrequested_dict[rkey] = counting_dict['requested_dict'][rkey]
    
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "absrequested_dict (%s)" % (absrequested_dict,)
            #print "absrequested_dict (%s)" % (absrequested_dict,)
    
            #sorted_res_dict = {}
            sorted_res_dict = {}
            need_more = 1
            for window in sorted_windows_list :
                #if new_res['purpose_type_string'] == 'standing_reservation':
                #    print "sdebug: processing sorted_window (%s)" % (window,)
                if  DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "processing sorted window (%s)" % (window,)
                nodename = window[2]['nodename']
                # skip nodes we have already chosen
                if nodename in chosen_nodes_list:
                    continue
                if not nodename in node_set:
                    continue
    
                requested_dict = {}
                #print "absrequested_dict (%s)" % (absrequested_dict,)
                for rkey in absrequested_dict.keys():
                    if rkey == 'node' and absrequested_dict[rkey] != None and absrequested_dict[rkey] > 0:
                        #print "rkey is node, taking all consumables in (%s)" % (resources_dict[nodename]['consumable_dict'],)
                        for ckey in resources_dict[nodename]['consumable_dict'].keys():
                            if not absrequested_dict.has_key(ckey):
                                requested_dict[ckey] = resources_dict[nodename]['consumable_dict'][ckey]
                requested_dict.update(absrequested_dict)
    
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "requested_dict (%s)" % (requested_dict,)
                if not nodename in counting_dict['resource_dict'].keys() :
                    continue
                window_resource_dict = {}
                allocated_dict = {}
                if consumed_resource_dict.has_key(nodename) :
                    #consumed_node = consumed_resource_dict[nodename]['node']
                    window_resource_dict['node'] = consumed_resource_dict[nodename]['node']
                    #consumed_cpu = consumed_resource_dict[nodename]['cpu']
                    #consumed_memory = consumed_resource_dict[nodename]['memory']
                    for rkey in resources_dict[nodename]['consumable_dict'].keys():
                        window_resource_dict[rkey] = consumed_resource_dict[nodename][rkey]
                else :
                    #consumed_node = 0
                    #consumed_cpu = 0
                    #consumed_memory = 0
                    window_resource_dict['node'] = 0
                    for rkey in resources_dict[nodename]['consumable_dict'].keys():
                        window_resource_dict[rkey] = 0
                if not sorted_res_dict.has_key(nodename) :
                    #sorted_res_dict[nodename] = {
                    #  'node' : 0,
                    #  'cpu' : 0,
                    #  'memory' : 0
                    #  }
                    sorted_res_dict[nodename] = {'node' : 0}
                    for rkey in resources_dict[nodename]['consumable_dict'].keys():
                        sorted_res_dict[nodename][rkey] = 0
                if window[2].has_key('node') :
                    sorted_res_dict[nodename]['node'] = sorted_res_dict[nodename]['node'] + window[2]['node']
                    # window with 'node' resource type means node_exclusive,
                    # so all consumable resources should be available...
                    # actually, not true, resources must be explicitly named
                    #for rkey in resources_dict[nodename]['consumable_dict'].keys():
                    #    sorted_res_dict[nodename][rkey] = resources_dict[nodename]['consumable_dict'][rkey]
                #if window[2].has_key('cpu') and window[2]['cpu'] != None :
                #    sorted_res_dict[nodename]['cpu'] = sorted_res_dict[nodename]['cpu'] + window[2]['cpu']
                #if window[2].has_key('memory') and window[2]['memory'] != None :
                #    sorted_res_dict[nodename]['memory'] = sorted_res_dict[nodename]['memory'] + window[2]['memory']
                else:
                    for rkey in resources_dict[nodename]['consumable_dict'].keys():
                        if window[2].has_key(rkey) and window[2][rkey] != None :
                            sorted_res_dict[nodename][rkey] = sorted_res_dict[nodename][rkey] + window[2][rkey]
                #print "sorted_res_dict (%s)" % (sorted_res_dict,)
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "nodename (%s)" % nodename
                    print "window (%s)" % (window,)
                    print "sorted_res_dict[nodename]['node'] (%s)" % sorted_res_dict[nodename]['node']
                    #print "sorted_res_dict[nodename]['cpu'] (%s)" % sorted_res_dict[nodename]['cpu']
                    #print "sorted_res_dict[nodename]['memory'] (%s)" % sorted_res_dict[nodename]['memory']
                #print "nodename (%s)" % nodename
                #print "window (%s)" % (window,)
                #print "sorted_res_dict[nodename]['node'] (%s)" % sorted_res_dict[nodename]['node']
                request_resource_none = 0
                for rkey in requested_dict.keys():
                    if requested_dict[rkey] == None:
                        request_resource_none = 1
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "requested_dict[%s] == None, breaking" % (rkey,)
                        break
                #if requested_dict['node'] == None:
                #if requested_dict['node'] == None or requested_dict['node'] == 0:
                if requested_dict['node'] != None and requested_dict['node'] != 0:
                    if consumed_resource_dict.has_key(nodename) :
                        #allocated_node = 1 - consumed_resource_dict[nodename]['node']
                        allocated_dict['node'] = 1 - consumed_resource_dict[nodename]['node']
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "requested_dict['node'] is None, consumed_resource_dict.has_key(%s) allocated_dict['node'] (%s)" % (nodename, allocated_dict['node'])
                    else :
                        #allocated_node = 1
                        allocated_dict['node'] = 1
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "requested_dict['node'] is None, NOT consumed_resource_dict.has_key(%s) allocated_dict['node'] (%s)" % (nodename, allocated_dict['node'])
                else :
                    # here requested_dict['node'] != None :
                    # so make sure everything on the node is allocated
                    #allocated_node = requested_node
                    #print "allocated_dict (%s) requested_dict (%s)" % (allocated_dict, requested_dict)
                    allocated_dict['node'] = requested_dict['node']
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "requested_node is (%s), allocated_node (%s)" % (requested_dict['node'], allocated_dict['node'])
#UNNEEDED?                print "checking for rkey mismatch"
#UNNEEDED?                rkeymismatch = 0
#UNNEEDED?                for rkey in resources_dict[nodename]['consumable_dict'].keys():
#UNNEEDED?                    if not requested_dict.has_key(rkey):
#UNNEEDED?                        print "requested_dict[rkey] mismatch (%s) (%s) (%s)" % (rkey, requested_dict, resources_dict[nodename]['consumable_dict'])
#UNNEEDED?                        rkeymismatch = 1
#UNNEEDED?                        break
#UNNEEDED?
#UNNEEDED?                    # need to handle the case where requested_dict only has 'node':
#UNNEEDED?                    #requested_dict[rkey] mismatch (cpu) ({'node': 1}) ({'cpu': 32, 'memory': 67592482816})
#UNNEEDED?                    if not requested_dict.has_key(rkey) or requested_dict[rkey] == None :
#UNNEEDED?                    #if not requested_dict.has_key(rkey) or requested_dict[rkey] == None :
#UNNEEDED?                        if consumed_resource_dict.has_key(nodename) :
#UNNEEDED?                            allocated_dict[rkey] = resources_db_handle[0][nodename][rkey] - consumed_resource_dict[nodename]['consumable_dict'][rkey]
#UNNEEDED?                        else :
#UNNEEDED?                            allocated_dict[rkey] = resources_db_handle[0][nodename]['consumable_dict'][rkey]
#UNNEEDED?                    else :
#UNNEEDED?                        allocated_dict[rkey] = requested_dict[rkey]
#UNNEEDED?                if rkeymismatch == 1:
#UNNEEDED?                    continue
#TRYTHIS allocate everything on the node, do not check for rkey mismatch
                    # need to handle the case where requested_dict only has 'node':
                    #requested_dict[rkey] mismatch (cpu) ({'node': 1}) ({'cpu': 32, 'memory': 67592482816})
                #print "adding rkeys to allocated_dict (%s) requested_dict(%s)" % (allocated_dict, requested_dict)
                for rkey in resources_dict[nodename]['consumable_dict'].keys():
                    #print "doing rkey(%s)" % rkey
                    #if not requested_dict.has_key(rkey) or requested_dict[rkey] == None :
                    #if (not requested_dict.has_key(rkey)) or requested_dict[rkey] == None or (requested_dict['node'] != None and requested_dict['node'] != 0):
                    #if not requested_dict.has_key(rkey) or requested_dict[rkey] == None :
                    if requested_dict.has_key('node') and requested_dict['node'] > 0:
                        #print "requested_dict[node] found (%s)" % (requested_dict['node'],)
                        if consumed_resource_dict.has_key(nodename) :
                            allocated_dict[rkey] = resources_db_handle[0][nodename][rkey] - consumed_resource_dict[nodename]['consumable_dict'][rkey]
                            #print "nodename in consumed_resource_dict, adding rmeainder to allocated_dict (%s)" % resources_db_handle[0][nodename][rkey] - consumed_resource_dict[nodename]['consumable_dict'][rkey]
                        else :
                            #print "nodename not in cosumed_resource_dict, setting full consumable (%s)" % resources_db_handle[0][nodename]['consumable_dict'][rkey]
                            allocated_dict[rkey] = resources_db_handle[0][nodename]['consumable_dict'][rkey]
                    else :
                        if not requested_dict.has_key(rkey):
                            #print "no %s in requested_dict (%s)" % (rkey, requested_dict)
                            pass
                        else:
                            #print "requested_dict['node'] (%s) not exclusive, adding requested amount (%s) or (%s)" % (requested_dict['node'], requested_dict[rkey], rkey)
                            allocated_dict[rkey] = requested_dict[rkey]
#TRYTHIS
    #
                #print "requested_dict (%s)" % (requested_dict,)
                #print "resources_db_handle[0][nodename] (%s)" % (resources_db_handle[0][nodename],)
                #print "allocated_dict (%s)" % (allocated_dict,)
                #if requested_node == None or requested_cpu == None or requested_memory == None :
                if requested_dict['node'] == None or requested_dict['node'] == 0 or request_resource_none == 1:
                    allocated_usage = 'node_shared'
    #                if requested_dict['node'] == None :
    #                    if consumed_resource_dict.has_key(nodename) :
    #                        #allocated_node = 1 - consumed_resource_dict[nodename]['node']
    #                        allocated_dict['node'] = 1 - consumed_resource_dict[nodename]['node']
    #                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
    #                            print "requested_dict['node'] is None, consumed_resource_dict.has_key(%s) allocated_node (%s)" % (nodename, allocated_node)
    #                    else :
    #                        #allocated_node = 1
    #                        allocated_dict['node'] == 1
    #                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
    #                            print "requested_dict['node'] is None, NOT consumed_resource_dict.has_key(%s) allocated_dict['node'] (%s)" % (nodename, allocated_dict['node'])
    #                else :
    #                    #allocated_node = requested_node
    #                    allocated_dict['node'] == requested_dict['node']
    #                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
    #                        print "requested_node is (%s), allocated_node (%s)" % (requested_dict['node'], allocated_dict['node'])
                    #if requested_cpu == None :
                    #    if consumed_resource_dict.has_key(nodename) :
                    #        allocated_cpu = resources_db_handle[0][nodename]['ConsumableCpus'] - consumed_resource_dict[nodename]['cpu']
                    #    else :
                    #        allocated_cpu = resources_db_handle[0][nodename]['ConsumableCpus']
                    #else :
                    #    allocated_cpu = requested_cpu
                    #if requested_memory == None :
                    #    if consumed_resource_dict.has_key(nodename) :
                    #        allocated_memory = resources_db_handle[0][nodename]['ConsumableMemory'] - consumed_resource_dict[nodename]['memory']
                    #    else :
                    #        allocated_memory = resources_db_handle[0][nodename]['ConsumableMemory']
                    #else :
                    #    allocated_memory = requested_memory
    #                for rkey in resources_dict[nodename]['consumable_dict'].keys():
    #                    if requested_dict[rkey] == None :
    #                        if consumed_resource_dict.has_key(nodename) :
    #                            allocated_dict[rkey] = resources_db_handle[0][nodename][rkey] - consumed_resource_dict[nodename][rkey]
    #                        else :
    #                            allocated_dict[rkey] = resources_db_handle[0][nodename][rkey]
    #                    else :
    #                        allocated_dict[rkey] = requested_dict[rkey]
    #
    #                print "allocated_dict (%s)" % (allocated_dict,)
    
    
                    sufficient_res = 1
                    #print "requested_dict (%s) allocated_dict (%s) sorted_res_dict (%s) window_resource_dict (%s)" % (requested_dict, allocated_dict, sorted_res_dict, window_resource_dict)
                    for rkey in resources_dict[nodename]['consumable_dict'].keys():
                        #if sorted_res_dict[nodename][rkey] - window_resource_dict[rkey] < allocated_dict[rkey] :
                        if allocated_dict.has_key(rkey) and sorted_res_dict[nodename][rkey] - window_resource_dict[rkey] < allocated_dict[rkey] :
                            sufficient_res = 0
                            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                                print "sufficient_res 0 %s sorted_res_dict[%s] (%s) window_resource_dict (%s) allocated_dict (%s)" % (rkey, nodename, sorted_res_dict[nodename], window_resource_dict, allocated_dict)
                            #print "sufficient_res 0 %s sorted_res_dict[%s] (%s) window_resource_dict (%s) allocated_dict (%s)" % (rkey, nodename, sorted_res_dict[nodename], window_resource_dict, allocated_dict)
                            break
                    #if not consumed_resource_dict.has_key(nodename) and sorted_res_dict[nodename]['node'] - consumed_node >= allocated_node and \
                    #if not consumed_resource_dict.has_key(nodename) and sorted_res_dict[nodename]['node'] - window_resource_dict['node'] >= allocated_dict['node'] and \
                    if not consumed_resource_dict.has_key(nodename) and sorted_res_dict[nodename]['node'] - window_resource_dict['node'] >= allocated_dict['node'] and \
                      sufficient_res == 1:
                        if consumed_resource_dict.has_key(nodename) :
                            # should never get here, as the enclosing if requires
                            # not consumed_resource_dict.has_key(nodename)...
                            consumed_resource_dict[nodename]['node'] = consumed_resource_dict[nodename]['node'] + allocated_dict['node']
                            #consumed_resource_dict[nodename]['cpu'] = consumed_resource_dict[nodename]['cpu'] + allocated_cpu
                            #consumed_resource_dict[nodename]['memory'] = consumed_resource_dict[nodename]['memory'] + allocated_memory
                            for rkey in consumed_resource_dict[nodename].keys():
                                consumed_resource_dict[nodename][rkey] = consumed_resource_dict[nodename][rkey] + allocated_dict[rkey]
                        else :
                            #consumed_resource_dict[nodename] = \
                            #  { 'node' : allocated_node,
                            #    'cpu' : allocated_cpu,
                            #    'memory' : allocated_memory
                            #  }
                            consumed_resource_dict[nodename] = \
                              { 'node' : allocated_dict['node'],}
                            for rkey in allocated_dict.keys():
                                consumed_resource_dict[nodename][rkey] = allocated_dict[rkey]
                        #allocated_dict_list.append(
                        #  {'nodename' : window[2]['nodename'],
                        #   'type' : new_res['node_usage'],
                        #   'initiatormap_index' : counting_dict['initiatormap_index'],
                        #   'node' : allocated_node,
                        #   'cpu' : allocated_cpu,
                        #   'memory' : allocated_memory}
                        #  )
                        #if nodename == LICENSENODE:
                        #    allocated_usage = 'node_shared'
                        #else:
                        #    allocated_usage = new_res['node_usage']
                        allocated_dict_list.append(
                          {'nodename' : window[2]['nodename'],
                           'type' : allocated_usage,
                           'initiatormap_index' : counting_dict['initiatormap_index'],
                           'node' : allocated_dict['node'],})
                        for rkey in allocated_dict.keys():
                            allocated_dict_list[-1][rkey] = allocated_dict[rkey]
                        #print "1. allocated_dict_list (%s)" % (allocated_dict_list,)
                    else :
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            # will also continue here, if consumed_resource_dict.has_key(nodename0, so this enforces one initiator per node rule
                            #print "not enough node (%s) cpu (%s) memory (%s)" % (sorted_res_dict[nodename]['node'] - consumed_node, sorted_res_dict[nodename]['cpu'] - consumed_cpu, sorted_res_dict[nodename]['memory'] - consumed_memory)
                            print "sorted_res_dict (%s)" % sorted_res_dict
                            #print "requested_node (%s) requested_cpu (%s) requested_memory (%s)" % (requested_node, requested_cpu, requested_memory)
                        continue
                else :
                    allocated_usage = 'node_exclusive'
                    sufficient_res = 1
                    for rkey in resources_dict[nodename]['consumable_dict'].keys():
                      #print "checking rkey (%s)" % (rkey,)
                      #if sorted_res_dict[nodename][rkey] - window_resource_dict[rkey] < allocated_dict[rkey] :
                      if sorted_res_dict[nodename][rkey] - window_resource_dict[rkey] < resources_dict[nodename]['consumable_dict'][rkey] :
                          if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                              print "%s setting sufficient_res to 0 for (%s), sorted_res_dict[nodename] (%s) window_resource_dict (%s) allocated_dict (%s)" % (rkey, nodename, sorted_res_dict[nodename], window_resource_dict, allocated_dict)
                          sufficient_res = 0
                          break
                      else:
                          if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                              print "%s still sufficient_res 1 for (%s), sorted_res_dict[nodename] (%s) window_resource_dict (%s) allocated_dict (%s)" % (rkey, nodename, sorted_res_dict[nodename], window_resource_dict, allocated_dict)
                          #print "%s still sufficient_res 1 for (%s), sorted_res_dict[nodename] (%s) window_resource_dict (%s) allocated_dict (%s)" % (rkey, nodename, sorted_res_dict[nodename], window_resource_dict, allocated_dict)
                    #print "sorted_res_dict[nodename] (%s) requested_dict (%s)" % (sorted_res_dict[nodename], requested_dict)
                    #if not consumed_resource_dict.has_key(nodename) and sorted_res_dict[nodename]['node'] - window_resource_dict['node'] >= requested_dict['node'] and \
                    #if counting_dict['node_usage'] == 'node_shared' or (not consumed_resource_dict.has_key(nodename) and sorted_res_dict[nodename]['node'] - window_resource_dict['node'] >= requested_dict['node'] and \
                    #  sufficient_res == 1):
                    if  sufficient_res == 1:
                        #print "sufficient_res == 1"
                    #if not consumed_resource_dict.has_key(nodename) and sorted_res_dict[nodename]['node'] - consumed_node >= requested_node and \
                    #  sorted_res_dict[nodename]['cpu'] - consumed_cpu >= requested_cpu and \
                    #  sorted_res_dict[nodename]['memory'] - consumed_memory >= requested_memory :
                        if counting_dict['node_usage'] == 'node_shared' :
                            final_requested_node = 0
                        else :
                            #final_requested_node = requested_node
                            final_requested_node = requested_dict['node']
                        if consumed_resource_dict.has_key(nodename) :
                            consumed_resource_dict[nodename]['node'] = consumed_resource_dict[nodename]['node'] + final_requested_node
                            #consumed_resource_dict[nodename]['cpu'] = consumed_resource_dict[nodename]['cpu'] + requested_cpu
                            #consumed_resource_dict[nodename]['memory'] = consumed_resource_dict[nodename]['memory'] + requested_memory
                            for rkey in resources_dict[nodename]['consumable_dict'].keys():
                                consumed_resource_dict[nodename][rkey] = consumed_resource_dict[nodename][rkey] + requested_dict[rkey]
                        else :
                            #consumed_resource_dict[nodename] = \
                            #  { 'node' : final_requested_node,
                            #    'cpu' : requested_cpu,
                            #    'memory' : requested_memory
                            #  }
                            consumed_resource_dict[nodename] = \
                              { 'node' : final_requested_node,}
                            for rkey in resources_dict[nodename]['consumable_dict'].keys():
                                consumed_resource_dict[nodename][rkey] = requested_dict[rkey]
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "Nothing is None, nodename (%s) final_requested_node is (%s)" % (nodename, final_requested_node)
                        #allocated_dict_list.append(
                        #  {'nodename' : window[2]['nodename'],
                        #   'type' : new_res['node_usage'],
                        #   'initiatormap_index' : counting_dict['initiatormap_index'],
                        #   'node' : final_requested_node,
                        #   'cpu' : requested_cpu,
                        #   'memory' : requested_memory}
                        #  )
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "appending (%s) to allocated_dict_list" % (
                              {'nodename' : window[2]['nodename'],
                               'type' : new_res['node_usage'],
                               'initiatormap_index' : counting_dict['initiatormap_index'],
                               'node' : final_requested_node,},)
                        #if new_res['purpose_type_string'] == 'standing_reservation':
                        #    print "sdebug: appending (%s) to allocated_dict_list" % (
                        #  {'nodename' : window[2]['nodename'],
                        #   'type' : new_res['node_usage'],
                        #   'initiatormap_index' : counting_dict['initiatormap_index'],
                        #   'node' : final_requested_node,},)
                        #if nodename == LICENSENODE:
                        #    allocated_usage = 'node_shared'
                        #else:
                        #    allocated_usage = new_res['node_usage']
                        allocated_dict_list.append(
                          {'nodename' : window[2]['nodename'],
                           'type' : allocated_usage,
                           'initiatormap_index' : counting_dict['initiatormap_index'],
                           'node' : final_requested_node,})
                        #print "2a. allocated_dict_list (%s)" % (allocated_dict_list,)
                        #print "2a. requested_dict (%s)" % (requested_dict,)
                        #print "2a. resources_dict[nodename] (%s)" % (resources_dict[nodename],)
                        for rkey in resources_dict[nodename]['consumable_dict'].keys():
                            allocated_dict_list[-1][rkey] = requested_dict[rkey]
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            print "allocated_dict_list (%s)" % (allocated_dict_list,)
                        #print "2b. allocated_dict_list (%s)" % (allocated_dict_list,)
                    else :
                        if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                            #print "not enough node cpu memory (%s)" % (sorted_res_dict[nodename]['node'] - consumed_node, sorted_res_dict[nodename]['cpu'] - consumed_cpu, sorted_res_dict[nodename]['memory'] - consumed_memory)
                            print "not enough node cpu memory (%s) (%s)" % (requested_dict, window_resource_dict)
                            #print "sorted_res_dict (%s)" % sorted_res_dict
                            #print "requested_node (%s) requested_cpu (%s) requested_memory (%s)" % (requested_dict,)
                            print "requested_dict (%s)" % (requested_dict,)
                        #print "not enough node cpu memory (%s) (%s)" % (requested_dict, window_resource_dict)
                        #print "requested_dict (%s)" % (requested_dict,)
                        continue
                #print "appending to chosen_nodes_list (%s)" % (window[2]['nodename'],)
                chosen_nodes_list.append(window[2]['nodename'])
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "chosen_nodes_list.append(window[2]['nodename']) (%s)" % chosen_nodes_list
                #if new_res['purpose_type_string'] == 'standing_reservation':
                #    print "sdebug: chosen_nodes_list.append(window[2]['nodename']) (%s)" % chosen_nodes_list
                if counting_dict['amount_int'] == None and not (max_resource_int != None and len(chosen_nodes_list) >= max_resource_int) :
                    # choose _all_ nodes that meet requested_cpu and
                    # requested_memory, under max if present
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "setting need_more chosen to 1"
                    #if new_res['purpose_type_string'] == 'standing_reservation':
                    #    print "sdebug: setting need_more chosen to 1"
                    need_more = 1
                else :
                    need_more = 0
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "need_more == 0, breaking"
                    #print " counting_dict['amount_int'] (%s) max_resource_int (%s) len(chosen_nodes_list) (%s)" % (counting_dict['amount_int'],max_resource_int, len(chosen_nodes_list))
                    #print "need_more == 0, breaking"
                    #if new_res['purpose_type_string'] == 'standing_reservation':
                    #    print "sdebug: need_more == 0, breaking"
                    # breaking out of for window in sorted_windows_list :
                    break
        #for counting_dict in counting_dict_list :
        # after going through all windows in sorted_windows_list,
        # check to see if all counting_dicts are satisfied.
            #print "end of counting_dict_list loop"

        if need_more == 0:
            # break out of node_set loop
            #if new_res['purpose_type_string'] == 'standing_reservation':
            #    print "sdebug: breaking out of node_set loop"
            break

    sorting_allocated_dict_list = []
    for allocated_dict in allocated_dict_list :
        sorting_allocated_dict_list.append((allocated_dict['initiatormap_index'], allocated_dict))
    sorting_allocated_dict_list.sort()
    allocated_dict_list = map(getfirstindex, sorting_allocated_dict_list)
    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
        print "allocated_dict_list (%s)" % (allocated_dict_list,)
        print "chosen_nodes_list (%s)" % (chosen_nodes_list,)
    #if new_res['purpose_type_string'] == 'standing_reservation':
    #    print "sdebug: allocated_dict_list (%s)" % (allocated_dict_list,)
    #    print "sdebug: chosen_nodes_list (%s)" % (chosen_nodes_list,)
    #print "end of get_chosen_node_list"
    return (chosen_nodes_list, allocated_dict_list)

def insert_new_object_with_key(key, new_object, db_handle) :
    dict = db_handle[0]
    if dict.has_key(key) :
        raise KeyExists(key)
    dict[key] = new_object

def insert_new_object(new_object, db_handle) :
    dict = db_handle[0]
    dict[new_object['name']] = new_object

def update_object_attribute(name, value, object, db_handle) :
    dict = db_handle[0]
    temp_object = copy.copy(dict[object['name']])
    temp_object[name] = value
    dict[temp_object['name']] = temp_object

def update_object_attributes(names_values_dict, object, db_handle) :
    dict = db_handle[0]
    temp_object = copy.copy(dict[object['name']])
    for key in names_values_dict.keys() :
        temp_object[key] = names_values_dict[key]
    dict[object['name']] = temp_object

def delete_object(name, db_handle) :
    dict = db_handle[0]
    if dict.has_key(name) :
        del dict[name]
    else :
        print "name (%s) does not exist!" % name
        raise NameNotInDB(name)

def bind_job_to_reservation(job_step_id_string, reservation_name_string, events_db_handle, jobs_db_handle) :
    temp_job_step = get_object(job_step_id_string, jobs_db_handle)
    reservation_binding_list = temp_job_step['reservation_binding']
    if reservation_binding_list == None :
        reservation_binding_list = [reservation_name_string]
    else :
        reservation_binding_list.append(reservation_name_string)
    update_object_attribute('reservation_binding', reservation_binding_list, temp_job_step, jobs_db_handle)

def unbind_job_from_reservation(job_step_id_string, reservation_name_string, events_db_handle, jobs_db_handle) :
    temp_job_step = get_object(job_step_id_string, jobs_db_handle)
    reservation_binding_list = temp_job_step['reservation_binding']
    new_reservation_binding_list = filter(
      lambda reservation_binding_string, reservation_name=reservation_name_string : reservation_binding_string != reservation_name,
      reservation_binding_list
    )
    update_object_attribute('reservation_binding', new_reservation_binding_list, temp_job_step, jobs_db_handle)

def bind_reservation_to_job(reservation_id_string, job_step_id_string, events_db_handle, reservations_db_handle) :
    temp_res_step = get_object(reservation_id_string, reservations_db_handle)
    if not temp_res_step.has_key('job_binding') :
        temp_res_step['job_binding'] = None
    job_binding_list = temp_res_step['job_binding']
    if job_binding_list == None :
        job_binding_list = [job_step_id_string]
    else :
        job_binding_list.append(job_step_id_string)
    job_restriction = "if input_tuple[0]['name'] in " + `job_binding_list` + \
      ' : result = 0'
    print "job_restriction (%s)" % job_restriction
    update_object_attribute('job_binding', job_binding_list, temp_res_step, reservations_db_handle)
    update_object_attribute('job_restriction', job_restriction, temp_res_step, reservations_db_handle)

def unbind_reservation_from_job(reservation_id_string, job_step_id_string, events_db_handle, reservations_db_handle) :
    temp_res_step = get_object(reservation_id_string, reservations_db_handle)
    job_binding_list = temp_res_step['job_binding']
    new_job_binding_list = filter(
      lambda job_binding_string, job_id=job_step_id_string : job_binding_string != job_id,
      job_binding_list
    )
    if len(new_job_binding_list) == 0 :
        job_restriction = "if input_tuple[0]['user'] == '" + temp_res_step['creator_string'] + "' : result = 0"
    else :
        job_restriction = "if input_tuple[0]['name'] in " + `new_job_binding_list` + \
          ' : result = 0'
    print "job_restriction (%s)" % job_restriction
    update_object_attribute('job_binding', new_job_binding_list, temp_res_step, reservations_db_handle)
    update_object_attribute('job_restriction', job_restriction, temp_res_step, reservations_db_handle)

def move_old_jobs(events_db_handle, jobs_db_handle, old_jobs_db_handle) :
    update_job_info(jobs_db_handle)
    jobs_dict = jobs_db_handle[0]
    new_job_steps_dict = Catalina____RESOURCEMANAGER_PLACEHOLDER___.get_job_steps_dict()
    new_job_steps_names_list = new_job_steps_dict.keys()
    job_step_list = get_object_list(jobs_db_handle)
    for job_step in job_step_list :
        if not job_step['name'] in new_job_steps_names_list \
          and not jobs_dict[job_step['name']]['state'] in ['Idle', 'Running', 'Starting'] :
            key = get_new_db_key(old_jobs_db_handle)
            try :
                insert_new_object_with_key(key, job_step, old_jobs_db_handle)
            except KeyExists, exc:
                print "The key (%s) already exists in the db" % exc.key
            except :
                continue
            else :
                try :
                    delete_object(job_step['name'], jobs_db_handle)
                except :
                    continue

def move_old_reservations(events_db_handle, reservations_db_handle, old_reservations_db_handle) :
    reservations_list = get_object_list(reservations_db_handle)
    for reservation in reservations_list :
        if reservation.has_key('epiloguerc') \
          and reservation['epiloguerc'] != 0 :
            continue
        if reservation['end_time_float'] < Now_float :
            key = get_new_db_key(old_reservations_db_handle)
            try :
                insert_new_object_with_key(key, reservation, old_reservations_db_handle)
            except KeyExists, exc:
                print "The key (%s) already exists in the db" % exc.key
            except :
                continue
            else :
                try :
                    delete_object(reservation['name'], reservations_db_handle)
                except :
                    continue

def update_job_info(jobs_db_handle) :
    dict = jobs_db_handle[0]
    new_job_steps_dict = Catalina____RESOURCEMANAGER_PLACEHOLDER___.get_job_steps_dict()
    new_job_ids = new_job_steps_dict.keys()
    #print "len(new_job_ids) (%s)" % (len(new_job_ids),)
    old_job_steps_list = get_object_list(jobs_db_handle)
    old_job_ids = get_object_names_list(old_job_steps_list)
    attribute_list = Catalina____RESOURCEMANAGER_PLACEHOLDER___.JOB_UPDATE_ATTRIBUTE_list
    altered_jobs_list = []
    for new_job_step in new_job_steps_dict.values() :
        if LICENSENODE != None:
            if new_job_step.has_key('license_request_list'):
                # this only handles package for each job...
                # license_request should have format
                # <package name>_<license count>
                # abaqus_16
                if new_job_step.has_key('resource_hosts'):
                    new_job_step['resource_hosts'].append(LICENSENODE)
                else:
                    new_job_step['resource_hosts'] = [LICENSENODE,]
                #print "new_job_step['resource_hosts'] (%s)" % (new_job_step['resource_hosts'],)
                license_request_list = string.split(new_job_step['license_request_list'], '+')
                requested_license_list = []
                for license_request in license_request_list:
                    package_name, license_count = string.split(new_job_step['license_request_list'], '_')
                    requested_license_list.append({package_name : int(license_count)})
                new_job_step['requested_resource_list'].append({'req_list' :
                  requested_license_list, 'type': 'node_shared'})
        job_altered = 0
        if new_job_step['name'] in old_job_ids :
            # Prevent default values in the new job step
            # from overwriting the old info, for certain
            # attributes...should clean this up...
            # Probably should do it the other way: only
            # change attributes that should be updated...
            old_job_step = dict[new_job_step['name']]
            temp_job_step = copy.copy(old_job_step)
            for key in attribute_list :
                if new_job_step.has_key(key) :
                    if DEBUGJOB != None:
                        print "updating (%s) (%s) for (%s)" % (key, new_job_step[key], temp_job_step['name'])
                    temp_job_step[key] = new_job_step[key]
                    if key == 'requested_resource_list' and (not old_job_step.has_key(key) or new_job_step[key] != old_job_step[key]):
                        job_altered = 1
                    if key == 'wall_clock_limit' and (not old_job_step.has_key(key) or new_job_step[key] != old_job_step[key]):
                        job_altered = 1
                    if key == 'reservation_binding' and (not old_job_step.has_key(key) or new_job_step[key] != old_job_step[key]):
                        job_altered = 1
                    if key == 'QOS' and (not old_job_step.has_key(key) or new_job_step[key] != old_job_step[key]):
                        job_altered = 1
                    if key == 'preemptible' and (not old_job_step.has_key(key) or new_job_step[key] != old_job_step[key]):
                        job_altered = 1
                    if key == 'preempting' and (not old_job_step.has_key(key) or new_job_step[key] != old_job_step[key]):
                        job_altered = 1
                    if key == 'run_at_risk_int' and (not old_job_step.has_key(key) or new_job_step[key] != old_job_step[key]):
                        job_altered = 1
                    if key == 'local_admin_priority_string' and (not old_job_step.has_key(key) or new_job_step[key] != old_job_step[key]):
                        job_altered = 1
                    if key == 'local_user_priority_string' and (not old_job_step.has_key(key) or new_job_step[key] != old_job_step[key]):
                        job_altered = 1
                    if job_altered == 1:
                        altered_jobs_list.append(new_job_step['name'])
            temp_job_step['last_seen_time_float'] = Now_float
            update_object_attributes( temp_job_step, old_job_step,
              jobs_db_handle)
        else :
            new_job_step['last_seen_time_float'] = Now_float
            insert_new_object(new_job_step, jobs_db_handle)
    for old_job_step in old_job_steps_list :
        if old_job_step['name'] in new_job_ids \
          and new_job_steps_dict[old_job_step['name']]['state'] == "Hold" :
            if DEBUGJOB != None and old_job_step['name'] == DEBUGJOB :
                print "setting system_queue_time to None for job (%s)" % old_job_step['name']
            update_object_attribute('system_queue_time', None, old_job_step,
              jobs_db_handle)

        if old_job_step['name'] in altered_jobs_list:
            update_object_attribute('system_queue_time', Now_float, old_job_step,
              jobs_db_handle)
            
        if not old_job_step['state'] in ['Completed','Canceled','Removed'] and \
          old_job_step['name'] not in new_job_ids \
          and old_job_step.has_key('last_seen_time_float') \
          and ( LOST_JOB_LIMIT == None \
          or Now_float - old_job_step['last_seen_time_float'] > LOST_JOB_LIMIT ) :
            update_object_attribute('state', 'Unknown', old_job_step,
              jobs_db_handle)
            if LOST_JOB_WARN == 'TRUE' and old_job_step['state'] == 'Running' :
                recipient = MAIL_RECIPIENT
                subject = "LOST_JOB_LIMIT exceeded"
                message = " Running Job (%s) is no longer detected.  State has been changed to Unknown.  " % old_job_step['name'] 
                warn(message, subject, recipient)
                catsyslog(message,'notice')

def get_QOS_priority(QOS) :
    try :
        if string.atoi(QOS) < len(QOS_PRIORITY_dict.keys()) :
            return QOS_PRIORITY_dict[QOS]
        else :
            return 0L
    except :
        return 0L

def get_QOS_target_expansion_factor(QOS) :
    # This returns the target expansion factor for
    # the different QOSs.
    try :
        if string.atoi(QOS) < len(QOS_TARGETXF_dict.keys()) :
            return QOS_TARGETXF_dict[QOS]
        else :
            return None
    except :
        return None

def get_QOS_target_queue_wait_time(QOS) :
    # This function returns a target queue wait time
    # for a given QOS.
    try :
        if string.atoi(QOS) < len(QOS_TARGETQT_dict.keys()) :
            return QOS_TARGETQT_dict[QOS]
        else :
            return None
    except :
        return None

def update_job_priorities (jobs_db_handle) :
    # priority calculation:
    # resource_number * RESOURCE_WEIGHT
    # expansion factor * EXPANSION_FACTOR_WEIGHT
    # system queue time * SYSTEM_QUEUE_TIME_WEIGHT
    # submit time * SUBMIT_TIME_WEIGHT
    # QOS_priority * QOS_PRIORITY_WEIGHT
    # QOS_target_expansion_factor * IGHT
    # QOS_target_queue_wait_time * QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT
    # Adjust these weights to emphasize different elements of the
    # priority calculation
    #RESOURCE_WEIGHT = 1100.0
    #EXPANSION_FACTOR_WEIGHT = 10.0
    #SYSTEM_QUEUE_TIME_WEIGHT = 0.1
    #SUBMIT_TIME_WEIGHT = 0.0
    #QOS_PRIORITY_WEIGHT = 6000.0
    #QOS_TARGET_EXPANSION_FACTOR_WEIGHT = 1
    #QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT = 1
    jobs_shelf = jobs_db_handle[0]
    for key in jobs_shelf.keys() :
        # Set up a dict to store priority calculation terms for later reports
        priority_element_dict = {
          'RESOURCE_WEIGHT'                    : RESOURCE_WEIGHT,
          'LOCAL_ADMIN_WEIGHT'                 : LOCAL_ADMIN_WEIGHT,
          'LOCAL_USER_WEIGHT'                  : LOCAL_USER_WEIGHT,
          'EXPANSION_FACTOR_WEIGHT'            : EXPANSION_FACTOR_WEIGHT,
          'SYSTEM_QUEUE_TIME_WEIGHT'           : SYSTEM_QUEUE_TIME_WEIGHT,
          'SUBMIT_TIME_WEIGHT'                 : SUBMIT_TIME_WEIGHT,
          'WALL_TIME_WEIGHT'                   : WALL_TIME_WEIGHT,
          'QOS_PRIORITY_WEIGHT'                : QOS_PRIORITY_WEIGHT,
          'QOS_TARGET_EXPANSION_FACTOR_WEIGHT' : QOS_TARGET_EXPANSION_FACTOR_WEIGHT,
          'QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT'  : QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT
        }

        temp_job = jobs_shelf[key]

        # Get number of resources from initiatormap
        # Should really object-orient the resources
        # each Resource should have its own methods for:
        # allocate, de-allocate, get_status...
        # This would make it possible to have a Resource.node
        # Resource.cpu, Resource.license, Resource.switch_port, etc...
        resourcemap_list = string.split(jobs_shelf[key]['initiatormap'], '+')
        resource_number = len(resourcemap_list)

        # Get local priority from local_priority_string
        if temp_job.has_key('local_admin_priority_string') and \
          temp_job['local_admin_priority_string'] != None :
            try :
                local_admin_float = float(temp_job['local_admin_priority_string'])
            except :
                local_admin_float = 0.0
        else :
            local_admin_float = 0.0
        if temp_job.has_key('local_user_priority_string') and \
          temp_job['local_user_priority_string'] != None :
            try :
                local_user_float = float(temp_job['local_user_priority_string']) 
                if local_user_float > 0.0 :
                    local_user_float = 0.0
            except :
                local_user_float = 0.0
        else :
            local_user_float = 0.0

        #feature 582
        if temp_job.has_key('fairshare_value') and \
          temp_job['fairshare_value'] != None :
            try :
                fairshare_value_float = float(temp_job['fairshare_value'])
            except :
                fairshare_value_float = 0.0
        else :
            fairshare_value_float = 0.0
  
        # if job has started, then we should calculate
        # expansion_factor queue_wait_time and submit_wait_time
        # differently.
        if temp_job.has_key('wall_clock_used') and temp_job['wall_clock_used'] != None and float(temp_job['wall_clock_used']) > 0.0:
            # this should be good for both Running and Preempted jobs
            start_time_float = Now_float - float(temp_job['wall_clock_used'])
            if temp_job.has_key('system_queue_time') and temp_job['system_queue_time'] != None and float(temp_job['system_queue_time']) > 0.0:
                expansion_factor = ((start_time_float - float(temp_job['system_queue_time'])) + float(temp_job['wall_clock_limit'])) / float(temp_job['wall_clock_limit'])
                queue_wait_time = start_time_float - float(temp_job['system_queue_time'])
            else:
                # if a running job has no system_queue_time, we did not
                # see it in Idle.  Use SubmitTime instead.
                expansion_factor = (start_time_float - float(temp_job['SubmitTime']) + float(temp_job['wall_clock_limit'])) / float(temp_job['wall_clock_limit'])
                queue_wait_time = start_time_float - float(temp_job['SubmitTime'])
            submit_wait_time = start_time_float - float(temp_job['SubmitTime'])
        else:
            # no wall_clock_used, assume still Idle
            # Calculate expansion factor
            if float(temp_job['wall_clock_limit']) <= 0.0:
                expansion_factor = 1.0
            else:
                expansion_factor = \
                  ( Now_float -
                      float(temp_job['speculative_system_queue_time']) + \
                    float(temp_job['wall_clock_limit']) ) / \
                  float(temp_job['wall_clock_limit'])
    
            # Calculate queue wait time
            queue_wait_time = Now_float - float(temp_job['speculative_system_queue_time'])
    
            # Calculate submit wait time
            submit_wait_time = Now_float - float(temp_job['SubmitTime'])
    
        # Get wall clock time
        wall_clock_time = temp_job['wall_clock_limit']

        # Get QOS priority
        try :
            QOS_priority = float(get_QOS_priority(temp_job['QOS']))
        except :
            print "Bad QOS_priority for job (%s)!" % temp_job['name']
            QOS_priority = 0.0

        # Get QOS target expansion factor
        QOS_target_expansion_factor = get_QOS_target_expansion_factor(temp_job['QOS'])
        if QOS_target_expansion_factor == None :
            QOS_target_xf_value = 0
        else :
            if expansion_factor >= QOS_target_expansion_factor :
                QOS_target_xf_value = float(QOS_MAX_PRIORITY_dict[temp_job['QOS']])
            else :
                QOS_target_xf_value = 1 / (QOS_target_expansion_factor - expansion_factor)
            if QOS_target_xf_value > float(QOS_MAX_PRIORITY_dict[temp_job['QOS']]) :
                QOS_target_xf_value = float(QOS_MAX_PRIORITY_dict[temp_job['QOS']])
        # Get QOS target queue time
        QOS_target_queue_wait_time = get_QOS_target_queue_wait_time(temp_job['QOS'])
        if QOS_target_queue_wait_time == None :
            QOS_target_qwt_value = 0
        else :
            if queue_wait_time >= QOS_target_queue_wait_time :
                QOS_target_qwt_value = float(QOS_MAX_PRIORITY_dict[temp_job['QOS']])
            else :
                QOS_target_qwt_value = 1 / (QOS_target_queue_wait_time - queue_wait_time)
            if QOS_target_qwt_value > float(QOS_MAX_PRIORITY_dict[temp_job['QOS']]) :
                QOS_target_qwt_value = float(QOS_MAX_PRIORITY_dict[temp_job['QOS']])

        priority_element_dict['resource_number'] = resource_number
        priority_element_dict['local_admin_float'] = local_admin_float
        priority_element_dict['local_user_float'] = local_user_float
        priority_element_dict['expansion_factor'] = expansion_factor
        priority_element_dict['queue_wait_time'] = queue_wait_time
        priority_element_dict['submit_wait_time'] = submit_wait_time
        priority_element_dict['wall_clock_time'] = wall_clock_time
        priority_element_dict['QOS_priority'] = QOS_priority
        priority_element_dict['QOS_target_xf_value'] = QOS_target_xf_value
        priority_element_dict['QOS_target_qwt_value'] = QOS_target_qwt_value

        if temp_job['system_priority_int'] != None :
            #priority = long(float( MAXPRIORITY * max_places_float)) + \
            #  long(temp_job['system_priority_int'] )
            priority = long(MAXPRIORITY) * long(max_places_float) + \
              long(temp_job['system_priority_int'] )
        else :
            priority = \
                long(resource_number * 100) * RESOURCE_WEIGHT + \
                long(local_admin_float * 100) * LOCAL_ADMIN_WEIGHT + \
                long(local_user_float * 100) * LOCAL_USER_WEIGHT + \
                long(expansion_factor * 100) * EXPANSION_FACTOR_WEIGHT + \
                long(queue_wait_time * 100) * SYSTEM_QUEUE_TIME_WEIGHT + \
                long(submit_wait_time * 100) * SUBMIT_TIME_WEIGHT + \
                long(wall_clock_time * 100) * WALL_TIME_WEIGHT + \
                long(QOS_priority * 100) * QOS_PRIORITY_WEIGHT + \
                long(QOS_target_xf_value * 100) * QOS_TARGET_EXPANSION_FACTOR_WEIGHT + \
                long(QOS_target_qwt_value * 100) * QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT
            
            #feature 582
            #if fairshare_value_float / total available >= 0.6, then penalize FAIRSHARE_BONUS_WEIGHT % for overall priority
            try :
                total_available_cpu = float(TOTAL_AVAILABLE)
            except :
                total_available_cpu = 1.0
                
            try :
                threshold_percentage = float(THRESHOLD_PERCENTAGE)
            except :
                threshold_percentage = 100.0
            
            try :
                penalty_percentage = float(PENALTY_PERCENTAGE)
            except :
                penalty_percentage = 0.0
            
            if total_available_cpu == 1 :
                pass
            elif ((fairshare_value_float / total_available_cpu ) > (threshold_percentage/100)) :
                #penalize
                priority = priority - (penalty_percentage * priority / 100)
            
            if QOS_MAX_PRIORITY_dict.has_key(temp_job['QOS']) :
                max_pri = long(float(QOS_MAX_PRIORITY_dict[temp_job['QOS']]) * max_places_float ) * 100L
            else :
                max_pri = long(MAXPRIORITY * max_places_float) * 100L
            #if priority > max_pri - max_pri * 0.1 :
            #    priority =  (priority * max_pri)/(priority + max_pri * 0.1)
            if priority > max_pri - (max_pri / 10) :
                priority =  (priority * max_pri * 10)/(priority * 10 + max_pri)
        update_object_attribute('priority', priority, temp_job, jobs_db_handle)
        update_object_attribute('priority_element_dict', priority_element_dict, temp_job, jobs_db_handle)

def cancel_risk_reservations(reservations_db_handle,
                             jobs_db_handle,
                             events_db_handle,
                             resources_db_handle) :
#    def nonrunning_risky(this_res, jobs_dict=jobs_dict) :
#        return this_res.has_key('job_runID') and \
#          jobs_dict.has_key(this_res['job_runID']) and \
#          this_res['purpose_type_string'] != 'running' and \
#          ( not jobs_dict[this_res['job_runID']].has_key('start_count_int') or \
#          jobs_dict[this_res['job_runID']]['start_count_int'] < 1 ) and \
#          jobs_dict[this_res['job_runID']].has_key('run_at_risk_int') and \
#          jobs_dict[this_res['job_runID']]['run_at_risk_int'] >= 1

    jobs_dict = jobs_db_handle[0]
    def risky(this_res, jobs_dict=jobs_dict) :
        return this_res.has_key('job_runID') and \
          jobs_dict.has_key(this_res['job_runID']) and \
          ((jobs_dict[this_res['job_runID']].has_key('run_at_risk_int') and \
          jobs_dict[this_res['job_runID']]['run_at_risk_int'] >= 1) or \
          (this_res['purpose_type_string'] in ['running',] and jobs_dict[this_res['job_runID']].has_key('preemptible') and \
          jobs_dict[this_res['job_runID']]['preemptible'] >= 1))
    def approaching_nonrisky(this_res, jobs_dict=jobs_dict) :
        return not ( this_res.has_key('job_runID') and \
          jobs_dict.has_key(this_res['job_runID']) and \
          ((jobs_dict[this_res['job_runID']].has_key('run_at_risk_int') and \
          jobs_dict[this_res['job_runID']]['run_at_risk_int'] >= 1 ) or \
          (this_res['purpose_type_string'] in ['running',] and jobs_dict[this_res['job_runID']].has_key('preemptible') and \
          jobs_dict[this_res['job_runID']]['preemptible'] >= 1))
          )
    def get_priority_tuple(x, jobs_dict=jobs_dict) :
        return (jobs_dict[x['job_runID']]['priority'],x)
    def get_reservation_from_tuple(x) :
        return x[1]
          
    resources_dict = resources_db_handle[0]
    reservations_dict = reservations_db_handle[0]
    job_steps_list = get_object_list(jobs_db_handle)
    reservations_list = get_object_list(reservations_db_handle)
    risk_reservations = filter(risky, reservations_list)
    priority_risk_reservations = map(get_priority_tuple, risk_reservations)
    priority_risk_reservations.sort()
    priority_risk_reservations.reverse()
    # take the risk reservations highest priority first
    risk_reservations = map(get_reservation_from_tuple, priority_risk_reservations)
    nonrisk_reservations = filter(approaching_nonrisky, reservations_list)
    retained_risk_reservations = []
    for risk_res in risk_reservations :
        atrisk_res_dict = {}
        # add up cpu and memory by node for all at_risk reservations
        for node in risk_res['node_list'] :
            if risk_res.has_key('allocated_dict_list') :
                for allocated_dict in risk_res['allocated_dict_list'] :
                    if allocated_dict.has_key('type') and allocated_dict['type'] == 'node_shared' :
                        #if atrisk_res_dict.has_key(node) :
                        #    atrisk_res_dict[node]['cpu'] = atrisk_res_dict[node]['cpu'] + allocated_dict['cpu']
                        #    atrisk_res_dict[node]['memory'] = atrisk_res_dict[node]['memory'] + allocated_dict['memory']
                        #else :
                        #    atrisk_res_dict[node] = { 'cpu' : allocated_dict['cpu'], 'memory' : allocated_dict['memory'] }
                        if not atrisk_res_dict.has_key(node) :
                            atrisk_res_dict[node] = {}
                        for conskey in allocated_dict.keys():
                            if conskey == ['nodename','type']:
                                continue
                            if atrisk_res_dict[node].has_key(conskey):
                                atrisk_res_dict[node][conskey] = atrisk_res_dict[node][conskey] + allocated_dict[conskey]
                            else:
                                atrisk_res_dict[node][conskey] = allocated_dict[conskey]
                    else :
                        #if atrisk_res_dict.has_key(node) :
                        #    atrisk_res_dict[node]['cpu'] = atrisk_res_dict[node]['cpu'] + resources_dict[node]['ConsumableCpus']
                        #    atrisk_res_dict[node]['memory'] = atrisk_res_dict[node]['memory'] + resources_dict[node]['ConsumableMemory']
                        #else :
                        #    atrisk_res_dict[node] = { 'cpu' : resources_dict[node]['ConsumableCpus'], 'memory' : resources_dict[node]['ConsumableMemory'] }
                        if not atrisk_res_dict.has_key(node) :
                            atrisk_res_dict[node] = {}
                        for conskey in resources_dict[node]['consumable_dict'].keys():
                            if atrisk_res_dict[node].has_key(conskey):
                                atrisk_res_dict[node][conskey] = atrisk_res_dict[node][conskey] + resources_dict[node]['consumable_dict'][conskey]
                            else:
                                atrisk_res_dict[node][conskey] = resources_dict[node]['consumable_dict'][conskey]
                        
        # add up cpu and memory by node for nonrisky reservations that
        # conflict
        nonrisk_res_dict = {}
        for nonrisk_res in nonrisk_reservations + retained_risk_reservations :
            # skip if nonrisk priority lower than risk priority
            #if not nonrisk_res['purpose_type_string'] in ['running','job']:
            #    continue
            #if not nonrisk_res.has_key( in ['running','job']:
            #print "nonrisk_res['job_runID'] (%s)" % (nonrisk_res['job_runID'],)
            #print "risk_res['job_runID'] (%s)" % (risk_res['job_runID'],)
            if nonrisk_res.has_key('job_runID')  \
              and nonrisk_res['job_runID'] != None \
              and risk_res.has_key('job_runID') \
              and risk_res['job_runID'] != None \
              and jobs_db_handle[0][nonrisk_res['job_runID']]['priority'] < \
              jobs_db_handle[0][risk_res['job_runID']]['priority'] \
              :
                continue
            # skip if nonrisk is not preempting
            #if nonrisk_res.has_key('job_runID')  \
            #  and nonrisk_res.has_key('job_runID') \
            #  and risk_res.has_key('job_runID') \
            #  and jobs_db_handle[0][nonrisk_res['job_runID']]['preempting'] <= 0 \
            #  :
            #    print "skipping %s" % (jobs_db_handle[0][nonrisk_res['job_runID']],)
            #    continue
            #else:
            #    print "doing %s" % (jobs_db_handle[0][nonrisk_res['job_runID']],)
            
            if nonrisk_res['start_time_float'] <= \
              Now_float + RUN_AT_RISK_CLEANUP_TIME and \
              ( nonrisk_res['start_time_float'] < risk_res['end_time_float'] <= nonrisk_res['end_time_float'] or \
              nonrisk_res['start_time_float'] <= risk_res['start_time_float'] < nonrisk_res['end_time_float'] or \
              (risk_res['start_time_float'] <= nonrisk_res['start_time_float'] and nonrisk_res['end_time_float'] <= risk_res['end_time_float'])) :

                # adding resources for nonrisk_res
                input_tuple = ( jobs_dict[risk_res['job_runID']], )
                result = apply_policy_code(
                  nonrisk_res['job_restriction'],
                  input_tuple)
                if result == 0 :
                    continue
                if nonrisk_res.has_key('allocated_dict_list') :
                    nonrisk_allocated_dict_list = nonrisk_res['allocated_dict_list']
                else :
                    nonrisk_allocated_dict_list = []
                    for nonrisknode in nonrisk_res['node_list'] :
                        #nonrisk_allocated_dict_list.append(
                        #  { 'nodename' : nonrisknode,
                        #    'type' : 'node_exclusive',
                        #    'cpu' : resources_dict[nonrisknode]['ConsumableCpus'],
                        #    'memory' : resources_dict[nonrisknode]['ConsumableMemory']
                        #  })
                        nonrisk_allocated_dict = \
                          { 'nodename' : nonrisknode,
                            'type' : 'node_exclusive',
                          }
                        for conskey in resources_dict[nonrisknode]['consumable_dict'].keys():
                            nonrisk_allocated_dict[conskey] = resources_dict[nonrisknode]['consumable_dict'][conskey]
                        nonrisk_allocated_dict_list.append(nonrisk_allocated_dict)
                for nonrisk_allocated_dict in nonrisk_allocated_dict_list :
                    nonrisknode = nonrisk_allocated_dict['nodename']
                    #if atrisk_res_dict.has_key(nonrisknode) :
                    #    if nonrisk_res_dict.has_key(nonrisknode) :
                    #        nonrisk_res_dict[nonrisknode]['cpu'] = nonrisk_res_dict[nonrisknode]['cpu'] + nonrisk_allocated_dict['cpu']
                    #        nonrisk_res_dict[nonrisknode]['memory'] = nonrisk_res_dict[nonrisknode]['memory'] + nonrisk_allocated_dict['memory']
                    #    else :
                    #        nonrisk_res_dict[nonrisknode] = { 'cpu' : nonrisk_allocated_dict['cpu'], 'memory' : nonrisk_allocated_dict['memory'] }
                    #else :
                    #    if nonrisk_res_dict.has_key(nonrisknode) :
                    #        nonrisk_res_dict[nonrisknode]['cpu'] = nonrisk_res_dict[nonrisknode]['cpu'] + resources_dict[nonrisknode]['ConsumableCpus']
                    #        nonrisk_res_dict[nonrisknode]['memory'] = nonrisk_res_dict[nonrisknode]['memory'] + resources_dict[nonrisknode]['ConsumableMemory']
                    #    else :
                    #        nonrisk_res_dict[nonrisknode] = { 'cpu' : resources_dict[nonrisknode]['ConsumableCpus'], 'memory' : resources_dict[nonrisknode]['ConsumableMemory'] }

                    if atrisk_res_dict.has_key(nonrisknode):
                        #if nonrisk_res_dict.has_key(nonrisknode):
                        #    for conskey in nonrisk_allocated_dict.keys():
                        #        if conskey == ['nodename','type']:
                        #            continue
                        #        nonrisk_res_dict[nonrisknode][conskey] = nonrisk_res_dict[nonrisknode][conskey] + nonrisk_allocated_dict[conskey]
                        #else :
                        #    for conskey in nonrisk_allocated_dict.keys():
                        #        if conskey == ['nodename','type']:
                        #            continue
                        #        nonrisk_res_dict[nonrisknode][conskey] = nonrisk_allocated_dict[conskey]
                        if not nonrisk_res_dict.has_key(nonrisknode):
                            nonrisk_res_dict[nonrisknode] = {}
                        for conskey in nonrisk_allocated_dict.keys():
                            if conskey == ['nodename','type']:
                                continue
                            if nonrisk_res_dict[nonrisknode].has_key(conskey):
                                nonrisk_res_dict[nonrisknode][conskey] = nonrisk_res_dict[nonrisknode][conskey] + nonrisk_allocated_dict[conskey]
                            else:
                                nonrisk_res_dict[nonrisknode][conskey] = nonrisk_allocated_dict[conskey]
                    else:
                        if not nonrisk_res_dict.has_key(nonrisknode):
                            nonrisk_res_dict[nonrisknode] = {}
                        for conskey in resources_dict[nonrisknode]['consumable_dict'].keys():
                            if nonrisk_res_dict[nonrisknode].has_key(conskey):
                                nonrisk_res_dict[nonrisknode][conskey] = nonrisk_res_dict[nonrisknode][conskey] + resources_dict[nonrisknode]['consumable_dict'][conskey]
                            else:
                                nonrisk_res_dict[nonrisknode][conskey] = resources_dict[nonrisknode]['consumable_dict'][conskey]
                     
        found_conflict = 0
        for node in nonrisk_res_dict.keys() :
            #checking nonrisk node
            if atrisk_res_dict.has_key(node) and resources_dict.has_key(node) :
                #if DEBUGJOB != None and risk_res['job_runID'] != None and risk_res['job_runID'] == DEBUGJOB :
                #    print "cpu conflict check %s %s %s" % (nonrisk_res_dict[node]['cpu'], atrisk_res_dict[node]['cpu'], resources_dict[node]['ConsumableCpus'])
                #    print "memory conflict check %s %s %s" % (nonrisk_res_dict[node]['memory'], atrisk_res_dict[node]['memory'], resources_dict[node]['ConsumableMemory'])
                #if nonrisk_res_dict[node]['cpu'] + atrisk_res_dict[node]['cpu'] > resources_dict[node]['ConsumableCpus'] :
                for conskey in resources_dict[node]['consumable_dict'].keys():
                    if nonrisk_res_dict[node].has_key(conskey):
                        nramount = nonrisk_res_dict[node][conskey]
                    else:
                        nramount = 0
                    if atrisk_res_dict[node].has_key(conskey):
                        aramount = atrisk_res_dict[node][conskey]
                    else:
                        aramount = 0
                    if nramount + aramount > resources_dict[node][conskey] :
                        #if DEBUGJOB != None and risk_res['job_runID'] != None and risk_res['job_runID'] == DEBUGJOB :
                        #    print "found cpu conflict %s %s %s" % (nonrisk_res_dict[node]['cpu'], atrisk_res_dict[node]['cpu'], resources_dict[node]['ConsumableCpus'])
                        found_conflict = 1
                #if nonrisk_res_dict[node]['memory'] + atrisk_res_dict[node]['memory'] > resources_dict[node]['ConsumableMemory'] :
                #    if DEBUGJOB != None and risk_res['job_runID'] != None and risk_res['job_runID'] == DEBUGJOB :
                #        print "found memory conflict %s %s %s" % (nonrisk_res_dict[node]['memory'], atrisk_res_dict[node]['memory'], resources_dict[node]['ConsumableMemory'])
                #    found_conflict = 1
            if found_conflict == 0 :
                if DEBUGJOB != None and risk_res['job_runID'] != None and risk_res['job_runID'] == DEBUGJOB :
                    print "not deleting (%s) (%s) (%s)" % (risk_res['name'], risk_res['purpose_type_string'], risk_res['job_runID'])
                if not risk_res in retained_risk_reservations:
                    retained_risk_reservations.append(risk_res)
            else :
                if DEBUGJOB != None and risk_res['job_runID'] != None and risk_res['job_runID'] == DEBUGJOB :
                    print "deleting (%s) (%s) (%s)" % (risk_res['name'], risk_res['purpose_type_string'], risk_res['job_runID'])
                # for preempted_job reservations, need to push these back,
                # instead of deleting.  maybe pushback is getting deleted?
                if reservations_db_handle[0].has_key(risk_res['name']) :
                    delete_object(risk_res['name'], reservations_db_handle)
                else:
                    if DEBUGJOB != None and risk_res['job_runID'] != None and risk_res['job_runID'] == DEBUGJOB :
                        print "could not find (%s) in reservations_db" % risk_res['name']
                    pass
                if risk_res['purpose_type_string'] == 'running' :
                    if SERVERMODE == 'NORMAL' :
                        if DEBUGJOB != None and risk_res['job_runID'] != None and risk_res['job_runID'] == DEBUGJOB :
                            print "deleting risk_res['name'] (%s)" % (risk_res['name'],)
                        try :
                            if jobs_dict[risk_res['job_runID']]['preemptible'] == 0 :
                                Catalina____RESOURCEMANAGER_PLACEHOLDER___.cancel_job(jobs_dict[risk_res['job_runID']], events_db_handle)
                            else :
                                Catalina____RESOURCEMANAGER_PLACEHOLDER___.preempt_job(jobs_dict[risk_res['job_runID']], events_db_handle)
                        except CancelJobFailure, exc:
                            print "cancel of %s failed" % \
                              exc.failed_job_step['name']
                        except :
                            continue
                    else :
                        if jobs_dict[risk_res['job_runID']]['preemptible'] == 0 :
                            print "%s should be canceled!" % jobs_dict[risk_res['job_runID']]['name']
                        else :
                            print "%s should be preempted!" % jobs_dict[risk_res['job_runID']]['name']

def cancel_overrun_jobs(events_db_handle, jobs_db_handle) :
    job_steps_list = get_object_list(jobs_db_handle)
    for job_step in job_steps_list :
        Catalina_job_step_state = job_step['state']
        if Catalina_job_step_state == 'Starting' or \
          Catalina_job_step_state == 'Running' :
            # Check to see if job has exceeded its duration + overrun
            duration = job_step['wall_clock_limit']
            # If job has 'wall_clock_used', use that, otherwise
            # calculate it from Now_float - Dispatch_Time
            if job_step.has_key('wall_clock_used') and job_step['wall_clock_used'] != None :
                wall_clock_used = job_step['wall_clock_used']
            else :
                start_time = job_step['Dispatch_Time']
                wall_clock_used = Now_float - start_time
            if wall_clock_used > duration + MAXJOBOVERRUN :
                # Cancel the job
                if SERVERMODE == 'NORMAL' :
                    try :
                    # Martin W. Margo 11/30/2005 10:58 AM
                    # I will give cancel_job more information, namely the reason
                    # why I want to cancel the job.
                        if NORMALMODECANCELJOB != None and NORMALMODECANCELJOB == 'yes' :
                            Catalina____RESOURCEMANAGER_PLACEHOLDER___.cancel_job(job_step, events_db_handle, \
                              "Reason: Job killed because it exceeds wallclock time." )
                        else :
                            event = {
                              'name' : 'cancel_job',
                              'cmd' : "llcancel %s not actually run, NORMALMODECANCELJOB != 'yes'" % job_step['name'],
                              'return_string' : ''
                              }
                            log_event(event, events_db_handle)
                            
                    except CancelJobFailure, exc:
                        print "cancel of %s failed" % \
                          exc.failed_job_step['name']
                    except :
                        continue
                else :
                    print "%s should be canceled!" % job_step['name']

def reload_job_resource_lists(events_db_handle, jobs_db_handle, resources_db_handle) :
    """ reload_job_resource_lists()
    Refreshes every job resource list. resource list contains nodes/hosts that are
    eligible to run this job. In most cases, they stay still but sometime a node
    went out fo the queue or join the queue"""
    jobs_dict = jobs_db_handle[0]
    job_step_list = jobs_dict.values()
    # reloading job resource lists
    for job_step in job_step_list :
        job_step_resource_dict_list = Catalina____RESOURCEMANAGER_PLACEHOLDER___.get_resource_dict_list(job_step, resources_db_handle)
        update_object_attribute('resource_dict_list', job_step_resource_dict_list, job_step, jobs_db_handle)

def update_job_resource_lists(jobs_db_handle, resources_db_handle) :
    """ for speed, this only gets the resource list for new jobs...
     there is the potential for incorrect scheduling when a node
     comes up.  reload_job_resource_lists should be run periodically..."""
    jobs_dict = jobs_db_handle[0]
    for key in jobs_dict.keys() :
        #Martin Margo was here 11/6/2006
        #this code can speeds up catalina tremendously
        #only update object attribute those jobs who doesn't have a resource_dict_list
        #alraeady
        
        
        #condition 1
        if jobs_dict[key].has_key('resource_dict_list') and len(jobs_dict[key]['resource_dict_list']) > 0 :
            #print "has resource_dict_list (%s), so continuing" % (jobs_dict[key]['resource_dict_list'],)
            continue
        
        #condition 2
        # For some speed, skip any jobs that were previously found to
        # have 0 resource list.  These should get checked again in
        # reload_job_resource_lists
        if jobs_dict[key]['ineligible_reason'] == 'BADRESOURCELIST' :
            #print "has ineligible_reason BADRESOURCE_LIST, so continuing"
            continue
        
        # if this job is not discovered before and has good resource list
        #print "updating job_resource_lists for (%s)" % key
        job_step_resource_dict_list = Catalina____RESOURCEMANAGER_PLACEHOLDER___.get_resource_dict_list(jobs_dict[key], resources_db_handle)
        update_object_attribute('resource_dict_list', job_step_resource_dict_list, jobs_dict[key], jobs_db_handle)

def update_job_speculative_system_queue_time(jobs_db_handle) :
    job_step_list = get_object_list(jobs_db_handle)
    for job_step in job_step_list :
        if job_step['system_queue_time'] == None :
            update_object_attribute('speculative_system_queue_time', Now_float, job_step, jobs_db_handle)
        else :
            update_object_attribute('speculative_system_queue_time', job_step['system_queue_time'], job_step, jobs_db_handle)

def sort_by_key (list, key) :
    def get_keytuple (x, key=key) :
        if x.has_key(key) :
            return (x[key], x)
        else :
            return (0, x)
    keylist = map(get_keytuple, list)
    keylist.sort()
    keylist.reverse()
    return map(lambda (value, x): x, keylist)

def get_eligible_and_running_jobs(jobs_db_handle, resources_db_handle, reservations_db_handle) :
    def get_filtered_jobs(input_tuple) :
        temp_jobs_list = input_tuple[0]
        temp_resources_list = input_tuple[1]
        temp_reservations_list = input_tuple[2]
        message_string = ''
        running_jobs_per_user = {}
        queued_jobs_per_user = {}
        running_jobs_per_account_per_qos = {}
        queued_jobs_per_account_per_qos = {}
        running_jobs_per_user_per_qos = {}
        queued_jobs_per_user_per_qos = {}
        running_node_sec_per_user_per_qos = {}
        queued_node_sec_per_user_per_qos = {}
        running_node_sec_per_account_per_qos = {}
        queued_node_sec_per_account_per_qos = {}
        idle_jobs = []
        runningstarting_jobs = []
        other_jobs = []
        eligible_jobs = []
        ineligible_jobs = []

        for job in temp_jobs_list :
            state = job['state']
            user = job['user']
            account = job['account']
            qos = job['QOS']
            if state == 'Idle' :
                idle_jobs.append(job)
            elif state == 'Running' or state == 'Starting' or state == 'Preempted' :
                if state == 'Preempted' :
                    pass
                runningstarting_jobs.append(job)
                if running_jobs_per_user.has_key(user) :
                    running_jobs_per_user[user] = \
                      running_jobs_per_user[user] + 1
                else :
                    running_jobs_per_user[user] = 1
                if running_jobs_per_account_per_qos.has_key(account) :
                    if running_jobs_per_account_per_qos[account].has_key(qos) :
                        running_jobs_per_account_per_qos[account][qos] = \
                          running_jobs_per_account_per_qos[account][qos] + 1
                    else :
                        running_jobs_per_account_per_qos[account][qos] = 1
                else :
                    running_jobs_per_account_per_qos[account] = { qos : 1, }
                if running_jobs_per_user_per_qos.has_key(user) :
                    if running_jobs_per_user_per_qos[user].has_key(qos) :
                        running_jobs_per_user_per_qos[user][qos] = \
                          running_jobs_per_user_per_qos[user][qos] + 1
                    else :
                        running_jobs_per_user_per_qos[user][qos] = 1
                else :
                    running_jobs_per_user_per_qos[user] = { qos : 1, }
                if running_node_sec_per_account_per_qos.has_key(account) :
                    if running_node_sec_per_account_per_qos[account].has_key(qos) :
                        running_node_sec_per_account_per_qos[account][qos] = \
                          running_node_sec_per_account_per_qos[account][qos] + job['resource_amount_int'] * job['wall_clock_limit']
                    else :
                        running_node_sec_per_account_per_qos[account][qos] = job['resource_amount_int'] * job['wall_clock_limit']
                else :
                    running_node_sec_per_account_per_qos[account] = { qos : job['resource_amount_int'] * job['wall_clock_limit'], }
            else :
                 other_jobs.append(job)

        if FIFOSCREEN == 'ON' :
            idle_jobs = sort_by_key(idle_jobs, 'SubmitTime')
            idle_jobs.reverse()
        else :
            idle_jobs = sort_by_key(idle_jobs, 'priority')
        for job in idle_jobs :
            user = job['user']
            account = job['account']
            qos = job['QOS']
            if BADRESOURCELIST == 'ON' :
                if not job.has_key('resource_dict_list') :
                    if DEBUGJOB != None and job['job_runID'] != None and job['job_runID'] == DEBUGJOB :
                        print "no resource_dict_list"
                    job['ineligible_reason'] = 'BADRESOURCELIST'
                    ineligible_jobs.append(job)
                    continue
                positive_resource_list = 0
                for resource_dict in job['resource_dict_list'] :
                    if len(resource_dict['resource_dict'].keys()) > 0 :
                        positive_resource_list = 1
                        break
                if positive_resource_list == 0 :
                    if DEBUGJOB != None and job['name'] != None and job['name'] == DEBUGJOB :
                        print "positive_resource_list == 0"
                    job['ineligible_reason'] = 'BADRESOURCELIST'
                    ineligible_jobs.append(job)
                    continue
                #if len(job['requested_resource_list']) > job['resource_dict_list'][0]['amount_int'] :
                #if len(job['resource_dict_list'][0]['resource_dict'].keys()) < job['resource_dict_list'][0]['amount_int'] :
                #    if DEBUGJOB != None and job['name'] != None and job['name'] == DEBUGJOB :
                #        print "too few keys in resource_dict_list"
                #        print "job['resource_dict_list'][0]['resource_dict'] (%s) job['resource_dict_list'][0]['amount_int'] (%s)" % (job['resource_dict_list'][0]['resource_dict'],job['resource_dict_list'][0]['amount_int'])
                #    job['ineligible_reason'] = 'BADRESOURCELIST'
                #    ineligible_jobs.append(job)
                #    continue
            if MAXJOBPERUSERPOLICY != None and MAXJOBPERUSERPOLICY == 'ON' :
                if running_jobs_per_user.has_key(user) :
                    if running_jobs_per_user[user] >= \
                      MAXJOBPERUSERCOUNT :
                        if DEBUGJOB != None and job['name'] != None and job['name'] == DEBUGJOB :
                            print "MAXJOBPERUSERPOLICY (%s) MAXJOBPERUSERCOUNT (%s) running_jobs_per_user[user] (%s)" % (MAXJOBPERUSERPOLICY, MAXJOBPERUSERCOUNT, running_jobs_per_user[user])
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'MAXJOBPERUSERPOLICY'
                        ineligible_jobs.append(job)
                        continue
            if QOS_MAXJOBPERUSERPOLICY_dict[job['QOS']] != None :
                if running_jobs_per_user.has_key(user) :
                    if running_jobs_per_user[user] >= \
                      QOS_MAXJOBPERUSERPOLICY_dict[job['QOS']] :
                        if DEBUGJOB != None and job['name'] != None and job['name'] == DEBUGJOB :
                            print "QOS_MAXJOBPERUSERPOLICY_dict[job['QOS']] (%s) running_jobs_per_user[user] (%s)" % (QOS_MAXJOBPERUSERPOLICY_dict[job['QOS']], running_jobs_per_user[user])
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'QOSMAXJOBPERUSERPOLICY'
                        ineligible_jobs.append(job)
                        continue
            if QOS_MAXJOBPERACCOUNTPOLICY_dict.has_key(job['QOS']) and QOS_MAXJOBPERACCOUNTPOLICY_dict[job['QOS']] != None :
                if running_jobs_per_account_per_qos.has_key(account) :
                    if running_jobs_per_account_per_qos[account].has_key(job['QOS']) :
                        if running_jobs_per_account_per_qos[account][qos] >= \
                          QOS_MAXJOBPERACCOUNTPOLICY_dict[job['QOS']] :
                            job['system_queue_time'] = None
                            job['ineligible_reason'] = 'MAXJOBPERACCOUNTPOLICY'
                            ineligible_jobs.append(job)
                            continue
            if QOS_MAXJOBPERUSERPERQOSPOLICY_dict.has_key(job['QOS']) and QOS_MAXJOBPERUSERPERQOSPOLICY_dict[job['QOS']] != None :
                if running_jobs_per_user_per_qos.has_key(user) :
                    if running_jobs_per_user_per_qos[user].has_key(job['QOS']) :
                        if running_jobs_per_user_per_qos[user][qos] >= \
                          QOS_MAXJOBPERUSERPERQOSPOLICY_dict[job['QOS']] :
                            job['system_queue_time'] = None
                            job['ineligible_reason'] = 'MAXJOBPERPERUSERPERQOSPOLICY'
                            ineligible_jobs.append(job)
                            continue
            if MAXJOBQUEUEDPERUSERPOLICY != None and MAXJOBQUEUEDPERUSERPOLICY == 'ON' :
                if queued_jobs_per_user.has_key(user) :
                    if queued_jobs_per_user[user] >= \
                      MAXJOBQUEUEDPERUSERCOUNT :
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'MAXJOBQUEUEDPERUSERPOLICY'
                        ineligible_jobs.append(job)
                        continue
            if QOS_MAXJOBQUEUEDPERUSERPOLICY_dict[job['QOS']] != None :
                if queued_jobs_per_user.has_key(user) :
                    if queued_jobs_per_user[user] >= \
                      QOS_MAXJOBQUEUEDPERUSERPOLICY_dict[job['QOS']] :
                        job['system_queue_time'] = None
                        job['ineligible_reason'] = 'QOSMAXJOBQUEUEDPERUSERPOLICY'
                        ineligible_jobs.append(job)
                        continue
            if QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict.has_key(job['QOS']) and QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict[job['QOS']] != None :
                if queued_jobs_per_account_per_qos.has_key(account) :
                    if queued_jobs_per_account_per_qos[account].has_key(job['QOS']) :
                        if queued_jobs_per_account_per_qos[account].has_key(qos) :
                            if queued_jobs_per_account_per_qos[account][qos] >= \
                              QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_dict[job['QOS']] :
                                job['system_queue_time'] = None
                                job['ineligible_reason'] = 'QOSMAXJOBQUEUEDPERACCOUNTPOLICY'
                                ineligible_jobs.append(job)
                                continue
            if QOS_MAXJOBQUEUEDPERUSERPERQOSPOLICY_dict.has_key(job['QOS']) and QOS_MAXJOBQUEUEDPERUSERPERQOSPOLICY_dict[job['QOS']] != None :
                if queued_jobs_per_user_per_qos.has_key(user) :
                    if queued_jobs_per_user_per_qos[user].has_key(job['QOS']) :
                        if queued_jobs_per_user_per_qos[user].has_key(qos) :
                            if queued_jobs_per_user_per_qos[user][qos] >= \
                              QOS_MAXJOBQUEUEDPERUSERPERQOSPOLICY_dict[job['QOS']] :
                                job['system_queue_time'] = None
                                job['ineligible_reason'] = 'QOSMAXJOBQUEUEDPERUSERPERQOSPOLICY'
                                ineligible_jobs.append(job)
                                continue
            if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict') and \
              QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict.has_key(job['QOS']) and \
              QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job['QOS']] != None :
                if running_node_sec_per_user_per_qos.has_key(user) :
                    if running_node_sec_per_user_per_qos[user].has_key(job['QOS']) :
                        if running_node_sec_per_user_per_qos[user][qos] >= \
                          QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job['QOS']] :
                            job['system_queue_time'] = None
                            job['ineligible_reason'] = 'QOSMAXNODESECRUNNINGPERUSERPOLICY'
                            ineligible_jobs.append(job)
                            continue
            if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict') and \
              QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict.has_key(job['QOS']) and \
              QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict[job['QOS']] != None :
                if queued_node_sec_per_user_per_qos.has_key(user) :
                    if queued_node_sec_per_user_per_qos[user].has_key(job['QOS']) :
                        if queued_node_sec_per_user_per_qos[user].has_key(qos) :
                            if queued_node_sec_per_user_per_qos[user][qos] >= \
                              QOS_MAXNODESECQUEUEDPERUSERPOLICY_dict[job['QOS']] :
                                job['system_queue_time'] = None
                                job['ineligible_reason'] = 'QOSMAXNODESECQUEUEDPERUSERPOLICY'
                                ineligible_jobs.append(job)
                                continue
            if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict') and \
              QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict.has_key(job['QOS']) and \
              QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job['QOS']] != None :
                if running_node_sec_per_account_per_qos.has_key(account) :
                    if running_node_sec_per_account_per_qos[account].has_key(job['QOS']) :
                        if running_node_sec_per_account_per_qos[account][qos] >= \
                          QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job['QOS']] :
                            job['system_queue_time'] = None
                            job['ineligible_reason'] = 'QOSMAXNODESECRUNNINGPERACCOUNTPOLICY'
                            ineligible_jobs.append(job)
                            continue
            if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict') and \
              QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict.has_key(job['QOS']) and \
              QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict[job['QOS']] != None :
                if queued_node_sec_per_account_per_qos.has_key(account) :
                    if queued_node_sec_per_account_per_qos[account].has_key(job['QOS']) :
                        if queued_node_sec_per_account_per_qos[account].has_key(qos) :
                            if queued_node_sec_per_account_per_qos[account][qos] >= \
                              QOS_MAXNODESECQUEUEDPERACCOUNTPOLICY_dict[job['QOS']] :
                                job['system_queue_time'] = None
                                job['ineligible_reason'] = 'QOSMAXNODESECQUEUEDPERACCOUNTPOLICY'
                                ineligible_jobs.append(job)
                                continue
            job['ineligible_reason'] = 'Eligible'
            job['system_queue_time'] = job['speculative_system_queue_time']
            eligible_jobs.append(job)
            if queued_jobs_per_user.has_key(user) :
                queued_jobs_per_user[user] = queued_jobs_per_user[user] + 1
            else :
                queued_jobs_per_user[user] = 1
            if queued_jobs_per_account_per_qos.has_key(account) :
                if queued_jobs_per_account_per_qos[account].has_key(job['QOS']) :
                    queued_jobs_per_account_per_qos[account][qos] = \
                      queued_jobs_per_account_per_qos[account][qos] + 1
                else :
                    queued_jobs_per_account_per_qos[account][qos] = 1
            else :
                queued_jobs_per_account_per_qos[account] = { qos : 1, }
            if queued_jobs_per_user_per_qos.has_key(user) :
                if queued_jobs_per_user_per_qos[user].has_key(job['QOS']) :
                    queued_jobs_per_user_per_qos[user][qos] = \
                      queued_jobs_per_user_per_qos[user][qos] + 1
                else :
                    queued_jobs_per_user_per_qos[user][qos] = 1
            else :
                queued_jobs_per_user_per_qos[user] = { qos : 1, }
            if queued_node_sec_per_account_per_qos.has_key(account) :
                if queued_node_sec_per_account_per_qos[account].has_key(job['QOS']) :
                    queued_node_sec_per_account_per_qos[account][qos] = \
                      queued_node_sec_per_account_per_qos[account][qos] + job['resource_amount_int'] * job['wall_clock_limit']
                else :
                    queued_node_sec_per_account_per_qos[account][qos] = job['resource_amount_int'] * job['wall_clock_limit']
            else :
                queued_node_sec_per_account_per_qos[account] = { qos : job['resource_amount_int'] * job['wall_clock_limit'], }
        result = (eligible_jobs, ineligible_jobs, runningstarting_jobs, message_string)
        return result

    temp_jobs_list = get_object_list(jobs_db_handle)
    #print "len(temp_jobs_list) (%s)" % (len(temp_jobs_list),)
    temp_resources_list = get_object_list(resources_db_handle)
    temp_reservations_list = get_object_list(reservations_db_handle)
    input_tuple = ( temp_jobs_list, temp_resources_list, temp_reservations_list )
    (eligible_jobs, ineligible_jobs, runningstarting_jobs, message_string) = \
      get_filtered_jobs(input_tuple)
    if FIFOSCREEN == 'ON' :
        eligible_jobs = sort_by_key(eligible_jobs, 'priority')
    for job in eligible_jobs :
        insert_new_object(job, jobs_db_handle)
    for job in ineligible_jobs :
        insert_new_object(job, jobs_db_handle)
    #print "len(eligible_jobs) (%s), len(runningstarting_jobs) (%s)" % (len(eligible_jobs), len(runningstarting_jobs))
    return (eligible_jobs, runningstarting_jobs)

def spec_string_to_start_dict(object) :
    # takes an object with a start_spec_string
    # returns a dictionary with times
    time_dict = {
      'minutes' : None,
      'hours' :   None,
      'mdays' :   None,
      'months'  : None,
      'wdays' :   None
    }
    minute, hour, mday, mon, wday = string.split(
      object['start_spec_string']
    )
    # convert each of these into a list of ints
    if minute == '*' :
        min_list = range(60)
    else :
        string_min_list = string.split(minute,',')
        min_list = map(string.atoi, string_min_list)
    if hour == '*' :
        hour_list = range(24)
    else :
        string_hour_list = string.split(hour,',')
        hour_list = map(string.atoi, string_hour_list)
    if mday == '*' :
        mday_list = range(1,32)
    else :
        string_mday_list = string.split(mday,',')
        mday_list = map(string.atoi, string_mday_list)
    if mon == '*' :
        mon_list = range(1,13)
    else :
        string_mon_list = string.split(mon,',')
        mon_list = map(string.atoi, string_mon_list)
    if wday == '*' :
        wday_list = range(7)
    else :
        string_wday_list = string.split(wday,',')
        wday_list = map(string.atoi, string_wday_list)
    time_dict['minutes'] = min_list
    time_dict['hours'] = hour_list
    time_dict['mdays'] = mday_list
    time_dict['months'] = mon_list
    time_dict['wdays'] = wday_list
    return time_dict

def get_new_candidate_time(start_time_dict, candidate_epoch, standing_TZ=None) :
    # gets start_time_dict, and current_candidate_epoch
    # returns new_candidate_epoch, new_candidate_tuple
    if os.environ.has_key('TZ') :
        old_TZ = os.environ['TZ']
    else :
        old_TZ = None
    if standing_TZ == None :
        if FORCETZ == 'NOFORCE' :
            os.environ['TZ'] = 'GMT0'
        else :
            os.environ['TZ'] = FORCETZ
    else :
        os.environ['TZ'] = standing_TZ
    mon_list = start_time_dict['months']
    mday_list = start_time_dict['mdays']
    hour_list = start_time_dict['hours']
    min_list = start_time_dict['minutes']
    wday_list = start_time_dict['wdays']
    found = 0
    #new_candidate_tuple = time.localtime(candidate_epoch)
    while found == 0 :
        candidate_tuple = time.localtime(candidate_epoch)
        #new_candidate_tuple = time.localtime(candidate_epoch)
        candidate_list = []
        for item in candidate_tuple :
            candidate_list.append(item)
        candidate_mon = candidate_list[1]
        candidate_mday = candidate_list[2]
        candidate_hour = candidate_list[3]
        candidate_min = candidate_list[4]
        candidate_wday = candidate_list[6]
        # Checking month
        if not candidate_mon in mon_list :
            # go to the start of the next month
            # increment candidate_epoch by 24 * 3600
            new_candidate_mon = candidate_mon
            while new_candidate_mon == candidate_mon :
                candidate_epoch = candidate_epoch + 24 * 3600
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_mon = new_candidate_tuple[1]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[2] = 1
            new_candidate_list[3] = 0
            new_candidate_list[4] = 0
            new_candidate_list[5] = 0
            new_candidate_list[6] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        #Checking wday
        # In Python, Monday is 0, so add 1 to conform to cron
        if not (candidate_wday + 1) % 7 in wday_list :
            # go to next wday
            # increment candidate_epoch by 24 * 3600
            new_candidate_wday = candidate_wday
            while new_candidate_wday == candidate_wday :
                candidate_epoch = candidate_epoch + 24 * 3600
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_wday = new_candidate_tuple[6]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[3] = 0
            new_candidate_list[4] = 0
            new_candidate_list[5] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        #Checking mday
        if not candidate_mday in mday_list :
            # go to next mday
            # increment candidate_epoch by 24 * 3600
            new_candidate_mday = candidate_mday
            while new_candidate_mday == candidate_mday :
                candidate_epoch = candidate_epoch + 24 * 3600
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_mday = new_candidate_tuple[2]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[3] = 0
            new_candidate_list[4] = 0
            new_candidate_list[5] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        #Checking hour
        if not candidate_hour in hour_list :
            #print "going to next hour"
            #print "hour_list is (%s)" % hour_list
            #print "candidate_hour is (%s)" % candidate_hour
            # go to next hour
            # increment candidate_epoch by 3600
            new_candidate_hour = candidate_hour
            while new_candidate_hour == candidate_hour :
                candidate_epoch = candidate_epoch + 3600
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_hour = new_candidate_tuple[3]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[4] = 0
            new_candidate_list[5] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        #Checking min
        if not candidate_min in min_list :
            #print "going to next min"
            #print "min_list is (%s)" % min_list
            #print "candidate_min is (%s)" % candidate_min
            # go to next min
            # increment candidate_epoch by 60
            new_candidate_min = candidate_min
            while new_candidate_min == candidate_min :
                candidate_epoch = candidate_epoch + 60
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_min = new_candidate_tuple[4]
            # Set all other lower rank fields to their first value
            new_candidate_list = []
            for item in new_candidate_tuple :
                new_candidate_list.append(item)
            new_candidate_list[5] = 0
            new_candidate_tuple = (
              new_candidate_list[0],
              new_candidate_list[1],
              new_candidate_list[2],
              new_candidate_list[3],
              new_candidate_list[4],
              new_candidate_list[5],
              new_candidate_list[6],
              new_candidate_list[7],
              new_candidate_list[8]
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)
            continue
        # Found start time
        found = 1
    # If reached here without continuing, then month, day,
    # hour, minute, weekday match, so return this time
    if old_TZ != None :
        os.environ['TZ'] = old_TZ
    return (candidate_epoch, candidate_tuple)

def migrate_shortpools(jobs_db_handle, resources_db_handle, reservations_db_handle) :
    # Look for standing reservations with a 'latency_float' attribute
    # for each standing reservation with a latency_float attribute,
    # create a lookahead reservation with earliest start =
    # Now_float + latency_float, duration = duration - ( Now_float - start ),
    # mode = lookahead, all other attributes same as standing res instance.
    # update standing res instance to have the node_list of the lookahead
    # reservation.
    # This should result in shortpool behaviour, with reservations migrating
    # to short-running nodes, if possible.
    existing_reservations = get_object_list(reservations_db_handle)
    job_reservations = filter( lambda reservation : \
      reservation['purpose_type_string'] == 'job' and \
      (reservation['start_count_int'] == None or \
      reservation['start_count_int'] == 0) , \
      existing_reservations )
    ignorable_job_reservations = []
    for job_reservation in job_reservations :
        ignorable_job_reservations.append(job_reservation['job_runID'])
    shortpool_reservations = filter( lambda reservation : \
      reservation['purpose_type_string'] == 'standing_reservation'\
      and reservation['latency_float'] != None , \
      existing_reservations )
    for reservation in shortpool_reservations :
        if reservation['start_time_float'] <= Now_float < reservation['end_time_float'] :
            earliest_start_float = Now_float + reservation['latency_float']
            duration_float = reservation['duration_float'] - \
              ( Now_float - reservation['start_time_float'] )
            latest_end_float = earliest_start_float + duration_float
            ignore_reservations_list=reservation['ignore_reservations_list']
            node_usage=reservation['node_usage']
            requested_resource_list=reservation['requested_resource_list']
            if ignore_reservations_list == None :
                ignore_reservations_list = [ reservation['name'] ]
            else :
                ignore_reservations_list.append( reservation['name'] )
            ignore_reservations_list = ignore_reservations_list + ignorable_job_reservations
            try :
                new_reservation = create_reservation(
                  resources_db_handle=resources_db_handle,
                  reservations_db_handle=reservations_db_handle,
                  jobs_db_handle=jobs_db_handle,
                  earliest_start_float=earliest_start_float,
                  latest_end_float=latest_end_float,
                  duration_float=duration_float,
                  resource_amount_int=reservation['resource_amount_int'],
                  node_usage=reservation['node_usage'],
                  requested_resource_list=reservation['requested_resource_list'],
                  job_restriction=reservation['job_restriction'],
                  node_restriction=reservation['node_restriction'],
                  node_sort_policy=reservation['node_sort_policy'],
                  conflict_policy=reservation['conflict_policy'],
                  purpose_type_string=reservation['purpose_type_string'],
                  maxhops=reservation['maxhops'],
                  ignore_reservations_list=ignore_reservations_list,
                  comment_string=reservation['comment_string'],
                  mode='lookahead'
                )
            except InsufficientNodes:
                continue
            else :
                if new_reservation['node_list'] != None :
                    if DEBUGJOB != None :
                        print "for reservation (%s) setting node_list to (%s)" % (reservation['name'],new_reservation['node_list'])
                    update_object_attribute('node_list',
                      new_reservation['node_list'],
                      reservation,
                      reservations_db_handle
                    )
                    # for shared nodes, need to update allocated_dict_list
                    update_object_attribute('allocated_dict_list',
                      new_reservation['allocated_dict_list'],
                      reservation,
                      reservations_db_handle
                    )

def update_standing_reservations(
  events_db_handle,
  jobs_db_handle,
  resources_db_handle,
  reservations_db_handle,
  standing_reservations_db_handle) :
    # conflict policy should allow conflicts with running jobs, choosing
    # nodes that are firstavailable or idle.  conflict policy should not
    # allow overlap with user reservations.
    # Wipe out the old standing reservations
    # filtering out job reservations this way allows overlap of nonjob
    # reservations.  The reservation ids for jobs are put into the 
    # ignore_reservations_list.  The first create_reservation call
    # for a standing_reservation wipes out all the job reservations.
    # Standing reservations can subsequently be given the old job reservation
    # ids.  The ignore_reservations_list still contains these old job
    # reservation ids, so these get ignored in create_reservation, causing
    # the new reservation to overlap the older ones...
    # Should either wipe out the old job reservations, or remake ignore
    # reservations list each time... Should be okay to pass in a none
    # ignore_reservations_list.  This will cause the job reservations
    # to get wiped out in create_reservation.  We want to get blocked
    # by the running reservations anyway...
    # Actually, we need to ignore reservations for running jobs
    # that pass job restriction for each running reservation.
    # Otherwise, the standing res instance can't get remade over
    # running jobs it allowed to run.
    jobs_dict = jobs_db_handle[0]
    reservations_dict = reservations_db_handle[0]
    standing_reservations_dict = standing_reservations_db_handle[0]
    running_reservations_list = []
    for reservation in reservations_dict.values() :
        if reservation['purpose_type_string'] == 'running' or \
          (reservation['purpose_type_string'] == 'job' and \
          reservation.has_key('job_runID') and \
          reservation['job_runID'] != None and \
          jobs_dict.has_key(reservation['job_runID']) and \
          jobs_dict[reservation['job_runID']].has_key('start_count_int') and \
          jobs_dict[reservation['job_runID']]['start_count_int'] != None and \
          jobs_dict[reservation['job_runID']]['start_count_int'] >= 1) :
            running_reservations_list.append(reservation)
    standing_reservations_list = get_object_list(
      standing_reservations_db_handle)
    srl_sort_list = []
    for standing_reservation in standing_reservations_list:
        srl_sort_list.append((standing_reservation['name'], standing_reservation))
    srl_sort_list.sort()
    standing_reservations_list = map(lambda x: x[1], srl_sort_list)
    for standing_reservation in standing_reservations_list :
        #print "updating standing reservation (%s)" % (standing_reservation,)
        # Starting from Now_float - duration_float,
        # find each of the next instances
        # Convert the potential starting time to a time tuple
        # Check to see if the values of the tuple match the lists
        # from the start_spec_string.  If all values match, then
        # attempt to create a standing reservation instance at that
        # start time.  If any of the tuple elements fails, increment
        # the potential starting time by the number of seconds appropriate
        # for that time tuple, unless month fails, in which case increment
        # by one day's worth of seconds.  This is to catch the first day
        # of each month without logic to determine days in a month.
        # Set all other tuple elements to their first possible value.
        # iterate until depth number of instances have been created.
        # Convert the start_spec_string into separate fields
        # move to function spec_string_to_start_dict
	# Check to see if there are enough accepted nodes to
        # ever fulfull this reservation
        for reservation in reservations_dict.values() :
            if reservation['purpose_type_string'] == \
              'standing_reservation' and \
              reservation['creator_string'] == standing_reservation['name']:
                try :
                    delete_object(reservation['name'],
                      reservations_db_handle)
                except NameNotInDB, exc :
                    print "Standing Reservation to be deleted (%s) not found" % \
                      exc.name
        accepted_nodes_list = get_accepted_nodes_list(standing_reservation['node_restriction'], resources_db_handle)
        if len(accepted_nodes_list) < standing_reservation['resource_amount_int'] :
            print "InsufficientAcceptedNodes"
            print standing_reservation['node_restriction']
            print accepted_nodes_list
            continue
        start_time_dict = spec_string_to_start_dict(standing_reservation)
        if standing_reservation.has_key('TZ_string') :
            standing_TZ = standing_reservation['TZ_string']
        else :
            standing_TZ = None
        number_found = 0
        depth = standing_reservation['depth']
        if standing_reservation.has_key('maxhops'):
            maxhops = standing_reservation['maxhops']
        else:
            maxhops = None
        resource_amount_int = standing_reservation['resource_amount_int']
        if standing_reservation.has_key('requested_resource_list') :
            requested_resource_list = standing_reservation['requested_resource_list']
        else :
            requested_resource_list = []
            req_dict_dict = {'type' : 'node_exclusive',
                             'req_list' : []
                            }
            for index in range(resource_amount_int) :
                requested_resource_list.append(req_dict_dict)
        if standing_reservation.has_key('node_usage') and standing_reservation['node_usage'] != None :
            node_usage = standing_reservation['node_usage']
        else :
            node_usage = 'node_exclusive'
        if standing_reservation.has_key('requested_resource_list') :
            requested_resource_list = standing_reservation['requested_resource_list']
        else :
            requested_resource_list = []
        latency_float = standing_reservation['latency_float']
        job_restriction = standing_reservation['job_restriction']
        node_restriction = standing_reservation['node_restriction']
        conflict_policy = standing_reservation['conflict_policy']
        duration_float = standing_reservation['duration_float']
        if standing_reservation['sort_policy'] == None :
            sort_policy = LAST_AVAILABLE_IGNORED_FIRST
        else :
            sort_policy = standing_reservation['sort_policy']
        affinity_calculation = standing_reservation['affinity_calculation']
        purpose_type_string = 'standing_reservation'
        creator_string = standing_reservation['name']
        if standing_reservation.has_key('creator_string') and type(standing_reservation['creator_string']) is str :
            comment_string = 'standing reservation: ' + standing_reservation['name'] + ' ;' + standing_reservation['comment_string'] + ' ;' + standing_reservation['creator_string'] + ' ;'
        else :
            comment_string = 'standing reservation: ' + standing_reservation['name'] + ' ;' + standing_reservation['comment_string'] + ' ;'
        candidate_epoch = Now_float - duration_float
        ignore_reservations_list = []
        if standing_reservation.has_key('overlap_running_int') and \
          standing_reservation['overlap_running_int'] != None and \
          standing_reservation['overlap_running_int'] >= 1 :
            #print "overlap_running_int found for (%s)" % standing_reservation['name']
            for reservation in running_reservations_list :
                if not jobs_dict.has_key(reservation['job_runID']) :
                    continue
                input_tuple = ( jobs_dict[reservation['job_runID']], )
                result = apply_policy_code(job_restriction, input_tuple)
                if result == 0 :
                    ignore_reservations_list.append(reservation['name'])
        #original_nodes = []
        #found_original_nodes = 0
        low_duration = 0
        while number_found < depth and low_duration == 0 :
            duration_float = standing_reservation['duration_float']
            ( candidate_epoch, new_candidate_tuple ) = \
              get_new_candidate_time( start_time_dict, candidate_epoch, standing_TZ=standing_TZ )

            earliest_start_float = candidate_epoch
            #print "pre create_res earliest_start_float (%s)" % (time.strftime("%H:%M:%S_%m/%d/%Y", time.localtime(earliest_start_float)),)
            latest_end_float = candidate_epoch + duration_float
            if earliest_start_float >= END_OF_SCHEDULING :
                break
            try :
                new_res = create_reservation(
                  resources_db_handle=resources_db_handle,
                  reservations_db_handle=reservations_db_handle,
                  jobs_db_handle=jobs_db_handle,
                  earliest_start_float=earliest_start_float,
                  latest_end_float=latest_end_float,
                  duration_float=duration_float,
                  latency_float=latency_float,
                  resource_amount_int=resource_amount_int,
                  node_usage=node_usage,
                  requested_resource_list=requested_resource_list,
                  job_restriction=job_restriction,
                  node_restriction=node_restriction,
                  node_sort_policy=sort_policy,
                  conflict_policy=conflict_policy,
                  affinity_calculation=affinity_calculation,
                  purpose_type_string=purpose_type_string,
                  creator_string=creator_string,
                  comment_string=comment_string,
                  maxhops=maxhops,
                  ignore_reservations_list=ignore_reservations_list,
                  accepted_nodes_list=accepted_nodes_list,
                  mode='real'
                )
            except InsufficientNodes:
                print "InsufficientNodes"
                # full size, full duration failed
                # grab as many nodes as possible (since it failed
                # due to insufficient nodes, we should not get more
                # than we asked for before...
                # At this point, it would also be possible to
                # create small sub reservations between earliest_start
                # and latest_end, first finding all sub reservations
                # with the full resource amount.  This would entail
                # reducing duration by small chunks (15 min.) and
                # doing trial reservations, keeping track of end_times
                # after finding all full resource amount sub reservations
                # could find all smaller than full amount sub reservations
                # by decrementing resource amount iteratively...
                # For now, just bail.
                partial_found = 0
                minimal_resource_amount_int = None
                #if earliest_start_float < Now_float :
                #    max_duration_float = min(duration_float,standing_reservation['duration_float'] - (Now_float - earliest_start_float))
                #else :
                #    max_duration_float = duration_float
                #latest_end_float = candidate_epoch + max_duration_float
                try :
                    print "trying a partial..."
                    new_res = create_reservation(
                      resources_db_handle=resources_db_handle,
                      reservations_db_handle=reservations_db_handle,
                      jobs_db_handle=jobs_db_handle,
                      earliest_start_float=earliest_start_float,
                      latest_end_float=latest_end_float,
                      duration_float=duration_float,
                      latency_float=latency_float,
                      resource_amount_int=minimal_resource_amount_int,
                      node_usage=node_usage,
                      requested_resource_list=requested_resource_list,
                      job_restriction=job_restriction,
                      node_restriction=node_restriction,
                      node_sort_policy=sort_policy,
                      conflict_policy=conflict_policy,
                      affinity_calculation=affinity_calculation,
                      purpose_type_string=purpose_type_string,
                      creator_string=creator_string,
                      comment_string=comment_string,
                      maxhops=maxhops,
                      accepted_nodes_list=accepted_nodes_list,
                      ignore_reservations_list=ignore_reservations_list,
                      mode='real'
                    )
                except InsufficientNodes :
                    if DEBUG == 'usr' :
                        print "No resource_amount Standing reservation instance failed"
                else :
                    print "No resource_amount Standing reservation instance succeeded"
                    partial_found = new_res['resource_amount_int']
                    #if found_original_nodes == 0 :
                    #    found_original_nodes = 1
                    #    original_nodes = new_res['node_list']
                    #    sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', `original_nodes`, LAST_AVAILABLE_IGNORED_FIRST)
                # Find the next full instance
                full_found = 0
                if earliest_start_float < Now_float :
                    full_earliest_start_float = Now_float
                    #max_duration_float = min(duration_float,standing_reservation['duration_float'] - (Now_float - earliest_start_float))
                else :
                    full_earliest_start_float = earliest_start_float
                    #max_duration_float = duration_float
                #last_change = - duration_float/2
                last_change = duration_float/2
                duration = duration_float
                #while full_found == 0 and duration_float > FUDGE_FACTOR :
                while full_found == 0 or abs(last_change) > FUDGE_FACTOR :
                    # should do a binary search, with FUDGE_FACTOR resolution
                    full_amount = resource_amount_int - partial_found
                    duration = max(1,duration - last_change)
                    try :
                        print "trying partial with duration (%s) last_change (%s) abs(last_change) %s FUDGE_FACTOR (%s) comment_string (%s)" % (duration,last_change,abs(last_change),FUDGE_FACTOR,comment_string)
                        new_res = create_reservation(
                          resources_db_handle=resources_db_handle,
                          reservations_db_handle=reservations_db_handle,
                          jobs_db_handle=jobs_db_handle,
                          earliest_start_float=full_earliest_start_float,
                          latest_end_float=latest_end_float,
                          duration_float=duration_float,
                          latency_float=latency_float,
                          resource_amount_int=full_amount,
                          node_usage=node_usage,
                          requested_resource_list=requested_resource_list,
                          job_restriction=job_restriction,
                          node_restriction=node_restriction,
                          node_sort_policy=sort_policy,
                          conflict_policy=conflict_policy,
                          affinity_calculation=affinity_calculation,
                          purpose_type_string=purpose_type_string,
                          creator_string=creator_string,
                          comment_string=comment_string,
                          maxhops=maxhops,
                          accepted_nodes_list=accepted_nodes_list,
                          ignore_reservations_list=ignore_reservations_list,
                          mode='real'
                        )
                    except InsufficientNodes :
                        # Set duration smaller by FUDGE_FACTOR and try again
                        #duration_float = duration_float - FUDGE_FACTOR
                        print "full, short duration failed"
                        if duration < FUDGE_FACTOR :
                            print "standing res instance failed, no time!"
                            #low_duration = 1
                            low_duration = 1
                            break
                        if last_change < 0 :
                            last_change = last_change
                        else :
                            #last_change = - last_change/2
                            last_change = last_change/2
                    else :
                        print "full, short duration succeeded"
                        if abs(last_change) < FUDGE_FACTOR :
                            full_found = 1
                            break
                        if last_change < 0 :
                            last_change = - last_change/2
                        else :
                            last_change = last_change
                        #if found_original_nodes < 2 :
                        #    found_original_nodes = found_original_nodes + 1
                        #    original_nodes = original_nodes + new_res['node_list']
                        #    sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', `original_nodes`, LAST_AVAILABLE_IGNORED_FIRST)
                #print "out of while"
                if partial_found > 0 or full_found > 0 :
                    number_found = number_found + 1
            else :
                # full size, full duration succeeded
                # Successful creation of standing reservation instance
                number_found = number_found + 1
                #if found_original_nodes == 0 :
                #    found_original_nodes = 2
                #    original_nodes = new_res['node_list']
                #    sort_policy = re.sub('___ORIGINAL_NODE_PLACEHOLDER___', `original_nodes`, LAST_AVAILABLE_IGNORED_FIRST)
            # increment time by 60 sec for next iteration,
            # so we don't keep choosing the same period
            new_candidate_min = new_candidate_tuple[4]
            original_candidate_min = new_candidate_tuple[4]
            while new_candidate_min == original_candidate_min :
                candidate_epoch = candidate_epoch + 60
                new_candidate_tuple = time.localtime(candidate_epoch)
                new_candidate_min = new_candidate_tuple[4]
            temp_tuple = new_candidate_tuple
            new_candidate_tuple = (
              temp_tuple[0],
              temp_tuple[1],
              temp_tuple[2],
              temp_tuple[3],
              new_candidate_min,
              0,
              temp_tuple[6],
              temp_tuple[7],
              temp_tuple[8],
            )
            # set candidate_epoch to the new candidate time
            candidate_epoch = time.mktime(new_candidate_tuple)

def update_running_reservations(runningstarting_jobs, reservations_db_handle, resources_db_handle, jobs_db_handle, events_db_handle) :
    # need to clean this up
    resources_dict = resources_db_handle[0]
    existing_reservations = get_object_list(reservations_db_handle)
    save_jobs_list = []
    runningstarting_names_list = get_object_names_list(runningstarting_jobs)
    for reservation in existing_reservations :
        if reservation['purpose_type_string'] in ['running', 'preempted_job'] :
            delete_object(reservation['name'], reservations_db_handle)
        elif reservation['purpose_type_string'] in ['pushback',] :
            if not (jobs_db_handle[0].has_key(reservation['job_runID']) and jobs_db_handle[0][reservation['job_runID']]['state'] in ['Preempted', 'Running']) :
                delete_object(reservation['name'], reservations_db_handle)
                
    # need to do something differenct for node_usage:node_shared jobs.
    # take their resources_requested_list and create reservations according
    # to that list.  The easy way here would be to add a cpu_map and
    # mb_map to this reservation.  Then, when open_windows gets found,
    # use these, in addition to the node_list to find open_windows.
    # Maybe don't need cpu_map and mb_map.  generate those from
    # resources_requested_list?
    preempted_jobs_list = []
    for job in runningstarting_jobs :
        #print "updating res for job (%s)" % (job,)
        if job['state'] == 'Running' :
            purpose_type_string = 'running'
        elif job['state'] == 'Preempted' :
            preempted_jobs_list.append(copy.deepcopy(job))
            continue
        else :
            purpose_type_string = job['state']
            continue
        #print "setting SubmitTime"
        submit_time = job['SubmitTime']
        new_res_name = `int(submit_time)`
        while reservations_db_handle[0].has_key(new_res_name) :
            submit_time = submit_time + 1
            new_res_name = `int(submit_time)`
        new_res = initialize_reservation(new_res_name)
        if job.has_key('wall_clock_used') and job['wall_clock_used'] != None :
            wall_clock_used = job['wall_clock_used']
        else :
            wall_clock_used = Now_float - job['Dispatch_Time']
        # for preempted job, here is the tricky part.  The reservation
        # duration should be from start time to end of last job
        # on any of the preempted nodes + remaining job time
        # would it work to: make all the running reservations first,
        # restrict preempt job reservations to the preemted nodes,
        # set duration to wall clock remaining,
        # create the preempt reservations, set duration and start to now.
        if job['wall_clock_limit'] < wall_clock_used :
            duration_float = wall_clock_used + FUDGE_FACTOR
        else :
            duration_float = job['wall_clock_limit'] + FUDGE_FACTOR
        #start_time_float = job['Dispatch_Time']
        start_time_float = Now_float - wall_clock_used
        # FIXTHIS: if a node gets allocated twice in node_shared,
        # then the node will only be listed once in 'allocated_hosts'
        # for LoadL.  Should this be fixed in Catalina_LL.py?
        # Or should resource_amount_int mean number of nodes?
        # Should base allocated_hosts off of taskmachines in Catalina_LL.py
        # How to map task_hosts to initmap/requested_resource_list?
        # There should be one requested_resource_list for each node
        # of initmap, and one req_list entry for each task on that node.
        # for every task_host, should reserve a req_list.
        # are the sequences of task_hosts, initmap and requested_resource_list
        # matched for both PBS and LoadL?  For now, assume that they are.
        #task_hosts = job['task_hosts']
        allocated_hosts = job['allocated_hosts']
        if job.has_key('resource_hosts'):
            task_hosts = job['task_hosts'] + job['resource_hosts']
            allocated_hosts = allocated_hosts + job['resource_hosts']
        else:
            task_hosts = job['task_hosts']
        #print "task_hosts (%s)" % (task_hosts,)
        #allocated_hosts = job['allocated_hosts']
        # If an allocated_host does not exist in the resource_dict,
        # I don't know how to make the reservation.
        unknown_hosts = 0
        for allocated_host in allocated_hosts :
            if not resources_db_handle[0].has_key(allocated_host) :
                print "failed to find allocated host (%s)" % (allocated_host,)
                unknown_hosts = unknown_hosts + 1
        if unknown_hosts > 0 :
            print resources_db_handle[0].keys()
            continue
        node_list = allocated_hosts
        resource_amount_int = job['resource_amount_int']
        new_res['duration_float'] = duration_float
        new_res['start_time_float'] = start_time_float
        new_res['end_time_float'] = start_time_float + duration_float
        new_res['node_list'] = node_list
        new_res['job_restriction'] = 'result = 1'
        new_res['resource_amount_int'] = resource_amount_int
        new_res['purpose_type_string'] = purpose_type_string
        new_res['job_runID'] = job['name']
        task_host_index = 0
        if job.has_key('requested_resource_list') :
            node_requested_list = filter(lambda x : x['type'] in ('node_shared', 'node_exclusive'), job['requested_resource_list'])
            #print "node_requested_list (%s)" % (node_requested_list,)
            #if len(node_list) != len(node_requested_list) :
            #    print "%s: node_list (%s) != requested_resource_list (%s)!" % \
            #      (job['name'], node_list, requested_resource_list)
            #for reqres in node_requested_list :
            allocated_dict_list = []
            # This only supports node cpu/memory.  To support floating
            # licenses, storage, etc. need to add these in as types.
            #for index in range(min(len(node_requested_list), len(node_list))) :
            #for index in range(len(node_requested_list)) :
            for index in range(min(len(node_requested_list), len(node_list))) :
                #print "node_requested_list[index] (%s)" % (node_requested_list[index],)
                # allocated_dict_list
                # This is a little tricky.  In create_reservation, need to
                # make sure that the node_list syncs up with
                # requested_resource_list...
                if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    print "node_requested_list (%s)" % (node_requested_list,)
                node_usage = node_requested_list[index]['type']
                if node_requested_list[index]['type'] == 'node_shared' :
                    node_amount = 0
                    #cpus = 0
                    #memory = 0
                    this_consumable_dict = {}
                    if node_requested_list[index].has_key('req_list') :
                        nodename = task_hosts[task_host_index]
                        #print "task_hosts[task_host_index] (%s) (%s) (%s)" % (task_hosts, task_hosts[task_host_index], task_host_index)
                        if nodename == '' :
                            # for serial jobs, taskinstancemachinemap is
                            # apparently left empty.  So, take the first
                            # allocated host, assuming this is a one node
                            # serial job
                            nodename = allocated_hosts[0]
                        #print "nodename (%s)" % (nodename,)
                        for req in node_requested_list[index]['req_list'] :
                            #if req.has_key('cpu') :
                            #    cpus = cpus + req['cpu']
                            #if req.has_key('memory') :
                            #    memory = memory + req['memory']
                            for req_key in req.keys():
                                if this_consumable_dict.has_key(req_key):
                                    this_consumable_dict[req_key] = this_consumable_dict[req_key] + req[req_key]
                                else:
                                    this_consumable_dict[req_key] = req[req_key]
                            task_host_index = task_host_index + 1
                    else :
                        #cpus = resources_dict[node_list[index]]['ConsumableCpus']
                        #memory = resources_dict[node_list[index]]['ConsumableMemory']
                        #cpus = 0
                        #memory = 0
                        this_consumable_dict = {}
                    #if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                    #    print "node_shared cpus(%s) memory (%s)" % (cpus, memory)
                else :
                    # node_exclusive, so assign all consumables_dict resources
                    # for that node
                    node_amount = 1
                    #cpus = resources_dict[node_list[index]]['ConsumableCpus']
                    #memory = resources_dict[node_list[index]]['ConsumableMemory']
                    this_consumable_dict = resources_dict[node_list[index]]['consumable_dict']
                    if node_requested_list[index].has_key('req_list') :
                        nodename = task_hosts[task_host_index]
                        if nodename == '' :
                            nodename = allocated_hosts[0]
                        for req in node_requested_list[index]['req_list'] :
                            task_host_index = task_host_index + 1

                    #nodename = node_list[index]
                    if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                        print "node_exclusive cpus (%s) memory (%s)" % (cpus, memory)
                #allocated_dict = { 'nodename' : nodename,
                #                   'type' : node_usage,
                #                   'node' : node_amount,
                #                   'cpu' : cpus,
                #                   'memory' : memory
                #                 }
                allocated_dict = { 'nodename' : nodename,
                                   'type' : node_usage,
                                   'node' : node_amount,
                                 }
                #for consumable_key in resources_dict[node_list[index]]['consumable_dict']
                for consumable_key in this_consumable_dict.keys():
                    if consumable_key == 'node':
                        continue
                    if node_usage == 'node_shared':
                        allocated_dict[consumable_key] = this_consumable_dict[consumable_key]
                    else:
                        allocated_dict[consumable_key] = resources_dict[node_list[index]]['consumable_dict'][consumable_key]
                      #{ 'nodename' : node,
                      #  'type' : 'node_exclusive',
                      #  'node' : 1,
                      #  'cpu' : resources_dict[node]['ConsumableCpus'],
                      #  'memory' : resources_dict[node]['ConsumableMemory'] }
                allocated_dict_list.append(allocated_dict)
        else :
            # if no requested_resource_list was found, assume that all
            # resources for the node were requested
            if DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
                print "no requested_resource_list"
            allocated_dict_list = []
            for index in range(len(node_list)) :
                allocated_dict = { 'nodename' : node_list[index],
                                   'type' : 'node_exclusive',
                                   'node' : 1,
                                 }
                this_consumable_dict = resources_dict[node_list[index]]['consumable_dict']
                for consumable_key in this_consumable_dict.keys():
                    if consumable_key == 'node':
                        continue
                    allocated_dict[consumable_key] = resources_dict[node_list[index]]['consumable_dict'][consumable_key]
                #cpus = resources_dict[node_list[index]]['ConsumableCpus']
                #memory = resources_dict[node_list[index]]['ConsumableMemory']
                #allocated_dict = { 'nodename' : node_list[index],
                #                   'type' : 'node_exclusive',
                #                   'cpu' : cpus,
                #                   'memory' : memory
                #                 }
                allocated_dict_list.append(allocated_dict)
        #print "running allocated_dict_list (%s)" % (allocated_dict_list,)
        new_res['allocated_dict_list'] = allocated_dict_list
        insert_new_object(new_res, reservations_db_handle)
    for preempted_job in preempted_jobs_list :
        #def create_job_reservation(
          #resources_db_handle,
          #reservations_db_handle,
          #jobs_db_handle,
          #job_step,
          #events_db_handle
          #) :
        # create a job reservation with remaining wall_clock duration
        # at the end of any running reservations
        new_res = create_job_reservation(resources_db_handle,
                               reservations_db_handle,
                               jobs_db_handle,
                               preempted_job,
                               events_db_handle
                               )
        if new_res != None :
            del_pushback_list = []
            for res_key in reservations_db_handle[0].keys() :
                if reservations_db_handle[0][res_key]['purpose_type_string'] == "pushback" and reservations_db_handle[0][res_key]['job_runID'] == preempted_job['name'] :
                    del_pushback_list.append(res_key)
            for pushback_key in del_pushback_list :
                delete_object(pushback_key, reservations_db_handle)
        else :
            print "preempted_job reservation failed for (%s)!" % preempted_job['name']

def get_open_windows_for_reservation_list(reservations, nodes) :
    # take a list of reservations
    # return a list of open_windows, sorted in ascending start order
    # (start, end, node_name)
    def sort_by_reservation_start(first, second) :
        if first['start_time_float'] < second['start_time_float'] :
            return -1
        if first['start_time_float'] == second['start_time_float'] :
            return 0
        if first['start_time_float'] > second['start_time_float'] :
            return 1
    reservations.sort(sort_by_reservation_start)
    open_windows_list = []
    for reservation in reservations :
        start_time_float = reservation['start_time_float']
        end_time_float = reservation['end_time_float']
        for node in reservation['node_list'] :
            if node in nodes :
                open_windows_list.append(
                  ( start_time_float, end_time_float, node )
                )
    return open_windows_list

def create_standing_reservation(
  standing_reservations_db_handle,
  start_spec_string,
  standing_TZ,
  duration_float,
  resource_amount_int,
  requested_resource_list,
  node_usage,
  affinity_calculation,
  sort_policy,
  depth,
  mode,
  comment='',
  conflict_policy=None,
  creator_string=username_string,
  latency_float=None,
  maxhops=None,
  job_restriction='result = 1',
  node_restriction='result = 1',
  overlap_running_int = 0
  ) :
    if start_spec_string == None :
        raise 'NoStartSpec'
    if duration_float == None :
        raise 'NoDuration'
    if duration_float < 0 :
        raise 'NegativeDuration'
    if resource_amount_int == None :
        raise 'MissingAmount'
    if depth == None :
        raise 'NoDepth'
    start_spec_reo = re.compile(r"((\*|(\d+,?)+)\s*){5}")
    if not start_spec_reo.match(start_spec_string) :
        raise 'BadStartSpec', start_spec_string
    new_standing_reservation_name = get_new_db_key(standing_reservations_db_handle)
    new_standing_reservation = initialize_reservation(new_standing_reservation_name)
    new_standing_reservation['start_spec_string'] = start_spec_string
    new_standing_reservation['TZ_string'] = standing_TZ
    new_standing_reservation['duration_float'] = duration_float
    new_standing_reservation['latency_float'] = latency_float
    new_standing_reservation['creator_string'] = creator_string
    new_standing_reservation['resource_amount_int'] = resource_amount_int
    new_standing_reservation['requested_resource_list'] = requested_resource_list
    new_standing_reservation['node_usage'] = node_usage
    new_standing_reservation['job_restriction'] = job_restriction
    new_standing_reservation['node_restriction'] = node_restriction
    new_standing_reservation['conflict_policy'] = conflict_policy
    new_standing_reservation['sort_policy'] = sort_policy
    new_standing_reservation['affinity_calculation'] = affinity_calculation
    new_standing_reservation['depth'] = string.atoi(depth)
    new_standing_reservation['comment_string'] = comment
    new_standing_reservation['maxhops'] = maxhops
    new_standing_reservation['overlap_running_int'] = overlap_running_int
    insert_new_object(
      new_object=new_standing_reservation,
      db_handle=standing_reservations_db_handle
    )
    return new_standing_reservation
    
def create_job_reservation(
  resources_db_handle,
  reservations_db_handle,
  jobs_db_handle,
  job_step,
  events_db_handle
) :
    # Create a job reservation by calling create_reservation and
    # passing in runID, resource_amount_int, duration_float, purpose_type_string='job'
    job_runID = job_step['name']
    print "create_job_reservation (%s)" % (job_runID,)
    preempted_ignore_list = []
    if job_step.has_key('run_at_risk_int') and job_step['run_at_risk_int'] >= 1 :
        duration_float = RUN_AT_RISK_MIN_RUNTIME + FUDGE_FACTOR
    elif job_step['state'] == 'Preempted' :
        # schedule preemptible jobs at full wallclock time,
        # we schedule at full wallclock, since the schedule
        # will be wrong, if no preemption occurs.
        #duration_float = RUN_AT_RISK_MIN_RUNTIME + FUDGE_FACTOR
        # for preempted jobs, need to restrict to the nodes the job
        # currently occupies, should put this into the Catalina_RM
        # module...
        if DEBUGJOB != None and DEBUGJOB == job_runID:
            print "setting duration_float for Preempted job to (%s)" % (job_step['wall_clock_limit'] - job_step['wall_clock_used'] + FUDGE_FACTOR,)
        duration_float = job_step['wall_clock_limit'] - job_step['wall_clock_used'] + FUDGE_FACTOR
        # ignore any 'pushback' reservations with the same 'job_runID'
        # delete the pushback reservations, if the create_res is successful
        for pushback_key in reservations_db_handle[0].keys() :
            if reservations_db_handle[0][pushback_key]['purpose_type_string'] == 'pushback' and \
              reservations_db_handle[0][pushback_key]['job_runID'] == job_step['name'] :
                preempted_ignore_list.append(pushback_key)
    else :
        duration_float = job_step['wall_clock_limit'] + FUDGE_FACTOR
    resource_amount_int = job_step['resource_amount_int']
    if job_step.has_key('node_usage') and job_step['node_usage'] == 'node_shared' :
        node_dict_type = 'node_shared'
    else :
        node_dict_type = 'node_exclusive'
    if job_step.has_key('maxhops') and job_step['maxhops'] != None and job_step['maxhops'] != 'None':
        maxhops = int(job_step['maxhops'])
    else :
        maxhops = None
    if job_step.has_key('requested_resource_list') :
        requested_resource_list = job_step['requested_resource_list']
    else :
        requested_resource_list = []
    if job_step['state'] == 'Preempted' :
        purpose_type_string = 'preempted_job'
    else :
        purpose_type_string = 'job'
    node_restriction = None
    conflict_policy = None
    node_sort_policy = None
    job_restriction = "if input_tuple[0]['name'] == '%s' : result = 0" % job_step['name']
    new_res = None
    try:
        new_res = create_reservation(
          resources_db_handle=resources_db_handle,
          reservations_db_handle=reservations_db_handle,
          jobs_db_handle=jobs_db_handle,
          duration_float=duration_float,
          resource_amount_int=resource_amount_int,
          requested_resource_list=requested_resource_list,
          ignore_reservations_list=preempted_ignore_list,
          node_sort_policy=node_sort_policy,
          node_usage=node_dict_type,
          node_restriction = node_restriction,
          conflict_policy=conflict_policy,
          job_runID=job_runID,
          job_restriction=job_restriction,
          purpose_type_string=purpose_type_string,
          job_step=job_step,
          maxhops=maxhops,
          mode='real'
          )
    except InsufficientNodes, exc:
        message = "job reservation (%s) failed for runID (%s) due to insufficient resources" % ( exc.res['name'], exc.res['job_runID'] )
        print message
        catsyslog(message,'notice')
    #if job_step['state'] == 'Preempted' :
    #    # should set start_time_float to Now_float, and duration accordingly
    #    # def update_object_attribute(name, value, object, db_handle) :
    #    start_time_float = Now_float
    #    update_object_attribute('start_time_float', start_time_float, new_res, reservations_db_handle)
    #    end_time_float = Now_float + duration_float
    #    update_object_attribute('end_time_float', end_time_float, new_res, reservations_db_handle)
    #    update_object_attribute('duration_float', duration_float, new_res, reservations_db_handle)

    return new_res


def create_job_reservations(eligible_jobs, resources_db_handle, reservations_db_handle, jobs_db_handle, events_db_handle) :
    reservations_created = 0
    jobs_dict = jobs_db_handle[0]
    running_jobs_per_user = {}
    running_jobs_per_account_per_qos = {}
    running_jobs_per_user_per_qos = {}
    running_node_sec_per_account_per_qos = {}
    running_node_sec_per_user_per_qos = {}
    for job_name in jobs_dict.keys() :
        state = jobs_dict[job_name]['state']
        user = jobs_dict[job_name]['user']
        account = jobs_dict[job_name]['account']
        qos = jobs_dict[job_name]['QOS']
        if state in ['Running','Starting'] :
            if running_jobs_per_user.has_key(user) :
                running_jobs_per_user[user] = running_jobs_per_user[user] + 1
            else :
                running_jobs_per_user[user] = 1
            if running_jobs_per_account_per_qos.has_key(account) :
                if running_jobs_per_account_per_qos[account].has_key(qos) :
                    running_jobs_per_account_per_qos[account][qos] = running_jobs_per_account_per_qos[account][qos] + 1
                else :
                    running_jobs_per_account_per_qos[account][qos] = 1
            else :
                running_jobs_per_account_per_qos[account] = { qos : 1, }
            if running_jobs_per_user_per_qos.has_key(user) :
                if running_jobs_per_user_per_qos[user].has_key(qos) :
                    running_jobs_per_user_per_qos[user][qos] = running_jobs_per_user_per_qos[user][qos] + 1
                else :
                    running_jobs_per_user_per_qos[user][qos] = 1
            else :
                running_jobs_per_user_per_qos[user] = { qos : 1, }
            if running_node_sec_per_user_per_qos.has_key(user) :
                if running_node_sec_per_user_per_qos[user].has_key(qos) :
                    running_node_sec_per_user_per_qos[user][qos] = running_node_sec_per_user_per_qos[user][qos] + jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit']
                else :
                    running_node_sec_per_user_per_qos[user][qos] = jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit']
            else :
                running_node_sec_per_user_per_qos[user] = { qos : jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit'], }
            if running_node_sec_per_account_per_qos.has_key(account) :
                if running_node_sec_per_account_per_qos[account].has_key(qos) :
                    running_node_sec_per_account_per_qos[account][qos] = running_node_sec_per_account_per_qos[account][qos] + jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit']
                else :
                    running_node_sec_per_account_per_qos[account][qos] = jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit']
            else :
                running_node_sec_per_account_per_qos[account] = { qos : jobs_dict[job_name]['resource_amount_int'] * jobs_dict[job_name]['wall_clock_limit'], }
    # Potential race condition.  If a job start was initiated last
    # iteration, but the RM does not report the job as Running or
    # Starting, then it won't contribute to the running_jobs_per_user
    # count.  This will allow jobs past the limit...
    # Consider keeping a list of all active job reservations for
    # that user...
    active_reservations_per_user = {}
    active_reservations_per_account_per_qos = {}
    active_reservations_per_user_per_qos = {}
    active_reserved_node_sec_per_account_per_qos = {}
    active_reserved_node_sec_per_user_per_qos = {}
    existing_reservations = get_object_list(reservations_db_handle)
    for reservation in existing_reservations :
        #if reservation.has_key('job_runID'):
        #    print "checking reservation for job (%s)" % reservation['job_runID']
        #    print "reservation['purpose_type_string'] (%s) reservation['start_count_int'] (%s)" % (reservation['purpose_type_string'],reservation['start_count_int'])
        if reservation['purpose_type_string'] == 'job' and \
          ( not reservation.has_key('start_count_int') or \
          not reservation['start_count_int'] >= 1 or \
          Now_float > reservation['start_time_float'] + JOB_START_TIME_LIMIT or \
          ( jobs_dict.has_key(reservation['job_runID']) and \
          jobs_dict[reservation['job_runID']]['state'] in ['Running', 'Removed', 'Canceled', 'Completed']) ):
            #print "deleting reservation for job (%s)" % reservation['job_runID']
            delete_object(reservation['name'], reservations_db_handle)
            if ( (jobs_dict.has_key(reservation['job_runID']) and \
                not jobs_dict[reservation['job_runID']]['state'] in ['Unknown']) \
              or LOST_JOB_WARN == 'TRUE' ) and \
              ( jobs_dict.has_key(reservation['job_runID']) and \
                not jobs_dict[reservation['job_runID']]['state'] in ['Removed', 'Canceled', 'Completed'] ) and \
              SERVERMODE == 'NORMAL' and \
              Now_float > reservation['start_time_float'] + JOB_START_TIME_LIMIT \
              and ( JOB_START_WARN_LIMIT == None or \
              jobs_dict[reservation['job_runID']]['job_start_warns_int'] < JOB_START_WARN_LIMIT ) :
                message = "job (%s) has exceeded JOB_START_TIME_LIMIT, reservation deleted" % reservation['job_runID']
                recipient = MAIL_RECIPIENT
                subject = "Catalina job reservation timed out"
                if jobs_dict[reservation['job_runID']].has_key('job_start_warns_int') :
                    warn_count = jobs_dict[reservation['job_runID']]['job_start_warns_int'] + 1
                else :
                    warn_count = 1
                warn(message, subject, recipient)
                catsyslog(message,'notice')
                update_object_attribute('job_start_warns_int', warn_count, jobs_dict[reservation['job_runID']], jobs_db_handle)
    existing_reservations = get_object_list(reservations_db_handle)
    for reservation in existing_reservations :
        if ( reservation['purpose_type_string'] == 'job' \
          or reservation['purpose_type_string'] == 'running' ) \
          and jobs_dict.has_key(reservation['job_runID']) :
            if (Now_float + 1) >= reservation['start_time_float'] :
                active_res_owner = jobs_dict[reservation['job_runID']]['user']
                active_res_account = jobs_dict[reservation['job_runID']]['account']
                if active_reservations_per_user.has_key(active_res_owner) :
                    active_reservations_per_user[active_res_owner].append(reservation['job_runID'])
                else :
                    active_reservations_per_user[active_res_owner] = [reservation['job_runID'],]
                if active_reservations_per_account_per_qos.has_key(active_res_account) :
                    if active_reservations_per_account_per_qos[active_res_account].has_key(jobs_dict[reservation['job_runID']]['QOS']) :
                        active_reservations_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']].append(reservation['job_runID'])
                    else :
                        active_reservations_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']] = [reservation['job_runID'],]
                else :
                    active_reservations_per_account_per_qos[active_res_account] = { jobs_dict[reservation['job_runID']]['QOS'] : [reservation['job_runID'],] , }
                if active_reservations_per_user_per_qos.has_key(active_res_owner) :
                    if active_reservations_per_user_per_qos[active_res_owner].has_key(jobs_dict[reservation['job_runID']]['QOS']) :
                        active_reservations_per_user_per_qos[active_res_owner][jobs_dict[reservation['job_runID']]['QOS']].append(reservation['job_runID'])
                    else :
                        active_reservations_per_user_per_qos[active_res_owner][jobs_dict[reservation['job_runID']]['QOS']] = [reservation['job_runID'],]
                else :
                    active_reservations_per_user_per_qos[active_res_owner] = { jobs_dict[reservation['job_runID']]['QOS'] : [reservation['job_runID'],] , }
                if active_reserved_node_sec_per_user_per_qos.has_key(active_res_owner) :
                    if active_reserved_node_sec_per_user_per_qos[active_res_owner].has_key(jobs_dict[reservation['job_runID']]['QOS']) :
                        active_reserved_node_sec_per_user_per_qos[active_res_owner][jobs_dict[reservation['job_runID']]['QOS']] = active_reserved_node_sec_per_user_per_qos[active_res_owner][jobs_dict[reservation['job_runID']]['QOS']] + jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit']
                    else :
                        active_reserved_node_sec_per_user_per_qos[active_res_owner][jobs_dict[reservation['job_runID']]['QOS']] = jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit']
                else :
                    active_reserved_node_sec_per_user_per_qos[active_res_owner] = { jobs_dict[reservation['job_runID']]['QOS'] : jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit'], }
                if active_reserved_node_sec_per_account_per_qos.has_key(active_res_account) :
                    if active_reserved_node_sec_per_account_per_qos[active_res_account].has_key(jobs_dict[reservation['job_runID']]['QOS']) :
                        active_reserved_node_sec_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']] = active_reserved_node_sec_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']] + jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit']
                    else :
                        active_reserved_node_sec_per_account_per_qos[active_res_account][jobs_dict[reservation['job_runID']]['QOS']] = jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit']
                else :
                    active_reserved_node_sec_per_account_per_qos[active_res_account] = { jobs_dict[reservation['job_runID']]['QOS'] : jobs_dict[reservation['job_runID']]['resource_amount_int'] * jobs_dict[reservation['job_runID']]['wall_clock_limit'], }
    for job_step in eligible_jobs :
        if RESERVATION_DEPTH != None and reservations_created >= RESERVATION_DEPTH :
            break
        if MAXJOBPERUSERPOLICY != None and \
           MAXJOBPERUSERPOLICY == 'ON' and \
          ( running_jobs_per_user.has_key(job_step['user']) and \
          running_jobs_per_user[job_step['user']] >= \
          MAXJOBPERUSERCOUNT or \
          active_reservations_per_user.has_key(job_step['user']) and \
          len(active_reservations_per_user[job_step['user']]) >= \
          MAXJOBPERUSERCOUNT ) :
            update_object_attribute(
              'ineligible_reason',
              'MAXJOBPERUSERPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        if QOS_MAXJOBPERUSERPOLICY_dict.has_key(job_step['QOS']) and \
           QOS_MAXJOBPERUSERPOLICY_dict[job_step['QOS']] != None and \
          ( running_jobs_per_user.has_key(job_step['user']) and \
          running_jobs_per_user[job_step['user']] >= \
          QOS_MAXJOBPERUSERPOLICY_dict[job_step['QOS']] or \
          active_reservations_per_user.has_key(job_step['user']) and \
          len(active_reservations_per_user[job_step['user']]) >= \
          QOS_MAXJOBPERUSERPOLICY_dict[job_step['QOS']] ) :
            #print "policy violation (%s) (%s) (%s)" % (job_step, active_reservations_per_user, QOS_MAXJOBPERUSERPOLICY_dict)
            update_object_attribute(
              'ineligible_reason',
              'QOSMAXJOBPERUSERPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        if QOS_MAXJOBPERACCOUNTPOLICY_dict.has_key(job_step['QOS']) and \
           QOS_MAXJOBPERACCOUNTPOLICY_dict[job_step['QOS']] != None and \
          ( running_jobs_per_account_per_qos.has_key(job_step['account']) and \
          running_jobs_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) and \
          running_jobs_per_account_per_qos[job_step['account']][job_step['QOS']] >= \
          QOS_MAXJOBPERACCOUNTPOLICY_dict[job_step['QOS']] or \
          active_reservations_per_account_per_qos.has_key(job_step['account']) and \
          active_reservations_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) and \
          len(active_reservations_per_account_per_qos[job_step['account']][job_step['QOS']]) >= \
          QOS_MAXJOBPERACCOUNTPOLICY_dict[job_step['QOS']] ) :
            update_object_attribute(
              'ineligible_reason',
              'MAXJOBPERACCOUNTPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXJOBPERUSERPERQOSPOLICY_dict') and \
           QOS_MAXJOBPERUSERPERQOSPOLICY_dict.has_key(job_step['QOS']) and \
           QOS_MAXJOBPERUSERPERQOSPOLICY_dict[job_step['QOS']] != None and \
          ( running_jobs_per_user_per_qos.has_key(job_step['user']) and \
          running_jobs_per_user_per_qos[job_step['user']].has_key(job_step['QOS']) and \
          running_jobs_per_user_per_qos[job_step['user']][job_step['QOS']] >= \
          QOS_MAXJOBPERUSERPERQOSPOLICY_dict[job_step['QOS']] or \
          active_reservations_per_user_per_qos.has_key(job_step['user']) and \
          active_reservations_per_user_per_qos[job_step['user']].has_key(job_step['QOS']) and \
          len(active_reservations_per_user_per_qos[job_step['user']][job_step['QOS']]) >= \
          QOS_MAXJOBPERUSERPERQOSPOLICY_dict[job_step['QOS']] ) :
            update_object_attribute(
              'ineligible_reason',
              'QOS_MAXJOBPERUSERPERQOSPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict') and \
           QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict.has_key(job_step['QOS']) and \
           QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job_step['QOS']] != None and \
          ( running_node_sec_per_user_per_qos.has_key(job_step['user']) and \
          running_node_sec_per_user_per_qos[job_step['user']].has_key(job_step['QOS']) and \
          running_node_sec_per_user_per_qos[job_step['user']][job_step['QOS']] >= \
          QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job_step['QOS']] or \
          active_reserved_node_sec_per_user_per_qos.has_key(job_step['user']) and \
          active_reserved_node_sec_per_user_per_qos[job_step['user']].has_key(job_step['QOS']) and \
          active_reserved_node_sec_per_user_per_qos[job_step['user']][job_step['QOS']] >= QOS_MAXNODESECRUNNINGPERUSERPOLICY_dict[job_step['QOS']] ) :
            update_object_attribute(
              'ineligible_reason',
              'QOSMAXNODESECRUNNINGPERUSERPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        if sys.__dict__['modules']['Catalina'].__dict__.has_key('QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict') and \
           QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict.has_key(job_step['QOS']) and \
           QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job_step['QOS']] != None and \
          ( running_node_sec_per_account_per_qos.has_key(job_step['account']) and \
          running_node_sec_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) and \
          running_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] >= \
          QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job_step['QOS']] or \
          active_reserved_node_sec_per_account_per_qos.has_key(job_step['account']) and \
          active_reserved_node_sec_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) and \
          active_reserved_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] >= QOS_MAXNODESECRUNNINGPERACCOUNTPOLICY_dict[job_step['QOS']] ) :
            update_object_attribute(
              'ineligible_reason',
              'QOSMAXNODESECRUNNINGPERACCOUNTPOLICY',
              job_step,
              jobs_db_handle
              )
            update_object_attribute(
              'system_queue_time',
              None,
              job_step,
              jobs_db_handle
              )
            continue
        try :
            for reservation in existing_reservations :
                if reservation['purpose_type_string'] == 'job' and \
                  reservation.has_key('job_runID') and \
                  reservation['job_runID'] == job_step['name'] :
                    raise FoundJob()
        except FoundJob:
            continue
        try :
            start_job_time = time.time()
            new_res = create_job_reservation(
              resources_db_handle=resources_db_handle,
              reservations_db_handle=reservations_db_handle,
              jobs_db_handle=jobs_db_handle,
              job_step=job_step,
              events_db_handle=events_db_handle
            )
            if job_step.has_key('license_request_list'):
                license_request_list = job_step['license_request_list']
            else:
                license_request_list = None
            print "job_step['maxhops'] (%s) job_step['license_request_list'] (%s) job_step['req_dict_list'] (%s)" % (job_step['maxhops'], license_request_list, job_step['req_dict_list'])
            print "job elapsed (%s)" % (time.time() - start_job_time,)
            if new_res == None:
                raise ResIsNone()
            if new_res['start_time_float'] == None:
                raise NoResStartTime()
            if (Now_float + 1) >= new_res['start_time_float'] and \
              (Now_float - new_res['start_time_float']) <= FUDGE_FACTOR/2 :
                if running_jobs_per_user.has_key(job_step['user']) :
                    running_jobs_per_user[job_step['user']] = running_jobs_per_user[job_step['user']] + 1
                else :
                    running_jobs_per_user[job_step['user']] = 1
                if running_jobs_per_account_per_qos.has_key(job_step['account']) :
                    if running_jobs_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) :
                        running_jobs_per_account_per_qos[job_step['account']][job_step['QOS']] = running_jobs_per_account_per_qos[job_step['account']][job_step['QOS']] + 1
                    else :
                        running_jobs_per_account_per_qos[job_step['account']][job_step['QOS']] = 1
                else :
                    running_jobs_per_account_per_qos[job_step['account']] = { job_step['QOS'] : 1 }
                if running_node_sec_per_account_per_qos.has_key(job_step['account']) :
                    if running_node_sec_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) :
                        running_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] = running_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] + job_step['resource_amount_int'] * job_step['wall_clock_limit']
                    else :
                        running_node_sec_per_account_per_qos[job_step['account']][job_step['QOS']] = job_step['resource_amount_int'] * job_step['wall_clock_limit']
                else :
                    running_node_sec_per_account_per_qos[job_step['account']] = { job_step['QOS'] : job_step['resource_amount_int'] * job_step['wall_clock_limit'] }
                if active_reservations_per_user.has_key(job_step['user']) :
                    if not job_step['name'] in active_reservations_per_user[job_step['user']] :
                        active_reservations_per_user[job_step['user']].append(job_step['name'])
                else :
                    active_reservations_per_user[job_step['user']] = [job_step['name'],]
                if active_reservations_per_account_per_qos.has_key(job_step['account']) :
                    if active_reservations_per_account_per_qos[job_step['account']].has_key(job_step['QOS']) :
                        if not job_step['name'] in active_reservations_per_account_per_qos[job_step['account']][job_step['QOS']] :
                            active_reservations_per_account_per_qos[job_step['account']][job_step['QOS']].append(job_step['name'])
                    else :
                        active_reservations_per_account_per_qos[job_step['account']][job_step['QOS']] = [job_step['name'],]
                else :
                    active_reservations_per_account_per_qos[job_step['account']] = { job_step['QOS'] : [job_step['name'],], }
        except InsufficientNodes, exc:
            message = "job reservation (%s) failed for runID (%s) due to insufficient resources" % ( exc.res['name'], exc.res['job_runID'] )
            print message
            catsyslog(message,'notice')
        except :
            try :
                info_tuple = sys.exc_info()
                print "unspecified exception (%s) (%s) (%s)" % info_tuple
                traceback.print_tb(info_tuple[2])
            except :
                print "print of sys.exc_info() failed"
        else :
            reservations_created = reservations_created + 1

def set_system_priority(job_step, system_priority_int, jobs_db_handle) :
    if system_priority_int == 0 :
        update_object_attribute('system_priority_int', None, job_step, jobs_db_handle)
        update_object_attribute('system_priority_mark_string', None, job_step, jobs_db_handle)
    else :
        update_object_attribute('system_priority_int', system_priority_int, job_step, jobs_db_handle)
        update_object_attribute('system_priority_mark_string', '*', job_step, jobs_db_handle)

def set_resource_usability(resources_db_handle, jobs_db_handle) :
    resources_dict = resources_db_handle[0]
    jobs_dict = jobs_db_handle[0]
    for resource_name in resources_dict.keys() :
        node_seconds_int = 0
        for job_name in jobs_dict.keys() :
            if jobs_dict[job_name].has_key('resource_dict_list') :
                for element in jobs_dict[job_name]['resource_dict_list'] :
                    if element['resource_dict'].has_key(resource_name) :
                        node_seconds_int = node_seconds_int + long(element['amount_int']) * long(jobs_dict[job_name]['wall_clock_limit'])
        update_object_attribute('resource_usability_int', node_seconds_int, resources_dict[resource_name], resources_db_handle)
        if DEBUGJOB != None :
            print "%s resource_usability (%s)" % (resource_name, node_seconds_int)

def update_resources(resources_db_handle, cfg_resources_db_handle,jobs_db_handle) :
    #print "start update_resources"
    resource_shelf = resources_db_handle[0]
    old_resources = get_object_list(resources_db_handle)
    old_resources_names = get_object_names_list(old_resources)
    resource_list = Catalina____RESOURCEMANAGER_PLACEHOLDER___.get_resources_list()
    if LICENSENODE != None:
        new_resource = Catalina____RESOURCEMANAGER_PLACEHOLDER___.initialize_resource(LICENSENODE)
        new_resource['Machine'] = LICENSENODE
        new_resource['name'] = LICENSENODE
        new_resource['State'] = 'Idle'
        new_resource['consumable_dict'] = {}
        new_resource['ConfiguredClasses_list'] = 'license'
        new_resource['properties_list'] = ['license']
        for licensepackage in LICENSEDICT.keys():
            new_resource['consumable_dict'][licensepackage] = int(LICENSEDICT[licensepackage])
            new_resource['properties_list'].append(licensepackage)
        resource_list.append(new_resource)
    cfg_resources_shelf = cfg_resources_db_handle[0]
    cfg_resources_list = Catalina____RESOURCEMANAGER_PLACEHOLDER___.get_configured_resources_list(resources_db_handle)
    cfg_resources_names = get_object_names_list(cfg_resources_list)
    found_resource_dict = {}
    found_resource_names = []
    for resource in resource_list :
        found_resource_dict[resource['name']] = resource
        found_resource_names.append(resource['name'])
        if resource_shelf.has_key(resource['name']) :
            for key in resource.keys() :
                #print "checking resource key (%s) (%s)" % (key, resource[key])
                # This is kind of convoluted...
                if key == 'State_Change_Time' :
                    continue
                if key == 'State' :
                    if resource['State'] == 'Down':
                        #print "Down state for (%s)" % resource['name']
                        # Delay before considering a node really Down.
                        if resource['speculative_state'] != resource_shelf[resource['name']]['speculative_state'] :
                            #print "New Down state for (%s)!" % resource['name']
                            update_object_attribute('State_Change_Time', Now_float, resource_shelf[resource['name']], resources_db_handle)
                            update_object_attribute('speculative_state', resource[key], resource_shelf[resource['name']], resources_db_handle)
                        else :
                            #print "Old Down state for (%s)" % resource['name']
                            # node has reported Down again
                            if Now_float - resource_shelf[resource['name']]['State_Change_Time'] > RESOURCE_DOWN_TIME_LIMIT :
                                print "RESOURCE_DOWN_TIME_LIMIT exceeded for (%s)!" % resource['name']
                                update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                    else :
                        # No delay for non-Down nodes
                        if resource['State'] != resource_shelf[resource['name']]['State'] :
                            update_object_attribute('State_Change_Time', Now_float, resource_shelf[resource['name']], resources_db_handle)
                            update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                        if resource['State'] != resource_shelf[resource['name']]['speculative_state'] :
                            update_object_attribute('speculative_state', resource[key], resource_shelf[resource['name']], resources_db_handle)
                        
                    #if resource['State'] != resource_shelf[resource['name']]['State'] :
                    #    # Could put in a delay here to work around
                    #    # bogus Down nodes from LoadLeveler.
                    #    # 
                    #    update_object_attribute('State_Change_Time', Now_float, resource_shelf[resource['name']], resources_db_handle)
                    #    if resource['State'] != 'Down' :
                    #        update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                    #    else :
                    #        update_object_attribute('speculative_state', resource[key], resource_shelf[resource['name']], resources_db_handle)
                    #else :
                    #    if resource['State'] == 'Down' :
                    #        if Now_float - resource_shelf[resource['name']]['State_Change_Time'] > LOST_NODE_LIMIT :
                    #            update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                else :
                    #print "updating resource key (%s) (%s)" % (key, resource[key])
                    update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
        else :
            insert_new_object(resource, resources_db_handle)
    lost_resource_names_list = []
    down_resource_names_dict = {}
    for resource in old_resources :
        if not resource['State'] in ['Idle', 'Running'] :
            continue
        if not resource['name'] in found_resource_names :
            lost_resource_names_list.append(resource['name'])
        #elif not found_resource_dict[resource['name']]['State'] in ['Idle', 'Running'] :
        elif not found_resource_dict[resource['name']]['State'] in ['Idle', 'Running'] and resource_shelf[resource['name']]['State'] not in ['Idle','Running']:
            # put delay in here, too
            #if Now_float - resource_shelf[resource['name']]['State_Change_Time'] > RESOURCE_DOWN_TIME_LIMIT :
            #    down_resource_names_dict[resource['name']] = found_resource_dict[resource['name']]['State']
            # actually, the delay is in updating found_resource_dict state above
            # no delay here, else non-Down state changes will be missed.
            #if Now_float - resource_shelf[resource['name']]['State_Change_Time'] > RESOURCE_DOWN_TIME_LIMIT :
            #    if resource['speculative_state'] != resource_shelf[resource['name']]['speculative_state'] :
            #if Now_float - resource_shelf[resource['name']]['State_Change_Time'] > RESOURCE_DOWN_TIME_LIMIT :
            #    down_resource_names_dict[resource['name']] = found_resource_dict[resource['name']]['State']
            #print "adding down_resource (%s)" % resource['name']
            down_resource_names_dict[resource['name']] = found_resource_dict[resource['name']]['State']
    down_resource_names_list = map(lambda x, down_resource_names_dict=down_resource_names_dict : "%s - %s" % (x, down_resource_names_dict[x]), down_resource_names_dict.keys())

    #Martin Margo: Just a hack for now, remove the broken nodes from the job eligible nodes
    # KKY: took this out, so that there would be a delay for transiently
    # down nodes.
    # Should probably put the delay down here too.
    filter_out_down_nodes( down_resource_names_dict.keys() + lost_resource_names_list, jobs_db_handle)

    if len(lost_resource_names_list) > 0 or len(down_resource_names_list) > 0 :
        recipient = MAIL_RECIPIENT
        subject = "Available node count has decreased"
        message = """The following nodes are not found: 
%s

The following nodes are no longer in Idle or Running state:
%s""" % (lost_resource_names_list, down_resource_names_list)
        warn(message, subject, recipient)
        catsyslog(message,'notice')
    for resource in cfg_resources_list :
        if not resource['name'] in found_resource_names :
            if resource['name'] in resource_shelf.keys() :
                for key in resource.keys() :
                    update_object_attribute(key, resource[key], resource_shelf[resource['name']], resources_db_handle)
                if resource_shelf[resource['name']]['State'] != None :
                    update_object_attribute('State', None,
                      resource_shelf[resource['name']], resources_db_handle)
                    update_object_attribute('State_Change_Time', Now_float,
                      resource_shelf[resource['name']], resources_db_handle)
            else :
                insert_new_object(resource, resources_db_handle)
    for resource_name in old_resources_names :
        if resource_name not in cfg_resources_names and \
          resource_name not in found_resource_names :
            delete_object(resource_name, resources_db_handle)
    if LICENSENODE != None:
        #print "LICENSENODE (%s)" % (LICENSENODE,)
        new_resource = Catalina____RESOURCEMANAGER_PLACEHOLDER___.initialize_resource(LICENSENODE)
        new_resource['Machine'] = LICENSENODE
        new_resource['name'] = LICENSENODE
        new_resource['State'] = 'Idle'
        new_resource['consumable_dict'] = {}
        new_resource['ConfiguredClasses_list'] = 'license'
        new_resource['properties_list'] = ['license']
        for licensepackage in LICENSEDICT.keys():
            new_resource['consumable_dict'][licensepackage] = int(LICENSEDICT[licensepackage])
            new_resource['properties_list'].append(licensepackage)
        if not resource_shelf.has_key(new_resource['Machine']) :
            #print " inserting (%s)" % (new_resource,)
            insert_new_object(new_resource, resources_db_handle)
        else:
            #print " updating (%s)" % (new_resource,)
            update_object_attributes(new_resource, new_resource, resources_db_handle)
        #print "fresh insert (%s)" % (resources_db_handle[0][new_resource['name']],)
    #else:
        #print "LICENSENODE is None"

def update_max_pushback(jobs_db_handle, reservations_db_handle, resources_db_handle) :
    # update 'max_pushback' for each preemptible job
    # create a dictionary, with nodenames as keys.  Put in a list
    # of non-job reservations for each nodename
    # for each running, preemptible job reservation, for each node allocated to
    # that job, see if there are any non-job reservations.  Find the earliest
    # one, and set max_pushback to the start of the earliest non-job
    # reservation on any of those nodes.
    jobs_dict = jobs_db_handle[0]
    reservations_dict = reservations_db_handle[0]
    resources_dict = resources_db_handle[0]
    node_res_dict = {}
    for res_key in reservations_dict.keys() :
        #if reservations_dict[res_key]['purpose_type_string'] == 'job' :
        #    continue
        for allocated_dict in reservations_dict[res_key]['allocated_dict_list'] :
            if node_res_dict.has_key(allocated_dict['nodename']) :
                node_res_dict[allocated_dict['nodename']].append(copy.deepcopy(reservations_dict[res_key]))
            else :
                node_res_dict[allocated_dict['nodename']] = [copy.deepcopy(reservations_dict[res_key]),]
    for node_key in node_res_dict.keys() :
        # create an earliest-to-latest event list for each node
        event_list = []
        for res in node_res_dict[node_key] :
            node_resource_list = filter(lambda x : x.has_key('nodename') and x['nodename'] == node_key, res['allocated_dict_list'])
            event_list.append((res['start_time_float'],
                              'decrement',
                              copy.deepcopy(node_resource_list)
                             ))
            event_list.append((res['end_time_float'],
                              'add',
                              copy.deepcopy(node_resource_list)
                             ))
        temp_list = []
        for event in event_list :
            temp_list.append((event[0], event))
        temp_list.sort()
        new_list = map(lambda x : x[1], temp_list)
                              
        node_res_dict[node_key] = new_list
    
    for res_key in reservations_dict.keys() :
        if reservations_dict[res_key]['purpose_type_string'] != 'running' \
          or jobs_dict[reservations_dict[res_key]['job_runID']]['preemptible'] == None \
          or jobs_dict[reservations_dict[res_key]['job_runID']]['preemptible'] < 1 :
            update_object_attribute('max_pushback_float', Now_float, reservations_dict[res_key], reservations_db_handle)
            continue
        max_pushback = END_OF_SCHEDULING
        for allocated_dict in reservations_dict[res_key]['allocated_dict_list'] :
            #allocated_node = 0
            #allocated_cpu = 0
            #allocated_memory = 0
            allocated_resource_dict = {'node' : 0}
            for rkey in resources_dict[allocated_dict['nodename']]['consumable_dict'].keys():
                allocated_resource_dict[rkey] = 0
            #required_node_node = allocated_dict['node']
            #required_node_cpu = allocated_dict['cpu']
            #required_node_memory = allocated_dict['memory']
            #required_dict = {}
            #for rkey in allocated_dict.keys():
            #    required_dict[rkey] = allocated_dict[rkey]
            #configured_node_node = 1
            #configured_node_cpu = resources_dict[allocated_dict['nodename']]['ConsumableCpus']
            #configured_node_memory = resources_dict[allocated_dict['nodename']]['ConsumableMemory']
            configured_node_dict = {'node' : 1}
            for rkey in resources_dict[allocated_dict['nodename']]['consumable_dict'].keys():
                configured_node_dict[rkey] = resources_dict[allocated_dict['nodename']]['consumable_dict'][rkey]
            if node_res_dict.has_key(allocated_dict['nodename']) :
                for event in node_res_dict[allocated_dict['nodename']] :
                    if max_pushback < event[0] :
                        break
                    #event_node = event[2]['node']
                    #event_cpu = event[2]['cpu']
                    #event_memory = event[2]['memory']
                    #event_dict = {}
                    #for rkey in event[2].keys():
                    #    event_dict[rkey] = event[2][rkey]
                    if event[1] == 'decrement' :
                        allocated_resource_dict['node'] = allocated_resource_dict['node'] + event[2]['node']
                        allocated_resource_dict['cpu'] = allocated_resource_dict['cpu ']+ event[2]['cpu']
                        allocated_resource_dict['memory'] = allocated__resource_dict['memory'] + event[2]['memory']
                    elif event[1] == 'add' :
                        allocated_resource_dict['node'] = allocated_resource_dict['node'] - event[2]['node']
                        allocated_resource_dict['cpu'] = allocated_resource_dict['cpu'] - event[2]['cpu']
                        allocated_resource_dict['memory'] = allocated_resource_dict['memory'] - event[2]['memory']
                    underconfigured = 0
                    for rkey in configured_node_dict.keys():
                        if configured_node_dict[rkey] < allocated_resource_dict[rkey]:
                            underconfigured = 1
                    #if event[2]['type'] == 'node_exclusive' \
                    #  or configured_node_node < allocated_resource_dict['node'] \
                    #  or configured_node_cpu < allocated_resource_dict['cpu'] \
                    #  or configured_node_memory < allocated_resource_dict['memory'] :
                    if event[2]['type'] == 'node_exclusive' or underconfigured == 1:
                        max_pushback = event[0] - 1
                        break
        update_object_attribute('max_pushback_float', max_pushback, reservations_dict[res_key], reservations_db_handle)

def schedule_jobs(events_db_handle, jobs_db_handle, resources_db_handle, reservations_db_handle, cfg_resources_db_handle, standing_reservations_db_handle) :
    # the jobs and reservations handles should be write, the resources handle
    # can be read
    # update speculative_system_queue_time for each job
    #  - if system_queue_time = None : speculative_system_queue_time = Now_float
    #    else : speculative_system_queue_time = system_queue_time
    # update priority for all jobs
    # screen jobs through policies.  Set new system_queue_time
    # for newly eligible jobs.  Set system_queue_time to None for ineligible
    # jobs
    # create new job reservations, in priority order
    print "update_resources and filter out bad nodes from job list"
    update_resources(resources_db_handle, cfg_resources_db_handle,jobs_db_handle)
    print "update_job_info"
    update_job_info(jobs_db_handle)
    print "cancel_overrun_jobs"
    cancel_overrun_jobs(events_db_handle, jobs_db_handle)
    print "cancel_bad_jobs"
    Catalina____RESOURCEMANAGER_PLACEHOLDER___.cancel_bad_jobs(jobs_db_handle, resources_db_handle, events_db_handle)
    print "update_job_resource_lists"
    update_job_resource_lists(jobs_db_handle, resources_db_handle)
    print "update_resource_usability"
    set_resource_usability(resources_db_handle, jobs_db_handle)
    print "update_job_speculative_system_queue_time"
    update_job_speculative_system_queue_time(jobs_db_handle)
    print "update_job_priorities"
    update_job_priorities(jobs_db_handle)
    print "get_eligible_and_running_jobs"
    (eligible_jobs, runningstarting_jobs) = get_eligible_and_running_jobs(jobs_db_handle,
        resources_db_handle, reservations_db_handle)
    print "update_running_reservations"
    update_running_reservations(runningstarting_jobs, reservations_db_handle, resources_db_handle, jobs_db_handle, events_db_handle)
    print "updating standing reservations"
    update_standing_reservations(
      events_db_handle=events_db_handle,
      jobs_db_handle=jobs_db_handle,
      resources_db_handle=resources_db_handle,
      reservations_db_handle=reservations_db_handle,
      standing_reservations_db_handle=standing_reservations_db_handle)
    print "migrating shortpools"
    migrate_shortpools(jobs_db_handle, resources_db_handle, reservations_db_handle)
    #print "update_max_pushback"
    #update_max_pushback(jobs_db_handle, reservations_db_handle, resources_db_handle)
    #if Catalina____RESOURCEMANAGER_PLACEHOLDER___.__dict__.has_key('cleanup_job_starts'):
    if sys.__dict__['modules']['Catalina____RESOURCEMANAGER_PLACEHOLDER___'].__dict__.has_key('cleanup_job_starts') :
        print "cleanup_job_starts"
        Catalina____RESOURCEMANAGER_PLACEHOLDER___.cleanup_job_starts(jobs_db_handle=jobs_db_handle, resources_db_handle=resources_db_handle, reservations_db_handle=reservations_db_handle)
    else:
        print "no cleanup_job_starts in dict "
        
    print "create_job_reservations"
    create_job_reservations(eligible_jobs, resources_db_handle,
        reservations_db_handle, jobs_db_handle, events_db_handle)
    print "cancel run-at-risk jobs"
    #cancel_risk_jobs(reservations_db_handle=reservations_db_handle,
    #                 jobs_db_handle=jobs_db_handle,
    #                 events_db_handle=events_db_handle
    #                 resources_db_handle=resources_db_handle)
    cancel_risk_reservations(reservations_db_handle=reservations_db_handle,
                     jobs_db_handle=jobs_db_handle,
                     events_db_handle=events_db_handle,
                     resources_db_handle=resources_db_handle)
    print "run_jobs"
    Catalina____RESOURCEMANAGER_PLACEHOLDER___.run_jobs(events_db_handle, jobs_db_handle, resources_db_handle, reservations_db_handle)

# To implement multiple resource scheduling,
# - get the multiple requirement spec from the RM
#   like PBS: tg-c002+16:ppn=1:compute
#   this should be req_list = [{tasks : <ppn>, req_code : <requirements code>}...]
# - in get_resource_list, use this to set the jobs resource_list_list,
#   a list of screened resources for each element of req_list
# - in create_reservation, run through get_screened_nodes, get_open_windows_list,
#   get_sized_windows_list, get_sorted_windows_list, get_chosen_nodes_list
#   and accompanying code once for each req_list element.
# - consolidate the several reservation windows into one.
# Naah, this won't work.  the separate reservations need to by synced
# - get the multiple requirement spec from the RM
#   like PBS: tg-c002+16:ppn=1:compute
#   this should be req_list = [{'tasks' : <ppn>, 'req_code' : <requirements code>}...]
# - in get_resource_list, use this to set the jobs resource_list_dict,
#   a dict of screened resources for each element of req_list and the
#   number of nodes for that element:
#   resource_list_dict = [{'amount' : <number of nodes for that element>,
#                          'resource_list' : <list of node names for that element>},
#                         ...
#                         ]
#   sort the list in increasing length of resource_list.
# - in get_sized_windows_list, use resource_list_dict to search for
#   a set of nodes filling the amount requirement for each element
#   of req_list.
# - in get_chosen_nodes_list, select nodes according to the amounts
#   in resource_list_dict
def create_reservation(
  resources_db_handle=None,
  reservations_db_handle=None,
  jobs_db_handle=None,
  old_res_id=None,
  earliest_start_float=None,
  latest_end_float=None,
  duration_float=None,
  latency_float=None,
  resource_amount_int=None,
  resource_amount_requested_int=None,
  requested_resource_list=[],
  job_restriction='result = 1',
  node_restriction=None,
  node_usage='node_exclusive',
  resource_dict_list=None,
  node_sort_policy=None,
  conflict_policy=None,
  affinity_calculation=None,
  purpose_type_string='generic',
  account_string=None,
  creator_string=username_string,
  job_runID=None,
  job_step=None,
  ignore_reservations_list=None,
  blocking_reservations_list=[],
  accepted_nodes_list=None,
  comment_string=None,
  notify_string=None,
  mode='lookahead',
  maxhops=None,
  max_resource_int=None
) :
    if duration_float == None :
        raise 'NoDuration'
    if duration_float < 0 :
        raise 'NegativeDuration'
    if resource_amount_int == None and earliest_start_float == None :
        raise 'MissingAmountAndStart'
    if earliest_start_float != None and earliest_start_float < 0 :
        raise 'NegativeStart'
    if latest_end_float != None and latest_end_float < 0 :
        raise 'NegativeEnd'
    if earliest_start_float != None and \
      latest_end_float != None and \
      earliest_start_float > latest_end_float :
        raise 'StartAfterEnd'
    if resource_amount_int != None and max_resource_int != None and \
      resource_amount_int > max_resource_int :
        raise 'RequestGTMax'
    if purpose_type_string == 'user_set' and account_string == None :
        raise 'NoAccount'
    if job_restriction == None :
        job_restriction = 'result = 1'

    existing_reservations = get_object_list(reservations_db_handle)

    # If this is not a job reservation, delete all job reservations
    # so that the
    # runjob routine doesn't start jobs within the new reservation
    # due to an old overlapping job reservation
    # The reservations db should be write-locked, so the runjobs
    # routine won't run while this is going on...this would need to
    # be changed if the db went to a non-locking implementation...
    # Do not delete job reservations here.  Let create_job_reservations
    # do it.  To prevent Catalina____RESOURCEMANAGER_PLACEHOLDER___.run_jobs from running old reservations,
    # do not run Catalina____RESOURCEMANAGER_PLACEHOLDER___.run_jobs, except within schedule_jobs, when the
    # reservations db is locked.

    if old_res_id != None :
        saved_reservations = []
        for reservation in existing_reservations :
            if reservation['name'] != old_res_id :
                saved_reservations.append(reservation)
        existing_reservations = saved_reservations
    # Filter out any reservations named in ignore_reservations_list
    if ignore_reservations_list != None :
        filtered_reservations = filter(
          lambda reservation, list=ignore_reservations_list :
              not reservation['name'] in list,
              existing_reservations )
        existing_reservations = filtered_reservations

    if job_step != None :
        submit_time = job_step['SubmitTime']
        reservation_name = `int(submit_time)`
        #if job_step.has_key('node_usage') :
            #node_usage = job_step['node_usage']
        while reservations_db_handle[0].has_key(reservation_name) :
            submit_time = submit_time + 1
            reservation_name = `int(submit_time)`
    elif purpose_type_string == 'standing_reservation' :
        name_start_time = earliest_start_float
        reservation_name = `int(name_start_time)`
        while reservations_db_handle[0].has_key(reservation_name) :
            name_start_time = name_start_time + 1
            reservation_name = `int(name_start_time)`
    else :
        reservation_name = get_new_db_key(reservations_db_handle)
    new_res = initialize_reservation(reservation_name)
    new_res['job_restriction'] = 'result = 1'
    new_res['purpose_type_string'] = 'unknown'
    new_res['earliest_start_float'] = earliest_start_float
    new_res['latest_end_float'] = latest_end_float
    new_res['duration_float'] = duration_float
    new_res['latency_float'] = latency_float
    new_res['resource_amount_int'] = resource_amount_int
    new_res['resource_amount_requested_int'] = resource_amount_int
    new_res['requested_resource_list'] = requested_resource_list
    new_res['max_resource_int'] = max_resource_int
    new_res['purpose_type_string'] = purpose_type_string
    new_res['comment_string'] = comment_string
    new_res['account_string'] = account_string
    new_res['creator_string'] = creator_string
    new_res['conflict_policy'] = conflict_policy
    new_res['node_restriction'] = node_restriction
    new_res['node_sort_policy'] = node_sort_policy
    new_res['node_usage'] = node_usage
    new_res['affinity_calculation'] = affinity_calculation
    new_res['job_restriction'] = job_restriction
    new_res['job_runID'] = job_runID
    new_res['job_step'] = job_step
    new_res['notify_string'] = notify_string
    new_res['maxhops'] = maxhops
    new_res['start_count_int'] = 0

    # dummy values, so that we can re-enter this function and
    # not overwrite an outer loop reservation_name.  insert
    # will actually create a placeholder reservation, job_rest
    # should be correct.
    #if re.match(r"^temp_pushback_for", purpose_type_string) :
    new_res['start_time_float'] = earliest_start_float
    new_res['end_time_float'] = latest_end_float
    new_res['node_list'] = []
    new_res['allocated_dict_list'] = []
    insert_new_object_with_key(reservation_name, new_res, reservations_db_handle
)


    if accepted_nodes_list == None :
        accepted_nodes_list = get_accepted_nodes_list(node_restriction, resources_db_handle)
    if resource_dict_list == None :
        if purpose_type_string in ['job', 'preempted_job'] :
            resource_dict_list = job_step['resource_dict_list']
            #print "resource_dict_list (%s)" % (resource_dict_list,)
        else :
            resource_dict_list = [{
              'amount_int' : resource_amount_int,
              'resource_dict' : {}
              }]
            for accepted_node in accepted_nodes_list :
                #resource_dict_list[0]['resource_dict'][accepted_node] = resources_db_handle[0][accepted_node]
                resource_dict_list[0]['resource_dict'][accepted_node] = {}
    new_res['requested_resource_list'] = requested_resource_list
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "len(accepted_nodes_list) (%s)" % len(accepted_nodes_list)
    # set jobs_dict
    if jobs_db_handle != None :
        jobs_dict = jobs_db_handle[0]
    else :
        raise "NoJobsDict"
    #if purpose_type_string in ['job','preempted_job'] :
    if purpose_type_string in ['job', 'temp_pushback'] or re.match(r"^temp_pushback_for", purpose_type_string) :
        if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
            print "doing job (%s)" % job_runID
        # filter accepted_nodes_list with resource_dict_list[0]['resource_dict']
        old_accepted_nodes_list = copy.copy(accepted_nodes_list)
        accepted_nodes_list = []
        for old_node in old_accepted_nodes_list:
            #print "doing old_node (%s)" % (old_node,)
            for resource_dict in resource_dict_list:
                #print "doing resource_dict (%s)" % (resource_dict,)
                if resource_dict['resource_dict'].has_key(old_node) and not old_node in accepted_nodes_list:
                    accepted_nodes_list.append(old_node)
                #else:
                #    print "%s not in resouce_dict (%s)" % (old_node, resource_dict['resource_dict'])
        #if LICENSENODE != None and job_step.has_key('license_request_list') and job_step['license_request_list'] != None and len(job_step['license_request_list']) > 0:
        #    accepted_nodes_list.append(LICENSENODE)
        #if 'flexnode' in accepted_nodes_list:
        #    print "flexnode in accepted_nodes_list"
        #else:
        #    print "no flexnode in accepted_nodes_list"
        #accepted_nodes_list = filter(lambda x,resource_dict_list=resource_dict_list : resource_dict_list[0]['resource_dict'].has_key(x), old_accepted_nodes_list)
        # If the job is bound to a list of reservations, meaning
        # it is only supposed to run in that list, filter
        # accepted nodes list with that set of reservations
        # How should jobs be bound to cpu and memory?  You don't
        # want jobs running on the other cpus or memory of a node...
        # A reservation can request just nodes, or nodes with cpus/memory
        # A job bound to the reservation should be bound to either just
        # the node or to the node and the requested cpus/memory...
        #print "bound_debug job_step (%s)" % (job_step,)
        if job_step['reservation_binding'] != None \
          and job_step['reservation_binding'] != [] :
            bound_nodes_list = []
            bound_nodes_dict = {}
            bound_reservations_list = []
            for reservation_name in job_step['reservation_binding'] :
                try :
                    bound_reservation = get_object(reservation_name, reservations_db_handle)
                except :
                    continue
                else :
                    bound_reservations_list.append(bound_reservation)
            for reservation in bound_reservations_list :
                bound_res_node_list = reservation['node_list']
                if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
                    print "for reservation (%s) bound_res_node_list (%s)" % (reservation['name'], bound_res_node_list)
                #print "bound_debug for reservation (%s) bound_res_node_list (%s)" % (reservation['name'], bound_res_node_list)
                # what if reservation has no allocated_dict_list?
                # FIXME get rid of cpu and memory, use consumable_dict instead
                if reservation.has_key('allocated_dict_list') :
                    allocated_dict_list = reservation['allocated_dict_list']
                else :
                    allocated_dict_list = []
                    for nodename in reservation['node_list'] :
                        #allocated_resource = {'nodename' : nodename,
                        #                      'type' : 'node_exclusive',
                        #                      'cpu' : resources_db_handle[0][nodename]['ConsumableCpus'],
                        #                      'memory' : resources_db_handle[0][nodename]['ConsumableMemory']
                        #                      }
                        allocated_resource = {'nodename' : nodename,
                                              'type' : 'node_exclusive',
                                              }
                        consumable_list = resources_db_handle[0][nodename]['consumable_dict'].keys()
                        for consumable in consumable_list:
                            if allocated_resource.has_key(consumable):
                                allocated_resource[consumable] = allocated_resource[consumable] + resources_db_handle[0][nodename]['consumable_dict'][consumable]
                            else:
                                allocated_resource[consumable] = resources_db_handle[0][nodename]['consumable_dict'][consumable]
                        allocated_dict_list.append(allocated_resource)
                for allocated_resource in allocated_dict_list :
                    # {'node': 1, 'nodename': 'gcn-14-74', 'initiatormap_index': 0, 'memory': 67639459840, 'type': 'node_exclusive', 'cpu': 16}
                    nodename = allocated_resource['nodename']
                    if bound_nodes_dict.has_key(nodename) :
                        #bound_nodes_dict[nodename]['cpu'] = \
                        #  bound_nodes_dict[nodename]['cpu'] + \
                        #    allocated_resource['cpu']
                        #bound_nodes_dict[nodename]['memory'] = \
                        #  bound_nodes_dict[nodename]['memory'] + \
                        #    allocated_resource['memory']
                        allocated_key_list = allocated_resource.keys()
                        for allocated_key in allocated_key_list:
                            if allocated_key in ['node','nodename','type','initiatormap_index']:
                                continue
                            bound_nodes_dict[nodename][allocated_key] = \
                              bound_nodes_dict[nodename][allocated_key] + \
                                allocated_resource[allocated_key]
                    else :
                        bound_nodes_dict[nodename] = {}
                        #bound_nodes_dict[nodename]['cpu'] = \
                        #    allocated_resource['cpu']
                        #bound_nodes_dict[nodename]['memory'] = \
                        #    allocated_resource['memory']
                        allocated_key_list = allocated_resource.keys()
                        for allocated_key in allocated_key_list:
                            if allocated_key in ['node','nodename','type','initiatormap_index']:
                                continue
                            bound_nodes_dict[nodename][allocated_key] = \
                                allocated_resource[allocated_key]
                for node_name in bound_res_node_list :
                    if node_name not in bound_nodes_list :
                        bound_nodes_list.append(node_name)
                # Do I need to also assign cpu and memory here?
                # Yes, bound_nodes_list needs to be bound_nodes_dict
                # with associated cpus and memory.  Also need to allow duplicate
                # nodes to allow sums of cpus and memory...
            if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
                print "bound_nodes_list (%s)" % (bound_nodes_list,)
            accepted_nodes_list = filter(lambda node_name, bound_nodes_list=bound_nodes_list : node_name in bound_nodes_list, accepted_nodes_list)
            if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
                print "reservation_binding (%s)" % job_step['reservation_binding']
                print "after filter, len(accepted_nodes_list) (%s)" % len(accepted_nodes_list)
            # Create reservations to block off times on the bound nodes
            # to prevent the job from running at the wrong time
            temp_blocking_reservations_list = []
            new_blocking_res_number = 0
            blocking_node_list = []
            side_allocated_dict_list = []
            prepost_allocated_dict_list = []
            for nodename in bound_nodes_dict.keys() :
                # For each relevant node, create two reservations,
                # one to sit before and one to sit after all bound
                # reservations on the node.
                # for cpu-memory scheduling, also need to create
                # a side reservation for unbound cpu/memory on the node
                # This should be one reservation, with a node_list and
                # allocated_dict_list
                #pre_blocking_res['node_list'] = [node_name,]
                blocking_node_list.append(nodename)
                side_allocated_dict_list.append(
                  { 'nodename' : nodename,
                    'type' : 'node_shared',
                    'cpu' : resources_db_handle[0][nodename]['ConsumableCpus'] - bound_nodes_dict[nodename]['cpu'],
                    'memory' : resources_db_handle[0][nodename]['ConsumableMemory'] - bound_nodes_dict[nodename]['memory']
                  } )
                # create allocated_dict_list for pre-blocking-res
                prepost_allocated_dict_list.append(
                  { 'nodename' : nodename,
                    'type' : 'node_exclusive',
                    'cpu' : resources_db_handle[0][nodename]['ConsumableCpus'],
                    'memory' : resources_db_handle[0][nodename]['ConsumableMemory']
                  } )
            new_blocking_res_number = new_blocking_res_number + 1
            pre_blocking_res_name = 'tempblock' + "%s" % new_blocking_res_number
            pre_blocking_res = initialize_reservation(pre_blocking_res_name)
            pre_blocking_res['start_time_float'] = 0.0
            pre_blocking_res['end_time_float'] = END_OF_SCHEDULING
            pre_blocking_res['allocated_dict_list'] = prepost_allocated_dict_list
            pre_blocking_res['node_list'] = blocking_node_list
            # create allocated_dict_list for post-blocking-res
            new_blocking_res_number = new_blocking_res_number + 1
            post_blocking_res_name = 'tempblock' + "%s" % new_blocking_res_number
            post_blocking_res = initialize_reservation(post_blocking_res_name)
            post_blocking_res['node_list'] = blocking_node_list
            post_blocking_res['start_time_float'] = Now_float
            post_blocking_res['end_time_float'] = END_OF_SCHEDULING
            post_blocking_res['allocated_dict_list'] = prepost_allocated_dict_list
            post_blocking_res['node_list'] = blocking_node_list
            # Do I need to create a side_blocking_res for unreserved cpus
            # and memory?  Shouldn't really need to, since cpu and
            # memory are tracked by count rather than named entity...
            for reservation in bound_reservations_list :
                # For each bound reservation, adjust the pre res
                # end time and the post res start time to reflect
                # the bound res start and end times, if they extend
                # the times.
                if reservation['start_time_float'] < pre_blocking_res['end_time_float'] :
                    pre_blocking_res['end_time_float'] = reservation['start_time_float']
                if reservation['end_time_float'] > post_blocking_res['start_time_float'] :
                    post_blocking_res['start_time_float'] = reservation['end_time_float']
            # instead of placing these kludgy temp blocking reservations,
            # can I just set earliest_start and latest_end to the start and
            # end of the blocking reservations?  This probably does not
            # handle L-shaped reservation sets properly.
            temp_blocking_reservations_list.append(pre_blocking_res)
            temp_blocking_reservations_list.append(post_blocking_res)
            if DEBUGJOB != None and job_runID != None and job_runID == DEBUGJOB :
                print "blocking reservations:"
                for blocking_reservation in temp_blocking_reservations_list :
                    print "blocking_reservation (%s, %s, %s)" % (time.asctime(time.localtime(blocking_reservation['start_time_float'])), time.asctime(time.localtime(blocking_reservation['end_time_float'])), blocking_reservation['node_list'])
        #screened_nodes = get_screened_nodes(
        #    job_step['name'], accepted_nodes_list, jobs_db_handle,
        #    resources_db_handle)
        #if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        #    print "len(screened_nodes) (%s)" % len(screened_nodes)
        # If reservation_binding is specified, use only those open_windows_list
        # appropriate for the listed reservations
        # Need to check here that job meets reservation['job_restriction'] and
        # that there are no conflicts with other jobs bound to the same reservation
        # build list of nodes with "pushback" or "preempted_job" reservations
        preempted_nodes_list = []
        for reservation in existing_reservations :
            if reservation['purpose_type_string'] in ['preempted_job', 'pushback'] :
               for node_name in reservation['node_list'] :
                   if not node_name in preempted_nodes_list :
                       preempted_nodes_list.append(node_name)
        blocking_reservations = []
        for reservation in existing_reservations :
            input_tuple = ( job_step, )
            result = apply_policy_code(reservation['job_restriction'],
                input_tuple)
            if result != 0 :
                if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
                    print "appending blocking reservation (%s)" % reservation['name']
                blockingres_appended = 0
                if job_step.has_key('run_at_risk_int') and \
                  job_step['run_at_risk_int'] >= 1 :
                    blocking_reservations.append(reservation)
                    #FIXTHIS
                    # was getting a double-block from pushback res
                    # check this kludge to make sure this does not
                    # leave a double block, nor break a pushback
                    blockingres_appended = 1
                else :
                    # why was there an instance of a job having a
                    # reservation, but not existing in the jobs dict?
                    try :
                        if reservation.has_key('job_runID') and \
                          reservation['job_runID'] != None and \
                          reservation['job_runID'] != job_step['name'] and \
                          ((jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                          jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1) or \
                          (job_step.has_key('preempting') and \
                           job_step['preempting'] >= 1 and \
                          jobs_dict[reservation['job_runID']].has_key('preemptible') and \
                          jobs_dict[reservation['job_runID']]['preemptible'] >= 1)) :
                            #temp_res = copy.deepcopy(reservation)
                            temp_res = initialize_reservation(reservation['name'] + '.temp')
                            temp_res['node_list'] = copy.deepcopy(reservation['node_list'])
                            temp_res['allocated_dict_list'] = copy.deepcopy(reservation['allocated_dict_list'])
                            temp_res['start_time_float'] = copy.copy(reservation['start_time_float'])
                            temp_res['end_time_float'] = copy.copy(reservation['end_time_float'])
                            existing_preempted_job = 0
                            if reservation['purpose_type_string'] == 'running' or \
                              (reservation['purpose_type_string'] in ['job'] and \
                              jobs_dict[reservation['job_runID']].has_key('start_count_int') and \
                              jobs_dict[reservation['job_runID']]['start_count_int'] != None and \
                              jobs_dict[reservation['job_runID']]['start_count_int'] >= 1) \
                              :
                                for node_name in reservation['node_list'] :
                                    if node_name in preempted_nodes_list :
                                        existing_preempted_job = 1
                            if ((reservation['purpose_type_string'] in ['running', ] or \
                              (reservation['purpose_type_string'] == 'job' and \
                              jobs_dict.has_key(reservation['job_runID']) and \
                              jobs_dict[reservation['job_runID']].has_key('start_count_int') and \
                              jobs_dict[reservation['job_runID']]['start_count_int'] != None and \
                              jobs_dict[reservation['job_runID']]['start_count_int'] >= 1)) and \
                              existing_preempted_job == 0) or \
                              reservation['purpose_type_string'] in ['preempted_job', 'pushback'] :
                                if job_step.has_key('preempting') and job_step['preempting'] >= 1 and jobs_dict[reservation['job_runID']].has_key('preemptible') and jobs_dict[reservation['job_runID']]['preemptible'] >= 1 :
                                    if job_step['priority'] > jobs_dict[reservation['job_runID']]['priority'] \
                                      and reservation['end_time_float'] - Now_float > PREEMPT_MIN_RUNTIME :
                                        # additional check for preemptible jobs.
                                        # if new_res is for a preempting job,
                                        # check to see if wallclock limit would
                                        # push back past max_pushback_float.
                                        # if it can fit, truncate the reservation.
                                        # if the nodes in the pushed back reservation
                                        # get used for a job, will subsequent jobs
                                        # be scheduled after truncation?  Do I
                                        # need to create the pushed back reservation?
                                        # does this change with other job reservations?
                                        # yes, I need to create the pushed-back
                                        # reservation, so that new jobs don't get
                                        # filled in.  This also means that max
                                        # pushback changes with each new job.
                                        # if the job reservation for a preemptible
                                        # job can be pushed back, then truncate it,
                                        # otherwise leave it full length.
                                        # Could do this by invoking create_reservation
                                        # to check if a reservation is possible
                                        # in the "pushed back" time, but that might
                                        # be expensive.  Perhaps it would be just
                                        # as good to check for any non-job reservations
                                        # on the same nodes, at the same time, that
                                        # would push nodes, cpus, memory over the
                                        # configured values.  To do that for each
                                        # reservation would be almost as much time
                                        # and a lot more code.
                                        #print "trying to create pushback_res of (%s) (%s) for (%s)" % (reservation['job_runID'], reservation['purpose_type_string'], job_step['name'])
                                        try :
                                            job_restriction = "if input_tuple[0]['name'] in ['%s','%s'] : result = 0" % (jobs_dict[reservation['job_runID']]['name'], new_res['job_runID'])
                                            pushback_node_restriction = "if input_tuple[0]['name'] in %s : result = 0" % (reservation['node_list'], )
                                            pushback_duration_float=jobs_dict[reservation['job_runID']]['wall_clock_limit'] - jobs_dict[reservation['job_runID']]['wall_clock_used'] + FUDGE_FACTOR
                                            if reservation['purpose_type_string'] in ['preempted_job', 'pushback'] :
                                                pushback_earliest_start_float=reservation['start_time_float'] + new_res['duration_float']
                                                pushback_latest_end_float=copy.copy(reservation['start_time_float'] + new_res['duration_float'] + jobs_dict[reservation['job_runID']]['wall_clock_limit'] - jobs_dict[reservation['job_runID']]['wall_clock_used'] + 1 + FUDGE_FACTOR)
                                            else :
                                                pushback_earliest_start_float=Now_float + new_res['duration_float']
                                                pushback_latest_end_float=copy.copy(Now_float + new_res['duration_float'] + jobs_dict[reservation['job_runID']]['wall_clock_limit'] - jobs_dict[reservation['job_runID']]['wall_clock_used'] + 1 + FUDGE_FACTOR)
                                            pushback_res = create_reservation(
                                              resources_db_handle=resources_db_handle,
                                              reservations_db_handle=reservations_db_handle,
                                              jobs_db_handle=jobs_db_handle,
                                              job_step=jobs_db_handle[0][reservation['job_runID']],
                                              mode='real',
                                              job_restriction=job_restriction,
                                              node_restriction=pushback_node_restriction,
                                              job_runID=reservation['job_runID'],
                                              ignore_reservations_list=[reservation['name']],
                                              purpose_type_string="temp_pushback_for_%s" % new_res['job_runID'],
                                              earliest_start_float=pushback_earliest_start_float,
                                              duration_float=pushback_duration_float,
                                              latest_end_float=pushback_latest_end_float,
                                              resource_amount_int = len(jobs_dict[reservation['job_runID']]['requested_resource_list']),
                                              requested_resource_list = jobs_dict[reservation['job_runID']]['requested_resource_list']
                                            )
                                        except InsufficientNodes, exc:
                                            # pushback reservation failed, so don't
                                            # try to preempt this one
                                            if DEBUGJOB != None :
                                                print "pushback of preemptible job (%s) not possible due to reservation (%s) blocking!" % (exc.res['job_runID'], reservation['name'])
                                            #print "pushback of preemptible job (%s) for (%s) not possible due to reservation (%s) blocking!" % (reservation['job_runID'], new_res['job_runID'], reservation['name'])
                                            blocking_reservations.append(reservation)
                                            blockingres_appended = 1
                                        except :
                                            #print "some other pushback res exception occurred"
                                            info_tuple = sys.exc_info()
                                            info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                                            traceback.print_tb(info_tuple[2])
                                            tb_list = traceback.format_tb(info_tuple[2])
                                            info_list = info_list + tb_list
                                            tb_text = string.join(info_list)
                                            message = tb_text
                                            print "message (%s)" % message

                                            blocking_reservations.append(reservation)
                                            blockingres_appended = 1
                                        else :
                                            # in get_chosen, if any of these
                                            # reservations nodes are chosen,
                                            # then a real pushback_res needs
                                            # to be made, to block any new job
                                            # reservations.
                                            # pushback res for duration of
                                            # both preempting and preemptible
                                            # jobs successful.  mode was real,
                                            # so this should be in the db.
                                            # do not append to blocking res
                                            # list.
                                            #print "pushback of preemptible job (%s) succeeded for reservation (%s) pushback_res['name'] (%s) !" % (pushback_res['job_runID'], reservation['name'], pushback_res['name'])
                                            #blocking_reservations.append(pushback_res)
                                            temp_res['start_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                                            temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                                            # seemed to work without this...
                                            blocking_reservations.append(pushback_res)
                                            blockingres_appended = 1
                                        #print "after try of pushback of (%s) (%s) (%s) res" % (reservation['purpose_type_string'], reservation['job_runID'], reservation['name'])
                                    else :
                                        # priority not overriding or PREEMPT_MIN_RUNTIME too small
                                        blocking_reservations.append(reservation)
                                        blockingres_appended = 1
                                else :
                                    # not preemptible reservation for non-run_at_risk job
                                    if DEBUGJOB != None:
                                        if job_step['preempting'] >= 1 and jobs_dict[reservation['job_runID']].has_key('preemptible') and jobs_dict[reservation['job_runID']]['preemptible'] >= 1 :
                                            if job_step['preempting'] >= 1 :
                                                print "job_step['preempting'] >= 1"
                                            if jobs_dict[reservation['job_runID']].has_key('preemptible') :
                                                print "jobs_dict[reservation['job_runID']].has_key('preemptible')"
                                            if jobs_dict[reservation['job_runID']]['preemptible'] >= 1 :
                                                print "jobs_dict[reservation['job_runID']]['preemptible'] >= 1"
                                        print "job_step['preempting'] (%s), job_step['priority'] (%s), jobs_dict[reservation['job_runID']]['priority'] (%s), jobs_dict[reservation['job_runID']]['preemptible']" % (job_step['preempting'], job_step['priority'], jobs_dict[reservation['job_runID']]['priority'], jobs_dict[reservation['job_runID']]['preemptible'])
                                    #blocking_reservations.append(reservation)
                                    temp_res['start_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                                    temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                            else :
                                # not getting here, not appending nonrunning
                                # reservations...
                                # This is goofy.  This will create an
                                # end_time_float before the start time.
                                # Did I do this for a reason?  It might
                                # be this way to choose run_at_risk occupied
                                # windows to be chosen last via last_available
                                #print "failed check for running/job reservation (%s) for non-run_at_risk job (%s)" % (reservation['name'], job_step['name'])
                                if jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                                  jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1 and \
                                  existing_preempted_job == 0 :
                                    temp_res['start_time_float'] = Now_float
                                    temp_res['end_time_float'] = Now_float
                                #else :
                                #    temp_res['start_time_float'] = Now_float
                                #    temp_res['end_time_float'] = Now_float
                            if blockingres_appended == 0:
                                # appending temp reservation
                                blocking_reservations.append(temp_res)
                        else :
                            # appending unmodified reservation
                            if blockingres_appended == 0:
                                blocking_reservations.append(reservation)
                    except :
                        info_tuple = sys.exc_info()
                        info_list = ["%s" % info_tuple[0], "%s" % info_tuple[1], '\n']
                        traceback.print_tb(info_tuple[2])
                        tb_list = traceback.format_tb(info_tuple[2])
                        info_list = info_list + tb_list
                        tb_text = string.join(info_list)
                        message = tb_text
                        print "message (%s)" % message
        if job_step['reservation_binding'] != None \
          and job_step['reservation_binding'] != [] :
            blocking_reservations = blocking_reservations + temp_blocking_reservations_list + blocking_reservations_list

        open_windows_list = get_open_windows_list(accepted_nodes_list, new_res, blocking_reservations, resources_db_handle)
        if job_step.has_key('resource_dict_list') :
            resource_dict_list = job_step['resource_dict_list']
        else :
            raise NoResourceDictList
        #print "len(open_windows_list) (%s)" % (len(open_windows_list),)
        if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
            print "checking open_windows_list before passing off to get_sized_windows"
            for new_window in open_windows_list :
                print "new_window[0] (%s), new_window[1] (%s), new_window[2] (%s)" % \
                  (time.asctime(time.localtime(new_window[0])), time.asctime(time.localtime(new_window[1])), new_window[2])
    # purpose_type_string != 'job', so use accepted_nodes_list to find open_windows_list
    else :
        # If this is not a job reservation, filter out all the job
        # reservations, so they don't block this one.  There may be
        # overlapping reservations until the job reservations get updated.
        # Leave blocking job reservations with startcount > 0
        if new_res['purpose_type_string'] != 'user_set' :
            non_job_reservations = filter(
              lambda reservation : reservation['purpose_type_string'] \
              != 'job' or reservation['start_count_int'] > 0, existing_reservations)
            # create a dictionary of reservations by node
            #reservations_by_node = {}
            #for reservation in non_job_reservations :
            #    for node in reservation
            truncated_reservations = []
            for reservation in non_job_reservations :
                temp_res = copy.deepcopy(reservation)
                if reservation['purpose_type_string'] in ['pushback', 'preempted_job'] :
                    temp_res['start_time_float'] = Now_float
                if reservation['purpose_type_string'] == 'running' and \
                  jobs_dict.has_key(reservation['job_runID']) and \
                  (jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
                  jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1) :
                    #temp_res = copy.deepcopy(reservation)
                    if reservation['purpose_type_string'] == 'running' or \
                      (reservation['purpose_type_string'] == 'job' and \
                      jobs_dict[reservation['job_runID']]['start_count_int'] != None and \
                      jobs_dict[reservation['job_runID']]['start_count_int'] >= 1) :
                        # need to check here for sufficient time before
                        # start of the next res...
                        # actually, did this earlier in the pushback section
                        temp_res['end_time_float'] = Now_float + RUN_AT_RISK_CLEANUP_TIME
                    else :
                        temp_res['end_time_float'] = Now_float
                    #truncated_reservations.append(temp_res)
                else :
                    pass
                    #truncated_reservations.append(reservation)
                truncated_reservations.append(temp_res)
            existing_reservations = truncated_reservations
        if new_res['purpose_type_string'] == 'preempted_job' :
            old_accepted_nodes_list = copy.copy(accepted_nodes_list)
            accepted_nodes_list = []
            for old_node in old_accepted_nodes_list:
                for resource_dict in resource_dict_list:
                    if resource_dict['resource_dict'].has_key(old_node) and not old_node in accepted_nodes_list:
                        accepted_nodes_list.append(old_node)
            #accepted_nodes_list = filter(lambda x,resource_dict_list=resource_dict_list : resource_dict_list[0]['resource_dict'].has_key(x), old_accepted_nodes_list)
        existing_reservations = existing_reservations + blocking_reservations_list
        #if purpose_type_string in ['preempted_job',] :
        #    for existing_reservation in existing_reservations :
        #        print "existing_reservation (%s)" % (existing_reservation['name'],)
        open_windows_list = get_open_windows_list(
          accepted_nodes_list, new_res, existing_reservations, resources_db_handle)
        #print "len(open_windows_list) (%s)" % len(open_windows_list)
        resource_dict_list = None
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "len(open_windows_list) (%s)" % len(open_windows_list)
    big_windows_list = get_big_windows_list(open_windows_list, new_res)
    #print "len(big_windows_list) (%s)" % len(big_windows_list)
    (sized_windows_list, current_start_time_float, counting_dict_list, node_bin_dict) = get_sized_windows_list(
      new_res, big_windows_list, resources_db_handle, jobs_db_handle, resource_dict_list=resource_dict_list, requested_resource_list = requested_resource_list)
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "final len(sized_windows_list) (%s)" % len(sized_windows_list)
    #print "final len(sized_windows_list) (%s)" % len(sized_windows_list)
    new_res['start_time_float'] = current_start_time_float
    relevant_nodes_list = []
    for window in sized_windows_list :
        relevant_nodes_list.append(window[2]['nodename'])
    active_reservations = []
    for reservation in existing_reservations :
        if (reservation['start_time_float'] <= current_start_time_float < reservation['end_time_float']) or (reservation['start_time_float'] < (current_start_time_float + duration_float) <= reservation['end_time_float']) :
            #if job_step.has_key('run_at_risk_int') and \
            #  job_step['run_at_risk_int'] < 1 and \
            if reservation.has_key('job_runID') and \
              reservation['job_runID'] != None and \
              jobs_dict.has_key(reservation['job_runID']) and \
              ((jobs_dict[reservation['job_runID']].has_key('run_at_risk_int') and \
              jobs_dict[reservation['job_runID']]['run_at_risk_int'] >= 1) or \
              (jobs_dict[reservation['job_runID']].has_key('preemptible') and \
              jobs_dict[reservation['job_runID']]['preemptible'] >= 1)) and \
              ( reservation['purpose_type_string'] == 'running' or \
              ( reservation['purpose_type_string'] == 'job' and \
              reservation['start_count_int'] > 0 )) :
                #temp_res = copy.deepcopy(reservation)
                #temp_res['affinity_calculation'] = "result = %s" % (- jobs_dict[reservation['job_runID']]['priority'],)
                #temp_res['job_restriction'] = 'result = 0'
                reservation['affinity_calculation'] = "result = %s" % (- jobs_dict[reservation['job_runID']]['priority'],)
                #active_reservations.append(temp_res)
                active_reservations.append(reservation)
            else :
                active_reservations.append(reservation)
            #if reservation['purpose_type_string'] not in ('job', 'running') \
            #  and (reservation['start_time_float'] <= current_start_time_float \
            #  < reservation['end_time_float'] or \
            #  reservation['start_time_float'] < current_start_time_float + duration_float <= reservation['end_time_float']) :
            #    active_reservations.append(reservation)
    windows_and_reservations_list = []
    if not (purpose_type_string in ['job', 'preempted_job', 'temp_pushback'] or re.match(r"^temp_pushback_for", purpose_type_string) ):
        job_step = Catalina____RESOURCEMANAGER_PLACEHOLDER___.initialize_job_step('temp_job_step')
    for window in sized_windows_list :
        relevant_reservations_list = []
        for reservation in active_reservations :
            if window[2]['nodename'] in reservation['node_list'] :
                relevant_reservations_list.append(reservation)
        windows_and_reservations_list.append((window, relevant_reservations_list, job_step))
    #print "main counting_dict_list (%s)" % (counting_dict_list,)
    sorted_windows_list = get_sorted_windows_list( sized_windows_list, new_res,
      accepted_nodes_list, node_sort_policy, resources_db_handle, reservations_db_handle, jobs_db_handle, windows_and_reservations_list )
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "len(sorted_windows_list) (%s)" % len(sorted_windows_list)
    #print "len(sorted_windows_list) (%s)" % len(sorted_windows_list)
    chosen_nodes_list, allocated_dict_list = get_chosen_nodes_list(new_res, sorted_windows_list, counting_dict_list, node_bin_dict, resources_db_handle, jobs_db_handle )
    #print "after get_chosen_nodes_list, allocated_dict_list (%s)" % (allocated_dict_list,)
    if DEBUGJOB !=None and job_runID != None and job_runID == DEBUGJOB :
        print "after get_chosen"
    #if purpose_type_string == 'job' :
    #    for allocated_dict in allocated_dict_list :
    #        if allocated_dict.has_key('nodename') :
    #            if not allocated_dict.has_key('type') :
    #                if job_step.has_key('node_usage') and job_step['node_usage'] == 'SHARED' :
    #                    allocated_dict['type'] = 'node_shared'
    #                else :
    #                    allocated_dict['type'] = 'node_exclusive'
    #                    if allocated_dict.has_key('cpu') :
    #                        allocated_dict['cpu'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableCpus']
    #                    if allocated_dict.has_key('memory') :
    #                        allocated_dict['memory'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableMemory']
    #else :
    #    for allocated_dict in allocated_dict_list :
    #        if allocated_dict.has_key('nodename') :
    #            if not allocated_dict.has_key('type') :
    #                allocated_dict['type'] = 'node_exclusive'
    #                if allocated_dict.has_key('cpu') :
    #                    allocated_dict['cpu'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableCpus']
    #                if allocated_dict.has_key('memory') :
    #                    allocated_dict['memory'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableMemory']
    #print "allocated_dict_list after (%s)" % (allocated_dict_list,)
    delete_object(new_res['name'], reservations_db_handle)
    if resource_amount_int != None and len(chosen_nodes_list) < resource_amount_int or len(chosen_nodes_list) == 0 :
        if DEBUGJOB != None and job_runID != None and job_runID == DEBUGJOB :
            print "len(chosen_nodes_list) (%s)" % len(chosen_nodes_list)
            print "resource_amount_int (%s)" % resource_amount_int
        if new_res.has_key('job_runID') and new_res['job_runID'] != None :
            # remove all "temp_pushback" reservations
            #print "didn't get all nodes for job (%s) res, removing all temp_pushback reservations" % (new_res['job_runID'],)
            pushback_res_list = []
            for res_key in reservations_db_handle[0] :
                if reservations_db_handle[0][res_key]['purpose_type_string'] == "temp_pushback_for_%s" % new_res['job_runID'] :
                    pushback_res_list.append(res_key)
                    #delete_object(res_key, reservations_db_handle)
            for res_key in pushback_res_list :
                #print "deleting temp_pushback reservations (%s)" % res_key
                delete_object(res_key, reservations_db_handle)
        raise InsufficientNodes(new_res)
    else :
        node_list = []
        # FIXME get rid of cpu and memory, use consumable_dict
        for allocated_dict in allocated_dict_list :
            #if new_res['node_usage'] == 'node_exclusive' :
            if allocated_dict.has_key('node') and allocated_dict['node'] > 0:
                #allocated_dict['node'] = 1
                if resources_db_handle[0].has_key(allocated_dict['nodename']) :
                    #if resources_db_handle[0][allocated_dict['nodename']].has_key('Cpus') :
                    #    allocated_dict['pcpus'] = resources_db_handle[0][allocated_dict['nodename']]['Cpus']
                    #    allocated_dict['cpu'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableCpus']
                    #if resources_db_handle[0][allocated_dict['nodename']].has_key('Memory') :
                    #    allocated_dict['pmemory'] = resources_db_handle[0][allocated_dict['nodename']]['Memory']
                    #    allocated_dict['memory'] = resources_db_handle[0][allocated_dict['nodename']]['ConsumableMemory']
                    for rkey in resources_db_handle[0][allocated_dict['nodename']]['consumable_dict'].keys():
                        if rkey == 'node':
                            continue
                        #print "resources_db_handle[0][%s] (%s)" % (allocated_dict['nodename'], resources_db_handle[0][allocated_dict['nodename']])
                        allocated_dict[rkey] = resources_db_handle[0][allocated_dict['nodename']]['consumable_dict'][rkey]
                        
            node_list.append(allocated_dict['nodename'])
        new_res['node_list'] = node_list
        new_res['end_time_float'] = current_start_time_float + duration_float
        if new_res.has_key('job_runID') and new_res['job_runID'] != None :
        #if new_res['purpose_type_string'] != "temp_pushback" :
            # remove any "pushback" reservations that don't overlap
            # the new reservation
            # if the job can't be immediately scheduled, the temp_pushback
            # reservations may interfere with a later scheduling slot.
            # will probably have to do a second non-pushback scheduling
            # run for the job, to see if it could be scheduled earlier.
            # should ignore the temp_pushback reservations, truncate the
            # running, then convert to persistent pushback, if it
            # interferes with the new reservation.
            #print "node_list (%s)" % (node_list,)
            pushback_del_list = []
            pushback_keep_list = []
            for res_key in reservations_db_handle[0].keys() :
                #if reservations_db_handle[0][res_key]['purpose_type_string'] == "temp_pushback" :
                if reservations_db_handle[0][res_key]['purpose_type_string'] == "temp_pushback_for_%s" % new_res['job_runID'] :
                    #pushback_res_list.append(res_key)
                    #print "checking temp_pushback (%s)" % res_key
                    # find the end time of the running reservation for the
                    # pushback res
                    # actually, need to do this for 'job' with start of 1,
                    # 'pushback' and 'preempted_job' reservations
                    overlap_found = 0
                    running_end_time = 0
                    running_start_time = END_OF_SCHEDULING
                    for running_res_key in reservations_db_handle[0].keys() :
                        if (
                          reservations_db_handle[0][running_res_key]['purpose_type_string'] in ['running', 'pushback', 'preempted_job'] \
                          or reservations_db_handle[0][running_res_key]['purpose_type_string'] == 'job' and reservations_db_handle[0][running_res_key]['start_count_int'] >= 1) \
                          and reservations_db_handle[0][running_res_key]['job_runID'] == reservations_db_handle[0][res_key]['job_runID'] :
                            running_end_time = reservations_db_handle[0][running_res_key]['end_time_float']
                            running_start_time = reservations_db_handle[0][running_res_key]['start_time_float']
                    if new_res['start_time_float'] < running_end_time and \
                       running_start_time < new_res['end_time_float'] :
                        # there is overlap
                        overlap_found = 1
                        #print "overlap_found for (%s)" % res_key
                        #update_object_attribute('purpose_type_string', 'pushback', reservations_db_handle[0][res_key], reservations_db_handle)
                    else :
                        pass
                        #print "no overlap for (%s)" % res_key
                        #pushback_list.append(res_key)
                        #continue
                        # no overlap
                    node_matched = 0
                    for allocated_dict in reservations_db_handle[0][res_key]['allocated_dict_list'] :
                        if allocated_dict['nodename'] in node_list :
                            node_matched = 1
                    if overlap_found == 0 or node_matched == 0 :
                        if not res_key in pushback_del_list :
                            pushback_del_list.append(res_key)
                        #delete_object(res_key, reservations_db_handle)
                    else :
                        update_object_attribute('purpose_type_string', 'pushback', reservations_db_handle[0][res_key], reservations_db_handle)
                        if not res_key in pushback_keep_list :
                            pushback_keep_list.append(res_key)
            for new_key in pushback_keep_list :
                for old_key in reservations_db_handle[0].keys() :
                    if reservations_db_handle[0][old_key]['purpose_type_string'] in ['pushback', 'preempted_job'] \
                      and old_key != new_key \
                      and reservations_db_handle[0][old_key]['start_time_float'] <= reservations_db_handle[0][new_key]['start_time_float'] \
                      and reservations_db_handle[0][new_key]['job_runID'] == reservations_db_handle[0][old_key]['job_runID'] :
                        if not old_key in pushback_del_list :
                            pushback_del_list.append(old_key)
            for del_key in pushback_del_list :
                delete_object(del_key, reservations_db_handle)
        #new_res['node_list'] = chosen_nodes_list
        new_res['allocated_dict_list'] = allocated_dict_list
        new_res['end_time_float'] = current_start_time_float + duration_float
        new_res['resource_amount_int'] = len(chosen_nodes_list)
        if DEBUGJOB != None and job_runID != None and job_runID == DEBUGJOB :
            print "allocated_dict_list (%s)" % (allocated_dict_list,)
            print "node_list (%s)" % (node_list,)
            print "chosen_nodes_list (%s)" % (chosen_nodes_list,)
        #print "allocated_dict_list (%s)" % (allocated_dict_list,)
        if mode != 'lookahead' :
            insert_new_object(new_res, reservations_db_handle)
        if old_res_id != None and mode == 'real' :
            delete_object(old_res_id, reservations_db_handle)

        return new_res

def roll_logs(events_db_handle, old_jobs_db_handle, old_reservations_db_handle) :
    # move old jobs, old reservations, events to date-stamped archive files
    # check size of db file, if db file exceeds DBSIZE_LIMIT, the
    # open a datestamped version in ARCHIVE_DIR, dump the db to that,
    # dump an empty dictionary to the production file.
    archive_stat = os.statvfs(ARCHIVE_DIR)
    # Check for 10,000 free blocks
    if archive_stat[statvfs.F_BAVAIL] < 10000 :
        raise 'InsufficientBlocks'
    # Check for 10 free inodes
    if archive_stat[statvfs.F_FAVAIL] != -1 and archive_stat[statvfs.F_FAVAIL] < 100000 :
        raise 'InsufficientINodes'
    db_names_list = (events_db_handle[1][1], old_jobs_db_handle[1][1], old_reservations_db_handle[1][1])
    db_list = (events_db_handle, old_jobs_db_handle, old_reservations_db_handle)
    uid_int = pwd.getpwnam(CAT_LOCK_OWNER)[2]
    gid_int = grp.getgrnam(CAT_LOCK_GROUP)[2]
    for db in db_list :
        db_name = db[1][1]
        file_stat = os.stat(DBDIR + '/' + db_name)
        size_bytes = file_stat[stat.ST_SIZE]
        if size_bytes > DBSIZE_LIMIT :
            archive_file_mode = None
            date_stamp = int(Now_float)
            while archive_file_mode == None :
                archive_file_name = ARCHIVE_DIR + '/' + db_name + '.' + "%s" % date_stamp
                if os.path.exists(archive_file_name) :
                    date_stamp = date_stamp + 1
                    continue
                break
        else :
            continue
        FO = open(archive_file_name, 'wb')
        db_dict = db[0]
        pickle = cPickle.Pickler(FO,1)
        pickle.fast = 1
        pickle.dump(db_dict)
        FO.close()
        os.chmod(archive_file_name,0664)
        os.chown(archive_file_name,uid_int,gid_int)
        for key in db_dict.keys() :
            del db_dict[key]

def log_event(event, events_db_handle) :
    # What should events be?  Dictionary, with 'name' and any
    # other relevant attributes
    event_name = get_new_db_key(events_db_handle)
    insert_new_object_with_key(event_name, event, events_db_handle)

def warn(message, subject, recipient) :
    if SERVERMODE not in ['NORMAL','TEST','SIM'] :
        print "warning in testmode"
        print "%s" % recipient
        print "%s" % subject
        print "%s" % message
    else :
        cmd = ECHO + ' ' + "'%s'" % message + ' | ' + MAILX + ' -s ' + '"%s"' % subject + " %s" % recipient
        os.system(cmd)

def catsyslog(message, priority) :
    # Could do this with the syslog module, but this way, the sys admin
    # can insert his/her own logger wrapper.
    if SERVERMODE not in ['NORMAL',] :
        cmd = LOGGER + ' -t CATALINA_DEBUG' + ' -p ' + LOGGER_FACILITY + '.debug ' + "'" + message + "'"
        os.system(cmd)
    else :
        cmd = LOGGER + ' -t CATALINA_NORMAL' + ' -p ' + LOGGER_FACILITY + '.' + priority + ' ' + "'" + message + "'"
        os.system(cmd)
    
####################
# convertToAbbreviatedForm()
# A function to convert nodelist string into abbreviated form. This string is
# passsed via Unix to RUNJOB. 
# previously to run a job, Catalina calls
# rj_LL ds355 888 99 0 ds355 ds355 ds355 ds355 ds355 ds366 ds366
# 
# However, there is a problem where this can be too long. Harkness has
# a job with 4096 tasks across 256 nodes, and it didn't get scheduled. UNIX
# limitation prevents us from passing 32KB worth of argument. Therefore, I 
# propose a compacted / abbreviated form as follows
# rj_LL ds355 888 99 0 ds355#5 ds3466#2
# 
# This is easier to read in show_events, consumed less space, and allows large
# jobs to run. The conversion takes very little time and the list is expanded
# once again in RUNJOB C program.
#
##
#incporporated feature request #560, the task geometry stuff
#Do not lump everything together, but keep the order in tact
#e.g. ds355 ds366 ds366 ds355 ds355 ds355 node list array 
# will be passed in as
# ds355#1 ds366#2 ds355#3
# previously, (the incorrect way) was
# ds355#4 ds366#2

######################
def convertToAbbreviatedForm(input_str):
    #split the input string by whitespace
    input_str = input_str.split(" ")
    ret_str = ""
    last_seen = input_str[0]
    last_seen_count = 0
    
    for word in input_str :
        if word == last_seen :
            last_seen_count +=1
        else :
            #save it to ret_str
            ret_str = ret_str + ' ' + last_seen + '#' + str(last_seen_count)
            #reset last_seen and last_seen_count
            last_seen = word
            last_seen_count=1
    #end of for
    
    #don't forget to add the last element. This is not done in the loop because
    #there is no different item after the last element. The data is still saved
    #in last_seen and last_seen_count
    ret_str = ret_str + ' ' + last_seen + '#' + str(last_seen_count)
    
    return ret_str.lstrip()
    
    
def filter_out_down_nodes(broken_node_list,jobs_db_handle):
    """ A function to take out the down nodes from a job resource_dict_list's 
    resource_dict. Only applies if the job state is not running. When a job is
    already running, removing its entry is too late"""
    
    object_list = get_object_list(jobs_db_handle)
    
    for broken in broken_node_list :
        
        #for every job in database
        for job in range(len(object_list)) :
            
            #if job state is running, it's too late
            if object_list[job]['state'] == 'Running' :
                continue
            
            #if job has no resource_dict_list, then don't need to
            # delete broken node
            if not object_list[job].has_key('resource_dict_list') or \
              not type(object_list[job]['resource_dict_list']) == list :
                continue
            
            #for every resource_list in each job 
            for res_list in range(len(object_list[job]['resource_dict_list'])) :
                
                try :
                    del object_list[job]['resource_dict_list'][res_list]['resource_dict'][broken]
                    
                except :
                    continue
    
def get_node_sets(new_res, switch_node_list, resources_db_handle, jobs_db_handle):
    #print "start of get_node_sets"
    # insert TOPOLOGY check here
    if TOPOLOGY_MODULE != None:
        # generate a list of switch_tuples and max_hop
        if new_res.has_key('maxhops') and new_res['maxhops'] != None:
            maxhops = int(new_res['maxhops'])
            #switch_node_list = node_bin_dict.keys()
            #for window in sorted_windows_list :
            #    if  DEBUGJOB != None and new_res['job_runID'] != None and new_res['job_runID'] == DEBUGJOB :
            #        print "processing sorted window (%s)" % (window,)
            #    nodename = window[2]['nodename']
            jobs_dict = jobs_db_handle[0]
            resources_dict = resources_db_handle[0]
            #node_sets_list = Topology.get_switch_sets(maxhops, switch_node_list, resources_dict, jobs_dict)
            #print "before get_switch_sets"
            node_sets_list = get_switch_sets(maxhops, switch_node_list, resources_dict, jobs_dict)
            #print "after get_switch_sets"
        else:
            maxhops = None
            #node_sets_list = [node_bin_dict.keys(),]
            # profile this.
            #switch_node_list = node_bin_dict.keys()
            jobs_dict = jobs_db_handle[0]
            resources_dict = resources_db_handle[0]
            node_sets_list = get_switch_sets(maxhops, switch_node_list, resources_dict, jobs_dict)
    else:
        #node_sets_list = [node_bin_dict.keys(),]
        node_sets_list = [switch_node_list,]
    # sort, smallest first
    node_sets_list_sort = []
    for node_set in node_sets_list:
        # if new_res has a resource amount, should exclude small node_sets here
        if len(node_set) < len(new_res['requested_resource_list']):
            continue
        # should also add a license node to all node sets.
        #if LICENSENODE != None:
        #if LICENSENODE != None and LICENSENODE in switch_node_list:
        #    node_set.append(LICENSENODE)
        #    node_sets_list_sort.append((len(node_set), node_set))
        node_set.append(LICENSENODE)
        node_sets_list_sort.append((len(node_set), node_set))
    # do not sort, as this will mess up the sorted node list
    #node_sets_list_sort.sort()
    sorted_node_sets_list = map(lambda x: x[1], node_sets_list_sort)
    #print sorted_node_sets_list
    #print "len(sorted_node_sets_list) (%s)" % (len(sorted_node_sets_list),)
    return sorted_node_sets_list
    
#end of file





