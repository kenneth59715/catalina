[main]
# Example project account checking script.  Only used by
# user_set_res.py.  Set this to the path of an executable
# that will take arguments:
# username=<username of job owner> account=<charge account> su=<charge units integer>
# return 1 for rejection.
RESLIST_CMD=/work/kenneth/cattest/install/reslist

# Settings for warning emails
MAIL_RECIPIENT=kenneth
MAILX=/usr/bin/mailx
ECHO=/usr/bin/echo

# Owner and group for lock files
CAT_LOCK_OWNER=kenneth
CAT_LOCK_GROUP=sys200

# Server mode. 'NORMAL' means really cancel and run
# jobs, otherwise, just log.
# 'SIM' means get job and resource info from sim commands instead
# of from a real resource manager.
SERVERMODE=TEST

# For simulation mode, query job command and query machine command
QJ_SIM=/work/kenneth/cattest/install/qj_sim
QM_SIM=/work/kenneth/cattest/install/qm_sim

DEBUG=None
# Catalina install directory
HOMEDIR=/work/kenneth/cattest/install
# Large directory, for storing archive db files
ARCHIVE_DIR=/work/kenneth/cattest/archive
# this config file
CONFIGFILE=%(HOMEDIR)s/catalina.config

# Database information
CONFIGURATION_DB=configuration
CONFIGURED_RESOURCES_DB=configured_resources
EVENTS_DB=events
JOBS_DB=jobs
OLD_JOBS_DB=old_jobs
OLD_RESERVATIONS_DB=old_reservations
RESERVATIONS_DB=reservations
RESOURCE_DB=resource
STANDING_RESERVATIONS_DB=standing_reservations
LOCK_SUFFIX=.lock

# Policy
# MACHINE_REFRESH_INTERVAL = n
# This governs Catalina on how frequent the updates should be. Fewer number
# makes Catalina more accurate, larger number makes Catalina faster. 
# On systems such as datastar where starterd and other daemons are not stable,
# due to nodes out of memory, it is not recommended to have a large value
# else some jobs (especially large jobs) will continually be pushed back/delayed.
# On other systems such as teragrid linux, PBS is more robust and stable, so
# making this number larger is okay.
MACHINE_REFRESH_INTERVAL=3

# ON = enforce max jobs running per user.  All other queued
# jobs for that user go into non-queued
MAXJOBPERUSERPOLICY=ON
# number of running jobs to allow each user
MAXJOBPERUSERCOUNT=4
# ON = enforce max jobs queued per user.  All other non-running
# jobs go in to non-queued
MAXJOBQUEUEDPERUSERPOLICY=ON
# number of queued jobs to allow each user
MAXJOBQUEUEDPERUSERCOUNT=4
# ON = enforce max jobs running per account.  All other queued
# jobs for that account go into non-queued
MAXJOBPERACCOUNTPOLICY=ON
# number of running jobs to allow each account
MAXJOBPERACCOUNTCOUNT=4
# ON = enforce max jobs queued per account.  All other non-running
# jobs go in to non-queued
MAXJOBQUEUEDPERACCOUNTPOLICY=ON
# number of queued jobs to allow each user
MAXJOBQUEUEDPERACCOUNTCOUNT=4
# ON = enforce requirements check.  If job does not find any
# resources, job goes into non-queued.
BADRESOURCELIST=ON
# seconds to pad reservations with.  job reservations are longer
# than actual requested wallclock limit by this amount.
FUDGE_FACTOR=600.0
# seconds job may exceed its wall clock limit
MAXJOBOVERRUN=120.0
# global max priority value.  Jobs may not have a priority larger
# than this.
MAXPRIORITY=2000000000000000
# sort code for choosing among possible nodes
# last_available will choose the nodes that free up at
# the latest time.  This preserves the largest backfill window.
NODE_SORT_POLICY_CODE_FILE=%(HOMEDIR)s/last_available
# number of jobs to set reservation for.  None means
# set for all eligible jobs
RESERVATION_DEPTH=None
# max bytes for each db file before archiving
DBSIZE_LIMIT=5000000
# Latest time considered for scheduling, in seconds from now.
SCHEDULING_WINDOW = 7776000.0
# Timeouts for job start and down nodes in seconds
JOB_START_TIME_LIMIT=900.0
DB_WARN_LIMIT=3
RESOURCE_DOWN_TIME_LIMIT=900.0
LOST_JOB_LIMIT=43200
LOST_JOB_WARN=TRUE
#Recommended for PBS (PBS does not keep info of completed jobs):
#JOB_START_WARN_LIMIT=3
#DB_WARN_LIMIT=3
#JOB_START_TIME_LIMIT=5.0
#LOST_JOB_LIMIT=0
#LOST_JOB_WARN=FALSE

# Priority
# Priority Calculation:
#(time in seconds, resource in nodes)
#priority =
#resource_number      * Resource_Weight                    +
#local_admin_float    * Local_Admin_Weight                 +
#local_user_float     * Local_User_Weight                  +
#expansion_factor     * Expansion_Factor_Weight            +
#queue_wait_time      * System_Queue_Time_Weight           +
#submit_wait_time     * Submit_Time_Weight                 +
#wall_clock_time      * Wall_Time_Weight                   +
#QOS_priority         * QOS_Priority_Weight                +
#QOS_target_xf_value  * QOS_Target_Expansion_Factor_Weight +
#QOS_target_qwt_value * QOS_Target_Queue_Wait_Time_Weight  +
#local_admin_float    * QOS_LOCAL_ADMIN_WEIGHT             +
#fairshare_value      * Fairshare_Bonus_Weight

RESOURCE_WEIGHT=11000.0
EXPANSION_FACTOR_WEIGHT=1.0
SYSTEM_QUEUE_TIME_WEIGHT=0.1
SUBMIT_TIME_WEIGHT=0.0
LOCAL_USER_WEIGHT=0.0

# feature 582
FAIRSHARE_BONUS_WEIGHT = 0.0
PENALTY_PERCENTAGE=20.0
TOTAL_AVAILABLE = 4800.0
THRESHOLD_PERCENTAGE=60.0

# Moved to QOS based setting. Do not use!.
#LOCAL_ADMIN_WEIGHT=0.0

WALL_TIME_WEIGHT=0.0
QOS_PRIORITY_WEIGHT = 6000.0
QOS_TARGET_EXPANSION_FACTOR_WEIGHT = 1.0
QOS_TARGET_QUEUE_WAIT_TIME_WEIGHT = 1.0

# each QOS has a starting priority value
QOS_PRIORITY_STRING = { '0' : 0,
                 '1' : 1,
                 '2' : 2,
                 '3' : 3,
                 '4' : 4,
                 '5' : 5,
                 '6' : 3,
                 '7' : 3,
                 '8' : 4,
                 '9' : 4,
                '10' : 5
                }
# each QOS has a max priority
QOS_MAX_PRIORITY_STRING = { '0' : 1000000000000L,
                 '1' : 1000000000000L,
                 '2' : 1000000000000L,
                 '3' : 1000000000000L,
                 '4' : 1000000000000L,
                 '5' : 1000000000000L,
                 '6' : 1000000000000L,
                 '7' : 1000000000000L,
                 '8' : 1000000000000L,
                 '9' : 1000000000000L,
                '10' : 1000000000000L
                }
# each QOS may have a target expansion factor
QOS_TARGETXF_STRING = { '0' : None,
                 '1' : None,
                 '2' : None,
                 '3' : None,
                 '4' : None,
                 '5' : None,
                 '6' : None,
                 '7' : 1.3,
                 '8' : None,
                 '9' : None,
                '10' : None
                }
# each QOS may have a target queue wait time
QOS_TARGETQT_STRING = { '0' : None,
                 '1' : None,
                 '2' : None,
                 '3' : None,
                 '4' : None,
                 '5' : None,
                 '6' : None,
                 '7' : 86400.0,
                 '8' : None,
                 '9' : None,
                '10' : None
                }
# each QOS may have a max running jobs per user policy
QOS_MAXJOBPERUSERPOLICY_STRING = { '0' : 4,
                 '1' : 4,
                 '2' : 4,
                 '3' : 4,
                 '4' : 4,
                 '5' : 4,
                 '6' : 4,
                 '7' : 4,
                 '8' : 4,
                 '9' : None,
                '10' : 4
                }
# each QOS may have a max queued jobs per user policy
QOS_MAXJOBQUEUEDPERUSERPOLICY_STRING = { '0' : 4,
                 '1' : 4,
                 '2' : 4,
                 '3' : 4,
                 '4' : 4,
                 '5' : 4,
                 '6' : 4,
                 '7' : 4,
                 '8' : 4,
                 '9' : None,
                '10' : 4
                }

# each QOS may have a max running jobs per account policy
QOS_MAXJOBPERACCOUNTPOLICY_STRING = { '0' : 4,
                 '1' : 4,
                 '2' : 4,
                 '3' : 4,
                 '4' : 4,
                 '5' : 4,
                 '6' : 4,
                 '7' : 4,
                 '8' : 4,
                 '9' : None,
                '10' : 4
                }
# each QOS may have a max queued jobs per account policy
QOS_MAXJOBQUEUEDPERACCOUNTPOLICY_STRING = { '0' : 4,
                 '1' : 4,
                 '2' : 4,
                 '3' : 4,
                 '4' : 4,
                 '5' : 4,
                 '6' : 4,
                 '7' : 4,
                 '8' : 4,
                 '9' : None,
                '10' : 4
                }


# each QOS may have a local admin weight factor.
# Local admin weight factor influence job's priority for each QOS.
# This value is multiplied by amount of user's SUs (= local_admin_float)
#
# priority += local_admin_weight * local_admin_float
QOS_LOCAL_ADMIN_WEIGHT_STRING = { '0' : 0.04,
                 '1' : 0.04,
                 '2' : 0.04,
                 '3' : 0.04,
                 '4' : 0.04,
                 '5' : 0.04,
                 '6' : 0.04,
                 '7' : 0.04,
                 '8' : 0.04,
                 '9' : None,
                '10' : 0.04
                }


                 
# for PBS
# used by testresL9.py for test job name
#SUBMITCMD = /usr/local/bin/qsub -q standard
#CANCELCMD = /usr/local/bin/qsub -q standard
#TEST_JOB = testjob.PBS.1
#USERNAMESUFFIX = @yylogin
# Translate PBS node states to Catalina node states
#RM_TO_CAT_RESOURCE_DICT_STRING = {
#  'free' : 'Idle',
#  'Idle' : 'Idle',
#  'down' : 'Down',
#  'Down' : 'Down',
#  'state-unknown,down' : 'Down',
#  'offline' : 'Down',
#  'busy' : 'Running',
#  'Running' : 'Running',
#  'job-exclusive' : 'Running',
#  'job-sharing' : 'Running'
#  }
# Translate PBS job states to Catalina node states
#RM_TO_CAT_JOB_DICT_STRING = {
#  'qtime' : 'Submit_Time',
#  'R' : 'Running',
#  'E' : 'Running',
#  'H' : 'Hold',
#  'Q' : 'Idle',
#  'S' : 'Hold',
#  'T' : 'Hold',
#  'W' : 'Hold'
#  }

# for LL
SUBMITCMD='/usr/lpp/LoadL/full/bin/llsubmit'
CANCELCMD='/usr/lpp/LoadL/full/bin/llcancel'
TEST_JOB = testjob.LL
USERNAMESUFFIX = 
LOADL_ADMIN_FILE=/paci/loadl/LoadL_admin
# Translate LoadL node states to Catalina node states
RM_TO_CAT_RESOURCE_DICT_STRING = {
  'None' : 'None',
  'Idle' : 'Idle',
  'Down' : 'Down',
  'Drain' : 'Drain',
  'Draining' : 'Draining',
  'Busy' : 'Running',
  'Running' : 'Running',
  'Starting' : 'Running'
  }
# Translate LoadL job states to Catalina node states
RM_TO_CAT_JOB_DICT_STRING = {
  'submittime' : 'Submit_Time',
  'Running' : 'Running',
  'Canceled' : 'Canceled',
  'Completed' : 'Completed',
  'Removed' : 'Removed',
  'Remove_Pending' : 'Running',
  'Starting' : 'Running',
  'Hold' : 'Hold',
  'Idle' : 'Idle'
  }

# Used by show_bf and show_q to make assumptions.
DEFAULT_JOB_CLASS = normal
# code used by Catalina to screen nodes.  If result = 0,
# node is accepted.
NODERESTCODE_STRING = 
 import string
 resource = input_tuple[0]
 if resource['State'] == 'Down' : result = 'Down'
 elif resource['State'] == 'Drain' : result = 'Drain'
 elif resource['State'] == 'Drained' : result = 'Drained'
 elif resource['State'] == 'None' : result = 'None'
 elif resource['State'] == None : result = None
 elif resource['State'] == 'Unknown' : result = 'Unknown'
 elif resource['Max_Starters'] == 0 : result = 'Max_Starters=0'
 else : result = 0
# User-settable limits dictionary
# should set number of instances, nodes/instance, seconds/instance
# the values can be an integer, with negative meaning no limit
USER_SET_LIMITS_DICT_STRING = {
  'sys200' : { 'instances_int' : -1,
                   'nodes_int' : -1,
                 'seconds_int' : -1
             },
  'use300' : { 'instances_int' : 4,
                   'nodes_int' : 32,
                 'seconds_int' : 64800 },
  'OTHERS' : { 'instance_int' : 3,
                  'nodes_int' : 13,
                'seconds_int' : 43200 }
  }

# used by user_set_res to compute potential cost of
# a user reservation
CLASS_PRIORITY_DICT_STRING = {
  'interactive' : 1.8,
  'premium' : 1.0,
  'express' : 1.8,
  'high' : 1.8,
  'normal' : 1.0,
  'low' : 0.5,
  'standby' : 0.0,
  'legion' : 0.0,
  'WH' : 0.0,
  'Diag' : 0.0,
  'Diag0' : 0.0,
  'Diag1' : 0.0,
  'Diag2' : 0.0,
  'Diag3' : 0.0,
  'Diag4' : 0.0,
  'Diag5' : 0.0,
  'Diag6' : 0.0,
  'Diag7' : 0.0,
  'Diag8' : 0.0,
  'Diag9' : 0.0
  }
# user_set_res uses this class to compute charge
DEFAULT_RES_CLASS = express
# user_set_res uses this proc count to compute charge
DEFAULT_PROC_CHARGE = 8

# Test config info
TEST_SHORTPOOL_AMOUNT = 2
TEST_SHORTPOOL_SPEC = 0 0 * * 0,1,2,3,4
TEST_SHORTPOOL_DURATION = 43200
TEST_STANDING_AMOUNT = 2
TEST_STANDING_SPEC = 0 12 * * 0,1,2,3,4
TEST_STANDING_DURATION = 43200
TEST_USERRES_AMOUNT = 2
TEST_USERRES_MOD_AMOUNT = 2
TEST_USERRES_END = 1209600
TEST_BINDING_AMOUNT = 2
TEST_NOEARLIEST_AMOUNT = 2

